{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LeNet_Deep_vs_Wide_Parameters.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYealP-NwY-5",
        "outputId": "759cb43c-5b1a-40c8-9900-2ffcca75c3dd"
      },
      "source": [
        "!pip install torchcsprng==0.1.3+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torchcsprng==0.1.3+cu101 in /usr/local/lib/python3.6/dist-packages (0.1.3+cu101)\n",
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (from torchcsprng==0.1.3+cu101) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchcsprng==0.1.3+cu101) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchcsprng==0.1.3+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchcsprng==0.1.3+cu101) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchcsprng==0.1.3+cu101) (0.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2YFH02ME8oi",
        "outputId": "51944402-930e-4bb8-a773-b47c7200ea0a"
      },
      "source": [
        "!pip install opacus"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opacus in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.6/dist-packages (from opacus) (0.8.1+cu101)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from opacus) (1.18.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from opacus) (3.6.4)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.6/dist-packages (from opacus) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.40 in /usr/local/lib/python3.6/dist-packages (from opacus) (4.41.1)\n",
            "Requirement already satisfied: torchcsprng>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from opacus) (0.1.3+cu101)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from opacus) (1.7.0+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.4->opacus) (7.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->opacus) (20.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->opacus) (50.3.2)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->opacus) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->opacus) (1.15.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->opacus) (1.9.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->opacus) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->opacus) (8.6.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->opacus) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->opacus) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->opacus) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5BBXLk-TeO2"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch import Tensor\n",
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "import os\n",
        "import opacus\n",
        "from opacus import PrivacyEngine"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zobnA_-ToFR"
      },
      "source": [
        "# __all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "#            'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
        "#            'wide_resnet50_2', 'wide_resnet101_2']\n",
        "\n",
        "\n",
        "# model_urls = {\n",
        "#     'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "#     'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "#     'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "#     'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "#     'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "#     'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "#     'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "#     'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "#     'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "# }"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chG0K9-4wMbt"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, k, kernel_size = 5, stride = 1):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        \n",
        "        self.conv = nn.Conv2d(in_channels, out_channels*k, kernel_size, stride)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.pool = nn.AvgPool2d(kernel_size = 2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        out = self.pool(self.tanh(x))\n",
        "        \n",
        "        return out "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "morUfocyRsQf"
      },
      "source": [
        "### Vanilla LeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjZdPfhdRaDL"
      },
      "source": [
        "class LeNet5(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        super(LeNet5, self).__init__()\n",
        "        \n",
        "        self.feature_extractor = nn.Sequential(            \n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=120, out_features=84),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=84, out_features=n_classes),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.classifier(x)\n",
        "        return logits"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nGJ_8kARv3f",
        "outputId": "6f2b51c7-392b-4eed-e8b5-c07db6f00c28"
      },
      "source": [
        "from torchsummary import summary\n",
        "model = LeNet5(10)\n",
        "model.cuda()\n",
        "summary(model, (3, 32, 32))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 28, 28]             456\n",
            "              Tanh-2            [-1, 6, 28, 28]               0\n",
            "         AvgPool2d-3            [-1, 6, 14, 14]               0\n",
            "            Conv2d-4           [-1, 16, 10, 10]           2,416\n",
            "              Tanh-5           [-1, 16, 10, 10]               0\n",
            "         AvgPool2d-6             [-1, 16, 5, 5]               0\n",
            "            Conv2d-7            [-1, 120, 1, 1]          48,120\n",
            "              Tanh-8            [-1, 120, 1, 1]               0\n",
            "            Linear-9                   [-1, 84]          10,164\n",
            "             Tanh-10                   [-1, 84]               0\n",
            "           Linear-11                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.11\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 0.36\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZrRd5dt_JUp"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULAwF7F7RlGD"
      },
      "source": [
        "### Wide LeNet with Linear Bottle Neck"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEGn5y-L_Jt1"
      },
      "source": [
        "class Linear_WideLeNet5(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        super(Linear_WideLeNet5, self).__init__()\n",
        "        \n",
        "        self.feature_extractor = nn.Sequential(            \n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=32, kernel_size=5, stride=1, padding = 1),\n",
        "            nn.Conv2d(in_channels=32, out_channels=8, kernel_size=1, stride=1, padding = 1),\n",
        "            nn.Conv2d(in_channels=8, out_channels=62, kernel_size=5, stride=1),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=62, out_channels=11, kernel_size=1, stride=1, padding = 0),\n",
        "            nn.Conv2d(in_channels=11, out_channels=120, kernel_size=5, stride=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=120, out_features=84),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=84, out_features=n_classes),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        # print(x.shape)\n",
        "        x = torch.flatten(x, 1)\n",
        "        # print(x.shape)\n",
        "        logits = self.classifier(x)\n",
        "        return logits"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik6xsCOK_ViA",
        "outputId": "e6591989-3a4b-4017-f47b-124be383fbf2"
      },
      "source": [
        "from torchsummary import summary\n",
        "Linear_WideLeNet5 = Linear_WideLeNet5(10)\n",
        "Linear_WideLeNet5.cuda()\n",
        "summary(Linear_WideLeNet5, (3, 32, 32))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 120, 1, 1])\n",
            "torch.Size([2, 120])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 28, 28]             456\n",
            "              Tanh-2            [-1, 6, 28, 28]               0\n",
            "         AvgPool2d-3            [-1, 6, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 12, 12]           4,832\n",
            "            Conv2d-5            [-1, 8, 14, 14]             264\n",
            "            Conv2d-6           [-1, 62, 10, 10]          12,462\n",
            "              Tanh-7           [-1, 62, 10, 10]               0\n",
            "         AvgPool2d-8             [-1, 62, 5, 5]               0\n",
            "            Conv2d-9             [-1, 11, 5, 5]             693\n",
            "           Conv2d-10            [-1, 120, 1, 1]          33,120\n",
            "             Tanh-11            [-1, 120, 1, 1]               0\n",
            "           Linear-12                   [-1, 84]          10,164\n",
            "             Tanh-13                   [-1, 84]               0\n",
            "           Linear-14                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 62,841\n",
            "Trainable params: 62,841\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.24\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 0.49\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrVGYhZLT3GE"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0cETaj1T3I5"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-5qCooYT3s8"
      },
      "source": [
        "### Wide LeNet with Non-Linear Bottle Neck"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGvSMZjRT513"
      },
      "source": [
        "class Non_Linear_WideLeNet5(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        super(Non_Linear_WideLeNet5, self).__init__()\n",
        "        \n",
        "        self.feature_extractor = nn.Sequential(            \n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=32, kernel_size=5, stride=1, padding = 1),\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=8, kernel_size=1, stride=1, padding = 1),\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(in_channels=8, out_channels=62, kernel_size=5, stride=1),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=62, out_channels=11, kernel_size=1, stride=1, padding = 0),\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(in_channels=11, out_channels=120, kernel_size=5, stride=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=120, out_features=84),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=84, out_features=n_classes),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        # print(x.shape)\n",
        "        x = torch.flatten(x, 1)\n",
        "        # print(x.shape)\n",
        "        logits = self.classifier(x)\n",
        "        return logits"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZVduRCsT54x",
        "outputId": "019c3923-16fd-4ea3-8e83-f6d235811488"
      },
      "source": [
        "from torchsummary import summary\n",
        "Non_Linear_WideLeNet5 = Non_Linear_WideLeNet5(10)\n",
        "Non_Linear_WideLeNet5.cuda()\n",
        "summary(Non_Linear_WideLeNet5, (3, 32, 32))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 120, 1, 1])\n",
            "torch.Size([2, 120])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 28, 28]             456\n",
            "              Tanh-2            [-1, 6, 28, 28]               0\n",
            "         AvgPool2d-3            [-1, 6, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 12, 12]           4,832\n",
            "              Tanh-5           [-1, 32, 12, 12]               0\n",
            "            Conv2d-6            [-1, 8, 14, 14]             264\n",
            "              Tanh-7            [-1, 8, 14, 14]               0\n",
            "            Conv2d-8           [-1, 62, 10, 10]          12,462\n",
            "              Tanh-9           [-1, 62, 10, 10]               0\n",
            "        AvgPool2d-10             [-1, 62, 5, 5]               0\n",
            "           Conv2d-11             [-1, 11, 5, 5]             693\n",
            "             Tanh-12             [-1, 11, 5, 5]               0\n",
            "           Conv2d-13            [-1, 120, 1, 1]          33,120\n",
            "             Tanh-14            [-1, 120, 1, 1]               0\n",
            "           Linear-15                   [-1, 84]          10,164\n",
            "             Tanh-16                   [-1, 84]               0\n",
            "           Linear-17                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 62,841\n",
            "Trainable params: 62,841\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.29\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 0.54\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sIrW-Cf2JFe"
      },
      "source": [
        "best_acc = []\n",
        "best_train_acc = best_test_acc = start_epoch = 0\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "optimizer1 = optim.Adam(Linear_WideLeNet5.parameters(), lr = 0.001)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plDe7dSMxbNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b97603-0b08-4e65-ab79-bf11f20c7c86"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jT1k0Ez1z7G"
      },
      "source": [
        "def train(model, epoch):\n",
        "    global best_train_acc, best_test_acc\n",
        "    device = 'cuda'\n",
        "    model.to(device)\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print(f'Epoch {epoch} Step {batch_idx}/{len(trainloader)}', 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "        \n",
        "        acc = 100.*correct/total\n",
        "        if acc>best_train_acc:\n",
        "            best_train_acc = acc"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocE9d0Cp16Vk"
      },
      "source": [
        "def test(model, epoch):\n",
        "    global best_train_acc, best_test_acc\n",
        "    device = 'cuda'\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            print(f'Epoch {epoch} Step {batch_idx}/{len(testloader)}', 'Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
        "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_test_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'model': model.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_test_acc = acc"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWBOwCZc29zq"
      },
      "source": [
        "def train_wide(model, epoch):\n",
        "    global best_train_acc, best_test_acc\n",
        "    device = 'cuda'\n",
        "    model.to(device)\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer1.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print(f'Epoch {epoch} Step {batch_idx}/{len(trainloader)}', 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "        \n",
        "        acc = 100.*correct/total\n",
        "        if acc>best_train_acc:\n",
        "            best_train_acc = acc"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reR8gOBq1-FR",
        "outputId": "3e272b0e-b8f2-4a4e-e89c-d59883cc132c"
      },
      "source": [
        "batch_size = sample_size = 32\n",
        "\n",
        "privacy_engine = PrivacyEngine(\n",
        "    model,\n",
        "    batch_size,\n",
        "    sample_size,\n",
        "    alphas=[10, 100],\n",
        "    noise_multiplier= 1.3,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "privacy_engine.attach(optimizer)\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+50):\n",
        "    train(model, epoch)\n",
        "    test(model, epoch)\n",
        "\n",
        "best_acc.append([best_train_acc, best_test_acc])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/opacus/privacy_engine.py:104: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Step 0/1563 Loss: 2.301 | Acc: 12.500% (4/32)\n",
            "Epoch 0 Step 1/1563 Loss: 2.301 | Acc: 12.500% (8/64)\n",
            "Epoch 0 Step 2/1563 Loss: 2.301 | Acc: 11.458% (11/96)\n",
            "Epoch 0 Step 3/1563 Loss: 2.304 | Acc: 8.594% (11/128)\n",
            "Epoch 0 Step 4/1563 Loss: 2.303 | Acc: 10.625% (17/160)\n",
            "Epoch 0 Step 5/1563 Loss: 2.304 | Acc: 9.375% (18/192)\n",
            "Epoch 0 Step 6/1563 Loss: 2.305 | Acc: 8.929% (20/224)\n",
            "Epoch 0 Step 7/1563 Loss: 2.305 | Acc: 8.594% (22/256)\n",
            "Epoch 0 Step 8/1563 Loss: 2.305 | Acc: 8.681% (25/288)\n",
            "Epoch 0 Step 9/1563 Loss: 2.306 | Acc: 8.438% (27/320)\n",
            "Epoch 0 Step 10/1563 Loss: 2.305 | Acc: 9.091% (32/352)\n",
            "Epoch 0 Step 11/1563 Loss: 2.304 | Acc: 10.417% (40/384)\n",
            "Epoch 0 Step 12/1563 Loss: 2.303 | Acc: 10.337% (43/416)\n",
            "Epoch 0 Step 13/1563 Loss: 2.303 | Acc: 11.161% (50/448)\n",
            "Epoch 0 Step 14/1563 Loss: 2.303 | Acc: 11.250% (54/480)\n",
            "Epoch 0 Step 15/1563 Loss: 2.303 | Acc: 10.938% (56/512)\n",
            "Epoch 0 Step 16/1563 Loss: 2.302 | Acc: 11.397% (62/544)\n",
            "Epoch 0 Step 17/1563 Loss: 2.302 | Acc: 11.458% (66/576)\n",
            "Epoch 0 Step 18/1563 Loss: 2.301 | Acc: 11.513% (70/608)\n",
            "Epoch 0 Step 19/1563 Loss: 2.302 | Acc: 11.406% (73/640)\n",
            "Epoch 0 Step 20/1563 Loss: 2.302 | Acc: 11.012% (74/672)\n",
            "Epoch 0 Step 21/1563 Loss: 2.301 | Acc: 10.938% (77/704)\n",
            "Epoch 0 Step 22/1563 Loss: 2.302 | Acc: 10.598% (78/736)\n",
            "Epoch 0 Step 23/1563 Loss: 2.302 | Acc: 10.417% (80/768)\n",
            "Epoch 0 Step 24/1563 Loss: 2.302 | Acc: 10.375% (83/800)\n",
            "Epoch 0 Step 25/1563 Loss: 2.302 | Acc: 10.457% (87/832)\n",
            "Epoch 0 Step 26/1563 Loss: 2.302 | Acc: 10.417% (90/864)\n",
            "Epoch 0 Step 27/1563 Loss: 2.302 | Acc: 10.268% (92/896)\n",
            "Epoch 0 Step 28/1563 Loss: 2.302 | Acc: 10.237% (95/928)\n",
            "Epoch 0 Step 29/1563 Loss: 2.302 | Acc: 10.729% (103/960)\n",
            "Epoch 0 Step 30/1563 Loss: 2.302 | Acc: 10.685% (106/992)\n",
            "Epoch 0 Step 31/1563 Loss: 2.302 | Acc: 11.035% (113/1024)\n",
            "Epoch 0 Step 32/1563 Loss: 2.302 | Acc: 11.080% (117/1056)\n",
            "Epoch 0 Step 33/1563 Loss: 2.302 | Acc: 11.213% (122/1088)\n",
            "Epoch 0 Step 34/1563 Loss: 2.302 | Acc: 11.161% (125/1120)\n",
            "Epoch 0 Step 35/1563 Loss: 2.301 | Acc: 11.545% (133/1152)\n",
            "Epoch 0 Step 36/1563 Loss: 2.301 | Acc: 11.655% (138/1184)\n",
            "Epoch 0 Step 37/1563 Loss: 2.301 | Acc: 11.595% (141/1216)\n",
            "Epoch 0 Step 38/1563 Loss: 2.301 | Acc: 11.619% (145/1248)\n",
            "Epoch 0 Step 39/1563 Loss: 2.301 | Acc: 11.484% (147/1280)\n",
            "Epoch 0 Step 40/1563 Loss: 2.301 | Acc: 11.509% (151/1312)\n",
            "Epoch 0 Step 41/1563 Loss: 2.301 | Acc: 11.458% (154/1344)\n",
            "Epoch 0 Step 42/1563 Loss: 2.301 | Acc: 11.701% (161/1376)\n",
            "Epoch 0 Step 43/1563 Loss: 2.301 | Acc: 11.506% (162/1408)\n",
            "Epoch 0 Step 44/1563 Loss: 2.301 | Acc: 11.528% (166/1440)\n",
            "Epoch 0 Step 45/1563 Loss: 2.301 | Acc: 11.481% (169/1472)\n",
            "Epoch 0 Step 46/1563 Loss: 2.301 | Acc: 11.503% (173/1504)\n",
            "Epoch 0 Step 47/1563 Loss: 2.300 | Acc: 11.589% (178/1536)\n",
            "Epoch 0 Step 48/1563 Loss: 2.300 | Acc: 11.735% (184/1568)\n",
            "Epoch 0 Step 49/1563 Loss: 2.300 | Acc: 11.688% (187/1600)\n",
            "Epoch 0 Step 50/1563 Loss: 2.300 | Acc: 11.581% (189/1632)\n",
            "Epoch 0 Step 51/1563 Loss: 2.300 | Acc: 11.659% (194/1664)\n",
            "Epoch 0 Step 52/1563 Loss: 2.300 | Acc: 11.733% (199/1696)\n",
            "Epoch 0 Step 53/1563 Loss: 2.300 | Acc: 11.632% (201/1728)\n",
            "Epoch 0 Step 54/1563 Loss: 2.300 | Acc: 11.705% (206/1760)\n",
            "Epoch 0 Step 55/1563 Loss: 2.300 | Acc: 11.719% (210/1792)\n",
            "Epoch 0 Step 56/1563 Loss: 2.300 | Acc: 11.787% (215/1824)\n",
            "Epoch 0 Step 57/1563 Loss: 2.300 | Acc: 11.853% (220/1856)\n",
            "Epoch 0 Step 58/1563 Loss: 2.299 | Acc: 11.917% (225/1888)\n",
            "Epoch 0 Step 59/1563 Loss: 2.299 | Acc: 11.875% (228/1920)\n",
            "Epoch 0 Step 60/1563 Loss: 2.300 | Acc: 11.885% (232/1952)\n",
            "Epoch 0 Step 61/1563 Loss: 2.299 | Acc: 11.946% (237/1984)\n",
            "Epoch 0 Step 62/1563 Loss: 2.299 | Acc: 11.806% (238/2016)\n",
            "Epoch 0 Step 63/1563 Loss: 2.299 | Acc: 11.865% (243/2048)\n",
            "Epoch 0 Step 64/1563 Loss: 2.299 | Acc: 11.971% (249/2080)\n",
            "Epoch 0 Step 65/1563 Loss: 2.299 | Acc: 11.884% (251/2112)\n",
            "Epoch 0 Step 66/1563 Loss: 2.299 | Acc: 11.894% (255/2144)\n",
            "Epoch 0 Step 67/1563 Loss: 2.299 | Acc: 12.086% (263/2176)\n",
            "Epoch 0 Step 68/1563 Loss: 2.298 | Acc: 12.183% (269/2208)\n",
            "Epoch 0 Step 69/1563 Loss: 2.298 | Acc: 12.277% (275/2240)\n",
            "Epoch 0 Step 70/1563 Loss: 2.298 | Acc: 12.324% (280/2272)\n",
            "Epoch 0 Step 71/1563 Loss: 2.298 | Acc: 12.283% (283/2304)\n",
            "Epoch 0 Step 72/1563 Loss: 2.298 | Acc: 12.500% (292/2336)\n",
            "Epoch 0 Step 73/1563 Loss: 2.298 | Acc: 12.669% (300/2368)\n",
            "Epoch 0 Step 74/1563 Loss: 2.298 | Acc: 12.583% (302/2400)\n",
            "Epoch 0 Step 75/1563 Loss: 2.297 | Acc: 12.541% (305/2432)\n",
            "Epoch 0 Step 76/1563 Loss: 2.297 | Acc: 12.541% (309/2464)\n",
            "Epoch 0 Step 77/1563 Loss: 2.297 | Acc: 12.540% (313/2496)\n",
            "Epoch 0 Step 78/1563 Loss: 2.297 | Acc: 12.500% (316/2528)\n",
            "Epoch 0 Step 79/1563 Loss: 2.297 | Acc: 12.617% (323/2560)\n",
            "Epoch 0 Step 80/1563 Loss: 2.297 | Acc: 12.577% (326/2592)\n",
            "Epoch 0 Step 81/1563 Loss: 2.297 | Acc: 12.500% (328/2624)\n",
            "Epoch 0 Step 82/1563 Loss: 2.297 | Acc: 12.425% (330/2656)\n",
            "Epoch 0 Step 83/1563 Loss: 2.296 | Acc: 12.500% (336/2688)\n",
            "Epoch 0 Step 84/1563 Loss: 2.296 | Acc: 12.574% (342/2720)\n",
            "Epoch 0 Step 85/1563 Loss: 2.296 | Acc: 12.682% (349/2752)\n",
            "Epoch 0 Step 86/1563 Loss: 2.296 | Acc: 12.644% (352/2784)\n",
            "Epoch 0 Step 87/1563 Loss: 2.296 | Acc: 12.571% (354/2816)\n",
            "Epoch 0 Step 88/1563 Loss: 2.296 | Acc: 12.640% (360/2848)\n",
            "Epoch 0 Step 89/1563 Loss: 2.296 | Acc: 12.708% (366/2880)\n",
            "Epoch 0 Step 90/1563 Loss: 2.296 | Acc: 12.603% (367/2912)\n",
            "Epoch 0 Step 91/1563 Loss: 2.296 | Acc: 12.534% (369/2944)\n",
            "Epoch 0 Step 92/1563 Loss: 2.296 | Acc: 12.433% (370/2976)\n",
            "Epoch 0 Step 93/1563 Loss: 2.295 | Acc: 12.566% (378/3008)\n",
            "Epoch 0 Step 94/1563 Loss: 2.295 | Acc: 12.566% (382/3040)\n",
            "Epoch 0 Step 95/1563 Loss: 2.295 | Acc: 12.630% (388/3072)\n",
            "Epoch 0 Step 96/1563 Loss: 2.295 | Acc: 12.629% (392/3104)\n",
            "Epoch 0 Step 97/1563 Loss: 2.295 | Acc: 12.564% (394/3136)\n",
            "Epoch 0 Step 98/1563 Loss: 2.294 | Acc: 12.626% (400/3168)\n",
            "Epoch 0 Step 99/1563 Loss: 2.294 | Acc: 12.625% (404/3200)\n",
            "Epoch 0 Step 100/1563 Loss: 2.294 | Acc: 12.624% (408/3232)\n",
            "Epoch 0 Step 101/1563 Loss: 2.294 | Acc: 12.592% (411/3264)\n",
            "Epoch 0 Step 102/1563 Loss: 2.294 | Acc: 12.712% (419/3296)\n",
            "Epoch 0 Step 103/1563 Loss: 2.294 | Acc: 12.710% (423/3328)\n",
            "Epoch 0 Step 104/1563 Loss: 2.294 | Acc: 12.738% (428/3360)\n",
            "Epoch 0 Step 105/1563 Loss: 2.294 | Acc: 12.765% (433/3392)\n",
            "Epoch 0 Step 106/1563 Loss: 2.293 | Acc: 12.938% (443/3424)\n",
            "Epoch 0 Step 107/1563 Loss: 2.293 | Acc: 13.079% (452/3456)\n",
            "Epoch 0 Step 108/1563 Loss: 2.293 | Acc: 13.102% (457/3488)\n",
            "Epoch 0 Step 109/1563 Loss: 2.293 | Acc: 13.153% (463/3520)\n",
            "Epoch 0 Step 110/1563 Loss: 2.293 | Acc: 13.119% (466/3552)\n",
            "Epoch 0 Step 111/1563 Loss: 2.293 | Acc: 13.114% (470/3584)\n",
            "Epoch 0 Step 112/1563 Loss: 2.293 | Acc: 13.081% (473/3616)\n",
            "Epoch 0 Step 113/1563 Loss: 2.293 | Acc: 13.103% (478/3648)\n",
            "Epoch 0 Step 114/1563 Loss: 2.292 | Acc: 13.098% (482/3680)\n",
            "Epoch 0 Step 115/1563 Loss: 2.292 | Acc: 13.039% (484/3712)\n",
            "Epoch 0 Step 116/1563 Loss: 2.292 | Acc: 13.114% (491/3744)\n",
            "Epoch 0 Step 117/1563 Loss: 2.292 | Acc: 13.162% (497/3776)\n",
            "Epoch 0 Step 118/1563 Loss: 2.292 | Acc: 13.209% (503/3808)\n",
            "Epoch 0 Step 119/1563 Loss: 2.291 | Acc: 13.307% (511/3840)\n",
            "Epoch 0 Step 120/1563 Loss: 2.291 | Acc: 13.275% (514/3872)\n",
            "Epoch 0 Step 121/1563 Loss: 2.291 | Acc: 13.371% (522/3904)\n",
            "Epoch 0 Step 122/1563 Loss: 2.291 | Acc: 13.415% (528/3936)\n",
            "Epoch 0 Step 123/1563 Loss: 2.291 | Acc: 13.458% (534/3968)\n",
            "Epoch 0 Step 124/1563 Loss: 2.291 | Acc: 13.500% (540/4000)\n",
            "Epoch 0 Step 125/1563 Loss: 2.291 | Acc: 13.566% (547/4032)\n",
            "Epoch 0 Step 126/1563 Loss: 2.291 | Acc: 13.558% (551/4064)\n",
            "Epoch 0 Step 127/1563 Loss: 2.291 | Acc: 13.525% (554/4096)\n",
            "Epoch 0 Step 128/1563 Loss: 2.290 | Acc: 13.590% (561/4128)\n",
            "Epoch 0 Step 129/1563 Loss: 2.290 | Acc: 13.630% (567/4160)\n",
            "Epoch 0 Step 130/1563 Loss: 2.290 | Acc: 13.669% (573/4192)\n",
            "Epoch 0 Step 131/1563 Loss: 2.290 | Acc: 13.684% (578/4224)\n",
            "Epoch 0 Step 132/1563 Loss: 2.290 | Acc: 13.816% (588/4256)\n",
            "Epoch 0 Step 133/1563 Loss: 2.290 | Acc: 13.876% (595/4288)\n",
            "Epoch 0 Step 134/1563 Loss: 2.289 | Acc: 13.843% (598/4320)\n",
            "Epoch 0 Step 135/1563 Loss: 2.289 | Acc: 13.879% (604/4352)\n",
            "Epoch 0 Step 136/1563 Loss: 2.289 | Acc: 13.937% (611/4384)\n",
            "Epoch 0 Step 137/1563 Loss: 2.289 | Acc: 14.017% (619/4416)\n",
            "Epoch 0 Step 138/1563 Loss: 2.289 | Acc: 13.916% (619/4448)\n",
            "Epoch 0 Step 139/1563 Loss: 2.289 | Acc: 13.996% (627/4480)\n",
            "Epoch 0 Step 140/1563 Loss: 2.288 | Acc: 14.029% (633/4512)\n",
            "Epoch 0 Step 141/1563 Loss: 2.288 | Acc: 14.018% (637/4544)\n",
            "Epoch 0 Step 142/1563 Loss: 2.288 | Acc: 14.052% (643/4576)\n",
            "Epoch 0 Step 143/1563 Loss: 2.288 | Acc: 14.062% (648/4608)\n",
            "Epoch 0 Step 144/1563 Loss: 2.288 | Acc: 14.138% (656/4640)\n",
            "Epoch 0 Step 145/1563 Loss: 2.287 | Acc: 14.170% (662/4672)\n",
            "Epoch 0 Step 146/1563 Loss: 2.287 | Acc: 14.264% (671/4704)\n",
            "Epoch 0 Step 147/1563 Loss: 2.287 | Acc: 14.231% (674/4736)\n",
            "Epoch 0 Step 148/1563 Loss: 2.287 | Acc: 14.283% (681/4768)\n",
            "Epoch 0 Step 149/1563 Loss: 2.287 | Acc: 14.354% (689/4800)\n",
            "Epoch 0 Step 150/1563 Loss: 2.287 | Acc: 14.404% (696/4832)\n",
            "Epoch 0 Step 151/1563 Loss: 2.286 | Acc: 14.330% (697/4864)\n",
            "Epoch 0 Step 152/1563 Loss: 2.286 | Acc: 14.400% (705/4896)\n",
            "Epoch 0 Step 153/1563 Loss: 2.286 | Acc: 14.387% (709/4928)\n",
            "Epoch 0 Step 154/1563 Loss: 2.286 | Acc: 14.435% (716/4960)\n",
            "Epoch 0 Step 155/1563 Loss: 2.286 | Acc: 14.463% (722/4992)\n",
            "Epoch 0 Step 156/1563 Loss: 2.286 | Acc: 14.471% (727/5024)\n",
            "Epoch 0 Step 157/1563 Loss: 2.286 | Acc: 14.498% (733/5056)\n",
            "Epoch 0 Step 158/1563 Loss: 2.286 | Acc: 14.564% (741/5088)\n",
            "Epoch 0 Step 159/1563 Loss: 2.285 | Acc: 14.629% (749/5120)\n",
            "Epoch 0 Step 160/1563 Loss: 2.285 | Acc: 14.732% (759/5152)\n",
            "Epoch 0 Step 161/1563 Loss: 2.285 | Acc: 14.699% (762/5184)\n",
            "Epoch 0 Step 162/1563 Loss: 2.285 | Acc: 14.705% (767/5216)\n",
            "Epoch 0 Step 163/1563 Loss: 2.285 | Acc: 14.729% (773/5248)\n",
            "Epoch 0 Step 164/1563 Loss: 2.285 | Acc: 14.716% (777/5280)\n",
            "Epoch 0 Step 165/1563 Loss: 2.284 | Acc: 14.778% (785/5312)\n",
            "Epoch 0 Step 166/1563 Loss: 2.284 | Acc: 14.783% (790/5344)\n",
            "Epoch 0 Step 167/1563 Loss: 2.284 | Acc: 14.807% (796/5376)\n",
            "Epoch 0 Step 168/1563 Loss: 2.284 | Acc: 14.867% (804/5408)\n",
            "Epoch 0 Step 169/1563 Loss: 2.284 | Acc: 14.890% (810/5440)\n",
            "Epoch 0 Step 170/1563 Loss: 2.284 | Acc: 14.894% (815/5472)\n",
            "Epoch 0 Step 171/1563 Loss: 2.284 | Acc: 14.989% (825/5504)\n",
            "Epoch 0 Step 172/1563 Loss: 2.283 | Acc: 14.993% (830/5536)\n",
            "Epoch 0 Step 173/1563 Loss: 2.283 | Acc: 14.960% (833/5568)\n",
            "Epoch 0 Step 174/1563 Loss: 2.283 | Acc: 15.018% (841/5600)\n",
            "Epoch 0 Step 175/1563 Loss: 2.283 | Acc: 15.146% (853/5632)\n",
            "Epoch 0 Step 176/1563 Loss: 2.283 | Acc: 15.148% (858/5664)\n",
            "Epoch 0 Step 177/1563 Loss: 2.282 | Acc: 15.204% (866/5696)\n",
            "Epoch 0 Step 178/1563 Loss: 2.282 | Acc: 15.206% (871/5728)\n",
            "Epoch 0 Step 179/1563 Loss: 2.282 | Acc: 15.226% (877/5760)\n",
            "Epoch 0 Step 180/1563 Loss: 2.282 | Acc: 15.262% (884/5792)\n",
            "Epoch 0 Step 181/1563 Loss: 2.281 | Acc: 15.316% (892/5824)\n",
            "Epoch 0 Step 182/1563 Loss: 2.281 | Acc: 15.318% (897/5856)\n",
            "Epoch 0 Step 183/1563 Loss: 2.281 | Acc: 15.438% (909/5888)\n",
            "Epoch 0 Step 184/1563 Loss: 2.281 | Acc: 15.439% (914/5920)\n",
            "Epoch 0 Step 185/1563 Loss: 2.281 | Acc: 15.423% (918/5952)\n",
            "Epoch 0 Step 186/1563 Loss: 2.281 | Acc: 15.408% (922/5984)\n",
            "Epoch 0 Step 187/1563 Loss: 2.280 | Acc: 15.392% (926/6016)\n",
            "Epoch 0 Step 188/1563 Loss: 2.280 | Acc: 15.410% (932/6048)\n",
            "Epoch 0 Step 189/1563 Loss: 2.280 | Acc: 15.378% (935/6080)\n",
            "Epoch 0 Step 190/1563 Loss: 2.280 | Acc: 15.380% (940/6112)\n",
            "Epoch 0 Step 191/1563 Loss: 2.280 | Acc: 15.462% (950/6144)\n",
            "Epoch 0 Step 192/1563 Loss: 2.279 | Acc: 15.528% (959/6176)\n",
            "Epoch 0 Step 193/1563 Loss: 2.279 | Acc: 15.544% (965/6208)\n",
            "Epoch 0 Step 194/1563 Loss: 2.279 | Acc: 15.577% (972/6240)\n",
            "Epoch 0 Step 195/1563 Loss: 2.278 | Acc: 15.577% (977/6272)\n",
            "Epoch 0 Step 196/1563 Loss: 2.278 | Acc: 15.625% (985/6304)\n",
            "Epoch 0 Step 197/1563 Loss: 2.278 | Acc: 15.641% (991/6336)\n",
            "Epoch 0 Step 198/1563 Loss: 2.278 | Acc: 15.656% (997/6368)\n",
            "Epoch 0 Step 199/1563 Loss: 2.277 | Acc: 15.672% (1003/6400)\n",
            "Epoch 0 Step 200/1563 Loss: 2.277 | Acc: 15.687% (1009/6432)\n",
            "Epoch 0 Step 201/1563 Loss: 2.277 | Acc: 15.671% (1013/6464)\n",
            "Epoch 0 Step 202/1563 Loss: 2.277 | Acc: 15.640% (1016/6496)\n",
            "Epoch 0 Step 203/1563 Loss: 2.277 | Acc: 15.610% (1019/6528)\n",
            "Epoch 0 Step 204/1563 Loss: 2.277 | Acc: 15.579% (1022/6560)\n",
            "Epoch 0 Step 205/1563 Loss: 2.276 | Acc: 15.564% (1026/6592)\n",
            "Epoch 0 Step 206/1563 Loss: 2.276 | Acc: 15.565% (1031/6624)\n",
            "Epoch 0 Step 207/1563 Loss: 2.276 | Acc: 15.640% (1041/6656)\n",
            "Epoch 0 Step 208/1563 Loss: 2.276 | Acc: 15.640% (1046/6688)\n",
            "Epoch 0 Step 209/1563 Loss: 2.276 | Acc: 15.640% (1051/6720)\n",
            "Epoch 0 Step 210/1563 Loss: 2.275 | Acc: 15.684% (1059/6752)\n",
            "Epoch 0 Step 211/1563 Loss: 2.275 | Acc: 15.669% (1063/6784)\n",
            "Epoch 0 Step 212/1563 Loss: 2.275 | Acc: 15.640% (1066/6816)\n",
            "Epoch 0 Step 213/1563 Loss: 2.274 | Acc: 15.625% (1070/6848)\n",
            "Epoch 0 Step 214/1563 Loss: 2.274 | Acc: 15.640% (1076/6880)\n",
            "Epoch 0 Step 215/1563 Loss: 2.274 | Acc: 15.668% (1083/6912)\n",
            "Epoch 0 Step 216/1563 Loss: 2.273 | Acc: 15.668% (1088/6944)\n",
            "Epoch 0 Step 217/1563 Loss: 2.273 | Acc: 15.711% (1096/6976)\n",
            "Epoch 0 Step 218/1563 Loss: 2.273 | Acc: 15.682% (1099/7008)\n",
            "Epoch 0 Step 219/1563 Loss: 2.273 | Acc: 15.682% (1104/7040)\n",
            "Epoch 0 Step 220/1563 Loss: 2.273 | Acc: 15.667% (1108/7072)\n",
            "Epoch 0 Step 221/1563 Loss: 2.272 | Acc: 15.695% (1115/7104)\n",
            "Epoch 0 Step 222/1563 Loss: 2.272 | Acc: 15.681% (1119/7136)\n",
            "Epoch 0 Step 223/1563 Loss: 2.272 | Acc: 15.709% (1126/7168)\n",
            "Epoch 0 Step 224/1563 Loss: 2.272 | Acc: 15.722% (1132/7200)\n",
            "Epoch 0 Step 225/1563 Loss: 2.272 | Acc: 15.777% (1141/7232)\n",
            "Epoch 0 Step 226/1563 Loss: 2.271 | Acc: 15.749% (1144/7264)\n",
            "Epoch 0 Step 227/1563 Loss: 2.271 | Acc: 15.748% (1149/7296)\n",
            "Epoch 0 Step 228/1563 Loss: 2.271 | Acc: 15.707% (1151/7328)\n",
            "Epoch 0 Step 229/1563 Loss: 2.271 | Acc: 15.720% (1157/7360)\n",
            "Epoch 0 Step 230/1563 Loss: 2.271 | Acc: 15.720% (1162/7392)\n",
            "Epoch 0 Step 231/1563 Loss: 2.271 | Acc: 15.733% (1168/7424)\n",
            "Epoch 0 Step 232/1563 Loss: 2.271 | Acc: 15.732% (1173/7456)\n",
            "Epoch 0 Step 233/1563 Loss: 2.270 | Acc: 15.745% (1179/7488)\n",
            "Epoch 0 Step 234/1563 Loss: 2.270 | Acc: 15.758% (1185/7520)\n",
            "Epoch 0 Step 235/1563 Loss: 2.270 | Acc: 15.771% (1191/7552)\n",
            "Epoch 0 Step 236/1563 Loss: 2.270 | Acc: 15.810% (1199/7584)\n",
            "Epoch 0 Step 237/1563 Loss: 2.270 | Acc: 15.809% (1204/7616)\n",
            "Epoch 0 Step 238/1563 Loss: 2.270 | Acc: 15.795% (1208/7648)\n",
            "Epoch 0 Step 239/1563 Loss: 2.270 | Acc: 15.768% (1211/7680)\n",
            "Epoch 0 Step 240/1563 Loss: 2.270 | Acc: 15.781% (1217/7712)\n",
            "Epoch 0 Step 241/1563 Loss: 2.269 | Acc: 15.819% (1225/7744)\n",
            "Epoch 0 Step 242/1563 Loss: 2.269 | Acc: 15.818% (1230/7776)\n",
            "Epoch 0 Step 243/1563 Loss: 2.269 | Acc: 15.817% (1235/7808)\n",
            "Epoch 0 Step 244/1563 Loss: 2.269 | Acc: 15.816% (1240/7840)\n",
            "Epoch 0 Step 245/1563 Loss: 2.268 | Acc: 15.854% (1248/7872)\n",
            "Epoch 0 Step 246/1563 Loss: 2.268 | Acc: 15.827% (1251/7904)\n",
            "Epoch 0 Step 247/1563 Loss: 2.268 | Acc: 15.852% (1258/7936)\n",
            "Epoch 0 Step 248/1563 Loss: 2.268 | Acc: 15.863% (1264/7968)\n",
            "Epoch 0 Step 249/1563 Loss: 2.267 | Acc: 15.863% (1269/8000)\n",
            "Epoch 0 Step 250/1563 Loss: 2.267 | Acc: 15.911% (1278/8032)\n",
            "Epoch 0 Step 251/1563 Loss: 2.267 | Acc: 15.885% (1281/8064)\n",
            "Epoch 0 Step 252/1563 Loss: 2.267 | Acc: 15.921% (1289/8096)\n",
            "Epoch 0 Step 253/1563 Loss: 2.267 | Acc: 15.908% (1293/8128)\n",
            "Epoch 0 Step 254/1563 Loss: 2.267 | Acc: 15.919% (1299/8160)\n",
            "Epoch 0 Step 255/1563 Loss: 2.267 | Acc: 15.894% (1302/8192)\n",
            "Epoch 0 Step 256/1563 Loss: 2.266 | Acc: 15.893% (1307/8224)\n",
            "Epoch 0 Step 257/1563 Loss: 2.266 | Acc: 15.940% (1316/8256)\n",
            "Epoch 0 Step 258/1563 Loss: 2.266 | Acc: 15.939% (1321/8288)\n",
            "Epoch 0 Step 259/1563 Loss: 2.266 | Acc: 15.889% (1322/8320)\n",
            "Epoch 0 Step 260/1563 Loss: 2.266 | Acc: 15.936% (1331/8352)\n",
            "Epoch 0 Step 261/1563 Loss: 2.265 | Acc: 15.971% (1339/8384)\n",
            "Epoch 0 Step 262/1563 Loss: 2.265 | Acc: 16.005% (1347/8416)\n",
            "Epoch 0 Step 263/1563 Loss: 2.265 | Acc: 16.039% (1355/8448)\n",
            "Epoch 0 Step 264/1563 Loss: 2.265 | Acc: 16.026% (1359/8480)\n",
            "Epoch 0 Step 265/1563 Loss: 2.264 | Acc: 16.071% (1368/8512)\n",
            "Epoch 0 Step 266/1563 Loss: 2.264 | Acc: 16.105% (1376/8544)\n",
            "Epoch 0 Step 267/1563 Loss: 2.264 | Acc: 16.126% (1383/8576)\n",
            "Epoch 0 Step 268/1563 Loss: 2.263 | Acc: 16.183% (1393/8608)\n",
            "Epoch 0 Step 269/1563 Loss: 2.263 | Acc: 16.169% (1397/8640)\n",
            "Epoch 0 Step 270/1563 Loss: 2.263 | Acc: 16.179% (1403/8672)\n",
            "Epoch 0 Step 271/1563 Loss: 2.263 | Acc: 16.234% (1413/8704)\n",
            "Epoch 0 Step 272/1563 Loss: 2.262 | Acc: 16.277% (1422/8736)\n",
            "Epoch 0 Step 273/1563 Loss: 2.262 | Acc: 16.275% (1427/8768)\n",
            "Epoch 0 Step 274/1563 Loss: 2.262 | Acc: 16.318% (1436/8800)\n",
            "Epoch 0 Step 275/1563 Loss: 2.262 | Acc: 16.316% (1441/8832)\n",
            "Epoch 0 Step 276/1563 Loss: 2.261 | Acc: 16.291% (1444/8864)\n",
            "Epoch 0 Step 277/1563 Loss: 2.261 | Acc: 16.277% (1448/8896)\n",
            "Epoch 0 Step 278/1563 Loss: 2.261 | Acc: 16.308% (1456/8928)\n",
            "Epoch 0 Step 279/1563 Loss: 2.261 | Acc: 16.339% (1464/8960)\n",
            "Epoch 0 Step 280/1563 Loss: 2.260 | Acc: 16.359% (1471/8992)\n",
            "Epoch 0 Step 281/1563 Loss: 2.260 | Acc: 16.390% (1479/9024)\n",
            "Epoch 0 Step 282/1563 Loss: 2.260 | Acc: 16.431% (1488/9056)\n",
            "Epoch 0 Step 283/1563 Loss: 2.259 | Acc: 16.450% (1495/9088)\n",
            "Epoch 0 Step 284/1563 Loss: 2.259 | Acc: 16.469% (1502/9120)\n",
            "Epoch 0 Step 285/1563 Loss: 2.259 | Acc: 16.477% (1508/9152)\n",
            "Epoch 0 Step 286/1563 Loss: 2.259 | Acc: 16.496% (1515/9184)\n",
            "Epoch 0 Step 287/1563 Loss: 2.258 | Acc: 16.504% (1521/9216)\n",
            "Epoch 0 Step 288/1563 Loss: 2.258 | Acc: 16.512% (1527/9248)\n",
            "Epoch 0 Step 289/1563 Loss: 2.258 | Acc: 16.476% (1529/9280)\n",
            "Epoch 0 Step 290/1563 Loss: 2.258 | Acc: 16.506% (1537/9312)\n",
            "Epoch 0 Step 291/1563 Loss: 2.258 | Acc: 16.535% (1545/9344)\n",
            "Epoch 0 Step 292/1563 Loss: 2.258 | Acc: 16.553% (1552/9376)\n",
            "Epoch 0 Step 293/1563 Loss: 2.258 | Acc: 16.507% (1553/9408)\n",
            "Epoch 0 Step 294/1563 Loss: 2.257 | Acc: 16.547% (1562/9440)\n",
            "Epoch 0 Step 295/1563 Loss: 2.257 | Acc: 16.533% (1566/9472)\n",
            "Epoch 0 Step 296/1563 Loss: 2.257 | Acc: 16.572% (1575/9504)\n",
            "Epoch 0 Step 297/1563 Loss: 2.257 | Acc: 16.569% (1580/9536)\n",
            "Epoch 0 Step 298/1563 Loss: 2.257 | Acc: 16.555% (1584/9568)\n",
            "Epoch 0 Step 299/1563 Loss: 2.257 | Acc: 16.604% (1594/9600)\n",
            "Epoch 0 Step 300/1563 Loss: 2.257 | Acc: 16.611% (1600/9632)\n",
            "Epoch 0 Step 301/1563 Loss: 2.256 | Acc: 16.608% (1605/9664)\n",
            "Epoch 0 Step 302/1563 Loss: 2.256 | Acc: 16.594% (1609/9696)\n",
            "Epoch 0 Step 303/1563 Loss: 2.256 | Acc: 16.612% (1616/9728)\n",
            "Epoch 0 Step 304/1563 Loss: 2.256 | Acc: 16.629% (1623/9760)\n",
            "Epoch 0 Step 305/1563 Loss: 2.256 | Acc: 16.636% (1629/9792)\n",
            "Epoch 0 Step 306/1563 Loss: 2.255 | Acc: 16.633% (1634/9824)\n",
            "Epoch 0 Step 307/1563 Loss: 2.255 | Acc: 16.660% (1642/9856)\n",
            "Epoch 0 Step 308/1563 Loss: 2.255 | Acc: 16.646% (1646/9888)\n",
            "Epoch 0 Step 309/1563 Loss: 2.255 | Acc: 16.673% (1654/9920)\n",
            "Epoch 0 Step 310/1563 Loss: 2.254 | Acc: 16.700% (1662/9952)\n",
            "Epoch 0 Step 311/1563 Loss: 2.254 | Acc: 16.757% (1673/9984)\n",
            "Epoch 0 Step 312/1563 Loss: 2.254 | Acc: 16.803% (1683/10016)\n",
            "Epoch 0 Step 313/1563 Loss: 2.253 | Acc: 16.819% (1690/10048)\n",
            "Epoch 0 Step 314/1563 Loss: 2.253 | Acc: 16.865% (1700/10080)\n",
            "Epoch 0 Step 315/1563 Loss: 2.253 | Acc: 16.871% (1706/10112)\n",
            "Epoch 0 Step 316/1563 Loss: 2.252 | Acc: 16.867% (1711/10144)\n",
            "Epoch 0 Step 317/1563 Loss: 2.252 | Acc: 16.853% (1715/10176)\n",
            "Epoch 0 Step 318/1563 Loss: 2.252 | Acc: 16.869% (1722/10208)\n",
            "Epoch 0 Step 319/1563 Loss: 2.252 | Acc: 16.904% (1731/10240)\n",
            "Epoch 0 Step 320/1563 Loss: 2.252 | Acc: 16.891% (1735/10272)\n",
            "Epoch 0 Step 321/1563 Loss: 2.251 | Acc: 16.916% (1743/10304)\n",
            "Epoch 0 Step 322/1563 Loss: 2.251 | Acc: 16.921% (1749/10336)\n",
            "Epoch 0 Step 323/1563 Loss: 2.251 | Acc: 16.898% (1752/10368)\n",
            "Epoch 0 Step 324/1563 Loss: 2.250 | Acc: 16.913% (1759/10400)\n",
            "Epoch 0 Step 325/1563 Loss: 2.250 | Acc: 16.967% (1770/10432)\n",
            "Epoch 0 Step 326/1563 Loss: 2.250 | Acc: 16.982% (1777/10464)\n",
            "Epoch 0 Step 327/1563 Loss: 2.249 | Acc: 16.978% (1782/10496)\n",
            "Epoch 0 Step 328/1563 Loss: 2.249 | Acc: 16.974% (1787/10528)\n",
            "Epoch 0 Step 329/1563 Loss: 2.249 | Acc: 16.979% (1793/10560)\n",
            "Epoch 0 Step 330/1563 Loss: 2.249 | Acc: 16.985% (1799/10592)\n",
            "Epoch 0 Step 331/1563 Loss: 2.248 | Acc: 16.971% (1803/10624)\n",
            "Epoch 0 Step 332/1563 Loss: 2.248 | Acc: 16.995% (1811/10656)\n",
            "Epoch 0 Step 333/1563 Loss: 2.248 | Acc: 16.982% (1815/10688)\n",
            "Epoch 0 Step 334/1563 Loss: 2.248 | Acc: 16.987% (1821/10720)\n",
            "Epoch 0 Step 335/1563 Loss: 2.248 | Acc: 17.048% (1833/10752)\n",
            "Epoch 0 Step 336/1563 Loss: 2.248 | Acc: 17.025% (1836/10784)\n",
            "Epoch 0 Step 337/1563 Loss: 2.248 | Acc: 17.021% (1841/10816)\n",
            "Epoch 0 Step 338/1563 Loss: 2.248 | Acc: 17.035% (1848/10848)\n",
            "Epoch 0 Step 339/1563 Loss: 2.247 | Acc: 17.059% (1856/10880)\n",
            "Epoch 0 Step 340/1563 Loss: 2.247 | Acc: 17.119% (1868/10912)\n",
            "Epoch 0 Step 341/1563 Loss: 2.247 | Acc: 17.105% (1872/10944)\n",
            "Epoch 0 Step 342/1563 Loss: 2.247 | Acc: 17.092% (1876/10976)\n",
            "Epoch 0 Step 343/1563 Loss: 2.247 | Acc: 17.106% (1883/11008)\n",
            "Epoch 0 Step 344/1563 Loss: 2.246 | Acc: 17.138% (1892/11040)\n",
            "Epoch 0 Step 345/1563 Loss: 2.246 | Acc: 17.124% (1896/11072)\n",
            "Epoch 0 Step 346/1563 Loss: 2.246 | Acc: 17.165% (1906/11104)\n",
            "Epoch 0 Step 347/1563 Loss: 2.246 | Acc: 17.170% (1912/11136)\n",
            "Epoch 0 Step 348/1563 Loss: 2.246 | Acc: 17.183% (1919/11168)\n",
            "Epoch 0 Step 349/1563 Loss: 2.245 | Acc: 17.196% (1926/11200)\n",
            "Epoch 0 Step 350/1563 Loss: 2.245 | Acc: 17.165% (1928/11232)\n",
            "Epoch 0 Step 351/1563 Loss: 2.245 | Acc: 17.170% (1934/11264)\n",
            "Epoch 0 Step 352/1563 Loss: 2.245 | Acc: 17.165% (1939/11296)\n",
            "Epoch 0 Step 353/1563 Loss: 2.245 | Acc: 17.152% (1943/11328)\n",
            "Epoch 0 Step 354/1563 Loss: 2.245 | Acc: 17.139% (1947/11360)\n",
            "Epoch 0 Step 355/1563 Loss: 2.244 | Acc: 17.161% (1955/11392)\n",
            "Epoch 0 Step 356/1563 Loss: 2.244 | Acc: 17.174% (1962/11424)\n",
            "Epoch 0 Step 357/1563 Loss: 2.244 | Acc: 17.188% (1969/11456)\n",
            "Epoch 0 Step 358/1563 Loss: 2.244 | Acc: 17.209% (1977/11488)\n",
            "Epoch 0 Step 359/1563 Loss: 2.244 | Acc: 17.205% (1982/11520)\n",
            "Epoch 0 Step 360/1563 Loss: 2.243 | Acc: 17.200% (1987/11552)\n",
            "Epoch 0 Step 361/1563 Loss: 2.243 | Acc: 17.231% (1996/11584)\n",
            "Epoch 0 Step 362/1563 Loss: 2.243 | Acc: 17.235% (2002/11616)\n",
            "Epoch 0 Step 363/1563 Loss: 2.242 | Acc: 17.222% (2006/11648)\n",
            "Epoch 0 Step 364/1563 Loss: 2.242 | Acc: 17.235% (2013/11680)\n",
            "Epoch 0 Step 365/1563 Loss: 2.242 | Acc: 17.247% (2020/11712)\n",
            "Epoch 0 Step 366/1563 Loss: 2.242 | Acc: 17.268% (2028/11744)\n",
            "Epoch 0 Step 367/1563 Loss: 2.242 | Acc: 17.306% (2038/11776)\n",
            "Epoch 0 Step 368/1563 Loss: 2.242 | Acc: 17.293% (2042/11808)\n",
            "Epoch 0 Step 369/1563 Loss: 2.241 | Acc: 17.280% (2046/11840)\n",
            "Epoch 0 Step 370/1563 Loss: 2.242 | Acc: 17.242% (2047/11872)\n",
            "Epoch 0 Step 371/1563 Loss: 2.241 | Acc: 17.272% (2056/11904)\n",
            "Epoch 0 Step 372/1563 Loss: 2.241 | Acc: 17.242% (2058/11936)\n",
            "Epoch 0 Step 373/1563 Loss: 2.241 | Acc: 17.254% (2065/11968)\n",
            "Epoch 0 Step 374/1563 Loss: 2.240 | Acc: 17.267% (2072/12000)\n",
            "Epoch 0 Step 375/1563 Loss: 2.240 | Acc: 17.279% (2079/12032)\n",
            "Epoch 0 Step 376/1563 Loss: 2.240 | Acc: 17.266% (2083/12064)\n",
            "Epoch 0 Step 377/1563 Loss: 2.240 | Acc: 17.254% (2087/12096)\n",
            "Epoch 0 Step 378/1563 Loss: 2.239 | Acc: 17.241% (2091/12128)\n",
            "Epoch 0 Step 379/1563 Loss: 2.239 | Acc: 17.229% (2095/12160)\n",
            "Epoch 0 Step 380/1563 Loss: 2.239 | Acc: 17.233% (2101/12192)\n",
            "Epoch 0 Step 381/1563 Loss: 2.238 | Acc: 17.253% (2109/12224)\n",
            "Epoch 0 Step 382/1563 Loss: 2.238 | Acc: 17.241% (2113/12256)\n",
            "Epoch 0 Step 383/1563 Loss: 2.238 | Acc: 17.244% (2119/12288)\n",
            "Epoch 0 Step 384/1563 Loss: 2.238 | Acc: 17.232% (2123/12320)\n",
            "Epoch 0 Step 385/1563 Loss: 2.238 | Acc: 17.212% (2126/12352)\n",
            "Epoch 0 Step 386/1563 Loss: 2.238 | Acc: 17.216% (2132/12384)\n",
            "Epoch 0 Step 387/1563 Loss: 2.238 | Acc: 17.204% (2136/12416)\n",
            "Epoch 0 Step 388/1563 Loss: 2.238 | Acc: 17.232% (2145/12448)\n",
            "Epoch 0 Step 389/1563 Loss: 2.237 | Acc: 17.260% (2154/12480)\n",
            "Epoch 0 Step 390/1563 Loss: 2.237 | Acc: 17.271% (2161/12512)\n",
            "Epoch 0 Step 391/1563 Loss: 2.237 | Acc: 17.267% (2166/12544)\n",
            "Epoch 0 Step 392/1563 Loss: 2.236 | Acc: 17.287% (2174/12576)\n",
            "Epoch 0 Step 393/1563 Loss: 2.236 | Acc: 17.291% (2180/12608)\n",
            "Epoch 0 Step 394/1563 Loss: 2.236 | Acc: 17.294% (2186/12640)\n",
            "Epoch 0 Step 395/1563 Loss: 2.236 | Acc: 17.282% (2190/12672)\n",
            "Epoch 0 Step 396/1563 Loss: 2.235 | Acc: 17.302% (2198/12704)\n",
            "Epoch 0 Step 397/1563 Loss: 2.235 | Acc: 17.329% (2207/12736)\n",
            "Epoch 0 Step 398/1563 Loss: 2.235 | Acc: 17.364% (2217/12768)\n",
            "Epoch 0 Step 399/1563 Loss: 2.235 | Acc: 17.383% (2225/12800)\n",
            "Epoch 0 Step 400/1563 Loss: 2.234 | Acc: 17.402% (2233/12832)\n",
            "Epoch 0 Step 401/1563 Loss: 2.234 | Acc: 17.428% (2242/12864)\n",
            "Epoch 0 Step 402/1563 Loss: 2.234 | Acc: 17.440% (2249/12896)\n",
            "Epoch 0 Step 403/1563 Loss: 2.234 | Acc: 17.443% (2255/12928)\n",
            "Epoch 0 Step 404/1563 Loss: 2.233 | Acc: 17.438% (2260/12960)\n",
            "Epoch 0 Step 405/1563 Loss: 2.233 | Acc: 17.457% (2268/12992)\n",
            "Epoch 0 Step 406/1563 Loss: 2.233 | Acc: 17.468% (2275/13024)\n",
            "Epoch 0 Step 407/1563 Loss: 2.233 | Acc: 17.456% (2279/13056)\n",
            "Epoch 0 Step 408/1563 Loss: 2.233 | Acc: 17.489% (2289/13088)\n",
            "Epoch 0 Step 409/1563 Loss: 2.233 | Acc: 17.485% (2294/13120)\n",
            "Epoch 0 Step 410/1563 Loss: 2.233 | Acc: 17.465% (2297/13152)\n",
            "Epoch 0 Step 411/1563 Loss: 2.232 | Acc: 17.476% (2304/13184)\n",
            "Epoch 0 Step 412/1563 Loss: 2.232 | Acc: 17.486% (2311/13216)\n",
            "Epoch 0 Step 413/1563 Loss: 2.232 | Acc: 17.497% (2318/13248)\n",
            "Epoch 0 Step 414/1563 Loss: 2.232 | Acc: 17.500% (2324/13280)\n",
            "Epoch 0 Step 415/1563 Loss: 2.231 | Acc: 17.518% (2332/13312)\n",
            "Epoch 0 Step 416/1563 Loss: 2.231 | Acc: 17.499% (2335/13344)\n",
            "Epoch 0 Step 417/1563 Loss: 2.232 | Acc: 17.479% (2338/13376)\n",
            "Epoch 0 Step 418/1563 Loss: 2.231 | Acc: 17.452% (2340/13408)\n",
            "Epoch 0 Step 419/1563 Loss: 2.231 | Acc: 17.426% (2342/13440)\n",
            "Epoch 0 Step 420/1563 Loss: 2.231 | Acc: 17.406% (2345/13472)\n",
            "Epoch 0 Step 421/1563 Loss: 2.231 | Acc: 17.432% (2354/13504)\n",
            "Epoch 0 Step 422/1563 Loss: 2.230 | Acc: 17.465% (2364/13536)\n",
            "Epoch 0 Step 423/1563 Loss: 2.230 | Acc: 17.460% (2369/13568)\n",
            "Epoch 0 Step 424/1563 Loss: 2.230 | Acc: 17.471% (2376/13600)\n",
            "Epoch 0 Step 425/1563 Loss: 2.230 | Acc: 17.466% (2381/13632)\n",
            "Epoch 0 Step 426/1563 Loss: 2.230 | Acc: 17.484% (2389/13664)\n",
            "Epoch 0 Step 427/1563 Loss: 2.229 | Acc: 17.494% (2396/13696)\n",
            "Epoch 0 Step 428/1563 Loss: 2.229 | Acc: 17.490% (2401/13728)\n",
            "Epoch 0 Step 429/1563 Loss: 2.229 | Acc: 17.500% (2408/13760)\n",
            "Epoch 0 Step 430/1563 Loss: 2.229 | Acc: 17.488% (2412/13792)\n",
            "Epoch 0 Step 431/1563 Loss: 2.229 | Acc: 17.506% (2420/13824)\n",
            "Epoch 0 Step 432/1563 Loss: 2.229 | Acc: 17.501% (2425/13856)\n",
            "Epoch 0 Step 433/1563 Loss: 2.229 | Acc: 17.490% (2429/13888)\n",
            "Epoch 0 Step 434/1563 Loss: 2.229 | Acc: 17.507% (2437/13920)\n",
            "Epoch 0 Step 435/1563 Loss: 2.228 | Acc: 17.510% (2443/13952)\n",
            "Epoch 0 Step 436/1563 Loss: 2.228 | Acc: 17.520% (2450/13984)\n",
            "Epoch 0 Step 437/1563 Loss: 2.228 | Acc: 17.501% (2453/14016)\n",
            "Epoch 0 Step 438/1563 Loss: 2.227 | Acc: 17.497% (2458/14048)\n",
            "Epoch 0 Step 439/1563 Loss: 2.227 | Acc: 17.472% (2460/14080)\n",
            "Epoch 0 Step 440/1563 Loss: 2.227 | Acc: 17.474% (2466/14112)\n",
            "Epoch 0 Step 441/1563 Loss: 2.227 | Acc: 17.463% (2470/14144)\n",
            "Epoch 0 Step 442/1563 Loss: 2.227 | Acc: 17.487% (2479/14176)\n",
            "Epoch 0 Step 443/1563 Loss: 2.227 | Acc: 17.490% (2485/14208)\n",
            "Epoch 0 Step 444/1563 Loss: 2.226 | Acc: 17.514% (2494/14240)\n",
            "Epoch 0 Step 445/1563 Loss: 2.226 | Acc: 17.538% (2503/14272)\n",
            "Epoch 0 Step 446/1563 Loss: 2.226 | Acc: 17.541% (2509/14304)\n",
            "Epoch 0 Step 447/1563 Loss: 2.225 | Acc: 17.550% (2516/14336)\n",
            "Epoch 0 Step 448/1563 Loss: 2.225 | Acc: 17.567% (2524/14368)\n",
            "Epoch 0 Step 449/1563 Loss: 2.225 | Acc: 17.556% (2528/14400)\n",
            "Epoch 0 Step 450/1563 Loss: 2.225 | Acc: 17.586% (2538/14432)\n",
            "Epoch 0 Step 451/1563 Loss: 2.225 | Acc: 17.582% (2543/14464)\n",
            "Epoch 0 Step 452/1563 Loss: 2.224 | Acc: 17.612% (2553/14496)\n",
            "Epoch 0 Step 453/1563 Loss: 2.224 | Acc: 17.607% (2558/14528)\n",
            "Epoch 0 Step 454/1563 Loss: 2.224 | Acc: 17.630% (2567/14560)\n",
            "Epoch 0 Step 455/1563 Loss: 2.224 | Acc: 17.640% (2574/14592)\n",
            "Epoch 0 Step 456/1563 Loss: 2.223 | Acc: 17.629% (2578/14624)\n",
            "Epoch 0 Step 457/1563 Loss: 2.223 | Acc: 17.638% (2585/14656)\n",
            "Epoch 0 Step 458/1563 Loss: 2.223 | Acc: 17.647% (2592/14688)\n",
            "Epoch 0 Step 459/1563 Loss: 2.223 | Acc: 17.656% (2599/14720)\n",
            "Epoch 0 Step 460/1563 Loss: 2.222 | Acc: 17.679% (2608/14752)\n",
            "Epoch 0 Step 461/1563 Loss: 2.222 | Acc: 17.688% (2615/14784)\n",
            "Epoch 0 Step 462/1563 Loss: 2.221 | Acc: 17.697% (2622/14816)\n",
            "Epoch 0 Step 463/1563 Loss: 2.221 | Acc: 17.740% (2634/14848)\n",
            "Epoch 0 Step 464/1563 Loss: 2.221 | Acc: 17.755% (2642/14880)\n",
            "Epoch 0 Step 465/1563 Loss: 2.221 | Acc: 17.737% (2645/14912)\n",
            "Epoch 0 Step 466/1563 Loss: 2.221 | Acc: 17.733% (2650/14944)\n",
            "Epoch 0 Step 467/1563 Loss: 2.220 | Acc: 17.735% (2656/14976)\n",
            "Epoch 0 Step 468/1563 Loss: 2.220 | Acc: 17.744% (2663/15008)\n",
            "Epoch 0 Step 469/1563 Loss: 2.220 | Acc: 17.766% (2672/15040)\n",
            "Epoch 0 Step 470/1563 Loss: 2.220 | Acc: 17.788% (2681/15072)\n",
            "Epoch 0 Step 471/1563 Loss: 2.220 | Acc: 17.783% (2686/15104)\n",
            "Epoch 0 Step 472/1563 Loss: 2.220 | Acc: 17.799% (2694/15136)\n",
            "Epoch 0 Step 473/1563 Loss: 2.220 | Acc: 17.820% (2703/15168)\n",
            "Epoch 0 Step 474/1563 Loss: 2.220 | Acc: 17.829% (2710/15200)\n",
            "Epoch 0 Step 475/1563 Loss: 2.219 | Acc: 17.831% (2716/15232)\n",
            "Epoch 0 Step 476/1563 Loss: 2.219 | Acc: 17.839% (2723/15264)\n",
            "Epoch 0 Step 477/1563 Loss: 2.219 | Acc: 17.841% (2729/15296)\n",
            "Epoch 0 Step 478/1563 Loss: 2.219 | Acc: 17.856% (2737/15328)\n",
            "Epoch 0 Step 479/1563 Loss: 2.219 | Acc: 17.865% (2744/15360)\n",
            "Epoch 0 Step 480/1563 Loss: 2.218 | Acc: 17.866% (2750/15392)\n",
            "Epoch 0 Step 481/1563 Loss: 2.219 | Acc: 17.855% (2754/15424)\n",
            "Epoch 0 Step 482/1563 Loss: 2.218 | Acc: 17.864% (2761/15456)\n",
            "Epoch 0 Step 483/1563 Loss: 2.218 | Acc: 17.878% (2769/15488)\n",
            "Epoch 0 Step 484/1563 Loss: 2.218 | Acc: 17.848% (2770/15520)\n",
            "Epoch 0 Step 485/1563 Loss: 2.218 | Acc: 17.831% (2773/15552)\n",
            "Epoch 0 Step 486/1563 Loss: 2.218 | Acc: 17.832% (2779/15584)\n",
            "Epoch 0 Step 487/1563 Loss: 2.218 | Acc: 17.841% (2786/15616)\n",
            "Epoch 0 Step 488/1563 Loss: 2.218 | Acc: 17.830% (2790/15648)\n",
            "Epoch 0 Step 489/1563 Loss: 2.217 | Acc: 17.857% (2800/15680)\n",
            "Epoch 0 Step 490/1563 Loss: 2.217 | Acc: 17.884% (2810/15712)\n",
            "Epoch 0 Step 491/1563 Loss: 2.217 | Acc: 17.893% (2817/15744)\n",
            "Epoch 0 Step 492/1563 Loss: 2.217 | Acc: 17.894% (2823/15776)\n",
            "Epoch 0 Step 493/1563 Loss: 2.217 | Acc: 17.883% (2827/15808)\n",
            "Epoch 0 Step 494/1563 Loss: 2.217 | Acc: 17.910% (2837/15840)\n",
            "Epoch 0 Step 495/1563 Loss: 2.217 | Acc: 17.937% (2847/15872)\n",
            "Epoch 0 Step 496/1563 Loss: 2.216 | Acc: 17.939% (2853/15904)\n",
            "Epoch 0 Step 497/1563 Loss: 2.216 | Acc: 17.922% (2856/15936)\n",
            "Epoch 0 Step 498/1563 Loss: 2.216 | Acc: 17.917% (2861/15968)\n",
            "Epoch 0 Step 499/1563 Loss: 2.216 | Acc: 17.919% (2867/16000)\n",
            "Epoch 0 Step 500/1563 Loss: 2.216 | Acc: 17.902% (2870/16032)\n",
            "Epoch 0 Step 501/1563 Loss: 2.216 | Acc: 17.885% (2873/16064)\n",
            "Epoch 0 Step 502/1563 Loss: 2.216 | Acc: 17.893% (2880/16096)\n",
            "Epoch 0 Step 503/1563 Loss: 2.215 | Acc: 17.882% (2884/16128)\n",
            "Epoch 0 Step 504/1563 Loss: 2.215 | Acc: 17.896% (2892/16160)\n",
            "Epoch 0 Step 505/1563 Loss: 2.215 | Acc: 17.892% (2897/16192)\n",
            "Epoch 0 Step 506/1563 Loss: 2.215 | Acc: 17.893% (2903/16224)\n",
            "Epoch 0 Step 507/1563 Loss: 2.215 | Acc: 17.907% (2911/16256)\n",
            "Epoch 0 Step 508/1563 Loss: 2.215 | Acc: 17.903% (2916/16288)\n",
            "Epoch 0 Step 509/1563 Loss: 2.215 | Acc: 17.892% (2920/16320)\n",
            "Epoch 0 Step 510/1563 Loss: 2.214 | Acc: 17.900% (2927/16352)\n",
            "Epoch 0 Step 511/1563 Loss: 2.214 | Acc: 17.908% (2934/16384)\n",
            "Epoch 0 Step 512/1563 Loss: 2.214 | Acc: 17.934% (2944/16416)\n",
            "Epoch 0 Step 513/1563 Loss: 2.214 | Acc: 17.941% (2951/16448)\n",
            "Epoch 0 Step 514/1563 Loss: 2.214 | Acc: 17.973% (2962/16480)\n",
            "Epoch 0 Step 515/1563 Loss: 2.214 | Acc: 17.981% (2969/16512)\n",
            "Epoch 0 Step 516/1563 Loss: 2.213 | Acc: 17.988% (2976/16544)\n",
            "Epoch 0 Step 517/1563 Loss: 2.213 | Acc: 18.020% (2987/16576)\n",
            "Epoch 0 Step 518/1563 Loss: 2.213 | Acc: 18.052% (2998/16608)\n",
            "Epoch 0 Step 519/1563 Loss: 2.213 | Acc: 18.065% (3006/16640)\n",
            "Epoch 0 Step 520/1563 Loss: 2.212 | Acc: 18.090% (3016/16672)\n",
            "Epoch 0 Step 521/1563 Loss: 2.212 | Acc: 18.091% (3022/16704)\n",
            "Epoch 0 Step 522/1563 Loss: 2.212 | Acc: 18.111% (3031/16736)\n",
            "Epoch 0 Step 523/1563 Loss: 2.212 | Acc: 18.124% (3039/16768)\n",
            "Epoch 0 Step 524/1563 Loss: 2.212 | Acc: 18.101% (3041/16800)\n",
            "Epoch 0 Step 525/1563 Loss: 2.212 | Acc: 18.114% (3049/16832)\n",
            "Epoch 0 Step 526/1563 Loss: 2.211 | Acc: 18.127% (3057/16864)\n",
            "Epoch 0 Step 527/1563 Loss: 2.211 | Acc: 18.146% (3066/16896)\n",
            "Epoch 0 Step 528/1563 Loss: 2.211 | Acc: 18.159% (3074/16928)\n",
            "Epoch 0 Step 529/1563 Loss: 2.210 | Acc: 18.166% (3081/16960)\n",
            "Epoch 0 Step 530/1563 Loss: 2.210 | Acc: 18.185% (3090/16992)\n",
            "Epoch 0 Step 531/1563 Loss: 2.210 | Acc: 18.186% (3096/17024)\n",
            "Epoch 0 Step 532/1563 Loss: 2.210 | Acc: 18.187% (3102/17056)\n",
            "Epoch 0 Step 533/1563 Loss: 2.209 | Acc: 18.212% (3112/17088)\n",
            "Epoch 0 Step 534/1563 Loss: 2.209 | Acc: 18.201% (3116/17120)\n",
            "Epoch 0 Step 535/1563 Loss: 2.209 | Acc: 18.225% (3126/17152)\n",
            "Epoch 0 Step 536/1563 Loss: 2.209 | Acc: 18.238% (3134/17184)\n",
            "Epoch 0 Step 537/1563 Loss: 2.209 | Acc: 18.250% (3142/17216)\n",
            "Epoch 0 Step 538/1563 Loss: 2.208 | Acc: 18.269% (3151/17248)\n",
            "Epoch 0 Step 539/1563 Loss: 2.208 | Acc: 18.293% (3161/17280)\n",
            "Epoch 0 Step 540/1563 Loss: 2.208 | Acc: 18.276% (3164/17312)\n",
            "Epoch 0 Step 541/1563 Loss: 2.208 | Acc: 18.277% (3170/17344)\n",
            "Epoch 0 Step 542/1563 Loss: 2.208 | Acc: 18.261% (3173/17376)\n",
            "Epoch 0 Step 543/1563 Loss: 2.208 | Acc: 18.262% (3179/17408)\n",
            "Epoch 0 Step 544/1563 Loss: 2.208 | Acc: 18.245% (3182/17440)\n",
            "Epoch 0 Step 545/1563 Loss: 2.207 | Acc: 18.252% (3189/17472)\n",
            "Epoch 0 Step 546/1563 Loss: 2.207 | Acc: 18.270% (3198/17504)\n",
            "Epoch 0 Step 547/1563 Loss: 2.207 | Acc: 18.294% (3208/17536)\n",
            "Epoch 0 Step 548/1563 Loss: 2.206 | Acc: 18.306% (3216/17568)\n",
            "Epoch 0 Step 549/1563 Loss: 2.206 | Acc: 18.284% (3218/17600)\n",
            "Epoch 0 Step 550/1563 Loss: 2.206 | Acc: 18.268% (3221/17632)\n",
            "Epoch 0 Step 551/1563 Loss: 2.206 | Acc: 18.274% (3228/17664)\n",
            "Epoch 0 Step 552/1563 Loss: 2.206 | Acc: 18.270% (3233/17696)\n",
            "Epoch 0 Step 553/1563 Loss: 2.206 | Acc: 18.293% (3243/17728)\n",
            "Epoch 0 Step 554/1563 Loss: 2.205 | Acc: 18.339% (3257/17760)\n",
            "Epoch 0 Step 555/1563 Loss: 2.205 | Acc: 18.357% (3266/17792)\n",
            "Epoch 0 Step 556/1563 Loss: 2.205 | Acc: 18.346% (3270/17824)\n",
            "Epoch 0 Step 557/1563 Loss: 2.205 | Acc: 18.352% (3277/17856)\n",
            "Epoch 0 Step 558/1563 Loss: 2.205 | Acc: 18.347% (3282/17888)\n",
            "Epoch 0 Step 559/1563 Loss: 2.205 | Acc: 18.359% (3290/17920)\n",
            "Epoch 0 Step 560/1563 Loss: 2.205 | Acc: 18.349% (3294/17952)\n",
            "Epoch 0 Step 561/1563 Loss: 2.204 | Acc: 18.339% (3298/17984)\n",
            "Epoch 0 Step 562/1563 Loss: 2.204 | Acc: 18.356% (3307/18016)\n",
            "Epoch 0 Step 563/1563 Loss: 2.204 | Acc: 18.368% (3315/18048)\n",
            "Epoch 0 Step 564/1563 Loss: 2.204 | Acc: 18.363% (3320/18080)\n",
            "Epoch 0 Step 565/1563 Loss: 2.204 | Acc: 18.369% (3327/18112)\n",
            "Epoch 0 Step 566/1563 Loss: 2.204 | Acc: 18.370% (3333/18144)\n",
            "Epoch 0 Step 567/1563 Loss: 2.203 | Acc: 18.387% (3342/18176)\n",
            "Epoch 0 Step 568/1563 Loss: 2.203 | Acc: 18.388% (3348/18208)\n",
            "Epoch 0 Step 569/1563 Loss: 2.203 | Acc: 18.394% (3355/18240)\n",
            "Epoch 0 Step 570/1563 Loss: 2.203 | Acc: 18.405% (3363/18272)\n",
            "Epoch 0 Step 571/1563 Loss: 2.203 | Acc: 18.406% (3369/18304)\n",
            "Epoch 0 Step 572/1563 Loss: 2.203 | Acc: 18.423% (3378/18336)\n",
            "Epoch 0 Step 573/1563 Loss: 2.203 | Acc: 18.423% (3384/18368)\n",
            "Epoch 0 Step 574/1563 Loss: 2.203 | Acc: 18.408% (3387/18400)\n",
            "Epoch 0 Step 575/1563 Loss: 2.203 | Acc: 18.430% (3397/18432)\n",
            "Epoch 0 Step 576/1563 Loss: 2.202 | Acc: 18.458% (3408/18464)\n",
            "Epoch 0 Step 577/1563 Loss: 2.202 | Acc: 18.458% (3414/18496)\n",
            "Epoch 0 Step 578/1563 Loss: 2.202 | Acc: 18.464% (3421/18528)\n",
            "Epoch 0 Step 579/1563 Loss: 2.202 | Acc: 18.486% (3431/18560)\n",
            "Epoch 0 Step 580/1563 Loss: 2.202 | Acc: 18.497% (3439/18592)\n",
            "Epoch 0 Step 581/1563 Loss: 2.202 | Acc: 18.482% (3442/18624)\n",
            "Epoch 0 Step 582/1563 Loss: 2.202 | Acc: 18.487% (3449/18656)\n",
            "Epoch 0 Step 583/1563 Loss: 2.201 | Acc: 18.499% (3457/18688)\n",
            "Epoch 0 Step 584/1563 Loss: 2.201 | Acc: 18.504% (3464/18720)\n",
            "Epoch 0 Step 585/1563 Loss: 2.201 | Acc: 18.515% (3472/18752)\n",
            "Epoch 0 Step 586/1563 Loss: 2.201 | Acc: 18.510% (3477/18784)\n",
            "Epoch 0 Step 587/1563 Loss: 2.200 | Acc: 18.532% (3487/18816)\n",
            "Epoch 0 Step 588/1563 Loss: 2.200 | Acc: 18.517% (3490/18848)\n",
            "Epoch 0 Step 589/1563 Loss: 2.200 | Acc: 18.501% (3493/18880)\n",
            "Epoch 0 Step 590/1563 Loss: 2.200 | Acc: 18.501% (3499/18912)\n",
            "Epoch 0 Step 591/1563 Loss: 2.200 | Acc: 18.502% (3505/18944)\n",
            "Epoch 0 Step 592/1563 Loss: 2.200 | Acc: 18.513% (3513/18976)\n",
            "Epoch 0 Step 593/1563 Loss: 2.200 | Acc: 18.513% (3519/19008)\n",
            "Epoch 0 Step 594/1563 Loss: 2.200 | Acc: 18.519% (3526/19040)\n",
            "Epoch 0 Step 595/1563 Loss: 2.199 | Acc: 18.535% (3535/19072)\n",
            "Epoch 0 Step 596/1563 Loss: 2.199 | Acc: 18.546% (3543/19104)\n",
            "Epoch 0 Step 597/1563 Loss: 2.199 | Acc: 18.531% (3546/19136)\n",
            "Epoch 0 Step 598/1563 Loss: 2.199 | Acc: 18.500% (3546/19168)\n",
            "Epoch 0 Step 599/1563 Loss: 2.199 | Acc: 18.484% (3549/19200)\n",
            "Epoch 0 Step 600/1563 Loss: 2.199 | Acc: 18.490% (3556/19232)\n",
            "Epoch 0 Step 601/1563 Loss: 2.199 | Acc: 18.496% (3563/19264)\n",
            "Epoch 0 Step 602/1563 Loss: 2.199 | Acc: 18.517% (3573/19296)\n",
            "Epoch 0 Step 603/1563 Loss: 2.199 | Acc: 18.528% (3581/19328)\n",
            "Epoch 0 Step 604/1563 Loss: 2.198 | Acc: 18.543% (3590/19360)\n",
            "Epoch 0 Step 605/1563 Loss: 2.198 | Acc: 18.544% (3596/19392)\n",
            "Epoch 0 Step 606/1563 Loss: 2.198 | Acc: 18.560% (3605/19424)\n",
            "Epoch 0 Step 607/1563 Loss: 2.198 | Acc: 18.560% (3611/19456)\n",
            "Epoch 0 Step 608/1563 Loss: 2.198 | Acc: 18.555% (3616/19488)\n",
            "Epoch 0 Step 609/1563 Loss: 2.198 | Acc: 18.555% (3622/19520)\n",
            "Epoch 0 Step 610/1563 Loss: 2.198 | Acc: 18.545% (3626/19552)\n",
            "Epoch 0 Step 611/1563 Loss: 2.197 | Acc: 18.556% (3634/19584)\n",
            "Epoch 0 Step 612/1563 Loss: 2.197 | Acc: 18.551% (3639/19616)\n",
            "Epoch 0 Step 613/1563 Loss: 2.197 | Acc: 18.562% (3647/19648)\n",
            "Epoch 0 Step 614/1563 Loss: 2.197 | Acc: 18.542% (3649/19680)\n",
            "Epoch 0 Step 615/1563 Loss: 2.197 | Acc: 18.557% (3658/19712)\n",
            "Epoch 0 Step 616/1563 Loss: 2.197 | Acc: 18.573% (3667/19744)\n",
            "Epoch 0 Step 617/1563 Loss: 2.197 | Acc: 18.573% (3673/19776)\n",
            "Epoch 0 Step 618/1563 Loss: 2.197 | Acc: 18.568% (3678/19808)\n",
            "Epoch 0 Step 619/1563 Loss: 2.197 | Acc: 18.574% (3685/19840)\n",
            "Epoch 0 Step 620/1563 Loss: 2.197 | Acc: 18.589% (3694/19872)\n",
            "Epoch 0 Step 621/1563 Loss: 2.196 | Acc: 18.614% (3705/19904)\n",
            "Epoch 0 Step 622/1563 Loss: 2.196 | Acc: 18.625% (3713/19936)\n",
            "Epoch 0 Step 623/1563 Loss: 2.196 | Acc: 18.615% (3717/19968)\n",
            "Epoch 0 Step 624/1563 Loss: 2.196 | Acc: 18.615% (3723/20000)\n",
            "Epoch 0 Step 625/1563 Loss: 2.196 | Acc: 18.625% (3731/20032)\n",
            "Epoch 0 Step 626/1563 Loss: 2.196 | Acc: 18.645% (3741/20064)\n",
            "Epoch 0 Step 627/1563 Loss: 2.196 | Acc: 18.655% (3749/20096)\n",
            "Epoch 0 Step 628/1563 Loss: 2.196 | Acc: 18.675% (3759/20128)\n",
            "Epoch 0 Step 629/1563 Loss: 2.196 | Acc: 18.661% (3762/20160)\n",
            "Epoch 0 Step 630/1563 Loss: 2.195 | Acc: 18.671% (3770/20192)\n",
            "Epoch 0 Step 631/1563 Loss: 2.195 | Acc: 18.661% (3774/20224)\n",
            "Epoch 0 Step 632/1563 Loss: 2.195 | Acc: 18.691% (3786/20256)\n",
            "Epoch 0 Step 633/1563 Loss: 2.195 | Acc: 18.681% (3790/20288)\n",
            "Epoch 0 Step 634/1563 Loss: 2.195 | Acc: 18.676% (3795/20320)\n",
            "Epoch 0 Step 635/1563 Loss: 2.194 | Acc: 18.691% (3804/20352)\n",
            "Epoch 0 Step 636/1563 Loss: 2.194 | Acc: 18.706% (3813/20384)\n",
            "Epoch 0 Step 637/1563 Loss: 2.194 | Acc: 18.681% (3814/20416)\n",
            "Epoch 0 Step 638/1563 Loss: 2.194 | Acc: 18.691% (3822/20448)\n",
            "Epoch 0 Step 639/1563 Loss: 2.194 | Acc: 18.711% (3832/20480)\n",
            "Epoch 0 Step 640/1563 Loss: 2.194 | Acc: 18.726% (3841/20512)\n",
            "Epoch 0 Step 641/1563 Loss: 2.193 | Acc: 18.745% (3851/20544)\n",
            "Epoch 0 Step 642/1563 Loss: 2.193 | Acc: 18.745% (3857/20576)\n",
            "Epoch 0 Step 643/1563 Loss: 2.193 | Acc: 18.774% (3869/20608)\n",
            "Epoch 0 Step 644/1563 Loss: 2.193 | Acc: 18.774% (3875/20640)\n",
            "Epoch 0 Step 645/1563 Loss: 2.193 | Acc: 18.789% (3884/20672)\n",
            "Epoch 0 Step 646/1563 Loss: 2.193 | Acc: 18.798% (3892/20704)\n",
            "Epoch 0 Step 647/1563 Loss: 2.192 | Acc: 18.808% (3900/20736)\n",
            "Epoch 0 Step 648/1563 Loss: 2.192 | Acc: 18.822% (3909/20768)\n",
            "Epoch 0 Step 649/1563 Loss: 2.192 | Acc: 18.822% (3915/20800)\n",
            "Epoch 0 Step 650/1563 Loss: 2.192 | Acc: 18.836% (3924/20832)\n",
            "Epoch 0 Step 651/1563 Loss: 2.192 | Acc: 18.851% (3933/20864)\n",
            "Epoch 0 Step 652/1563 Loss: 2.192 | Acc: 18.870% (3943/20896)\n",
            "Epoch 0 Step 653/1563 Loss: 2.191 | Acc: 18.869% (3949/20928)\n",
            "Epoch 0 Step 654/1563 Loss: 2.191 | Acc: 18.879% (3957/20960)\n",
            "Epoch 0 Step 655/1563 Loss: 2.191 | Acc: 18.883% (3964/20992)\n",
            "Epoch 0 Step 656/1563 Loss: 2.191 | Acc: 18.888% (3971/21024)\n",
            "Epoch 0 Step 657/1563 Loss: 2.190 | Acc: 18.907% (3981/21056)\n",
            "Epoch 0 Step 658/1563 Loss: 2.190 | Acc: 18.892% (3984/21088)\n",
            "Epoch 0 Step 659/1563 Loss: 2.190 | Acc: 18.892% (3990/21120)\n",
            "Epoch 0 Step 660/1563 Loss: 2.190 | Acc: 18.906% (3999/21152)\n",
            "Epoch 0 Step 661/1563 Loss: 2.189 | Acc: 18.929% (4010/21184)\n",
            "Epoch 0 Step 662/1563 Loss: 2.189 | Acc: 18.948% (4020/21216)\n",
            "Epoch 0 Step 663/1563 Loss: 2.189 | Acc: 18.962% (4029/21248)\n",
            "Epoch 0 Step 664/1563 Loss: 2.189 | Acc: 18.966% (4036/21280)\n",
            "Epoch 0 Step 665/1563 Loss: 2.188 | Acc: 18.975% (4044/21312)\n",
            "Epoch 0 Step 666/1563 Loss: 2.188 | Acc: 18.975% (4050/21344)\n",
            "Epoch 0 Step 667/1563 Loss: 2.188 | Acc: 18.998% (4061/21376)\n",
            "Epoch 0 Step 668/1563 Loss: 2.188 | Acc: 18.984% (4064/21408)\n",
            "Epoch 0 Step 669/1563 Loss: 2.188 | Acc: 18.969% (4067/21440)\n",
            "Epoch 0 Step 670/1563 Loss: 2.188 | Acc: 18.974% (4074/21472)\n",
            "Epoch 0 Step 671/1563 Loss: 2.187 | Acc: 18.964% (4078/21504)\n",
            "Epoch 0 Step 672/1563 Loss: 2.187 | Acc: 18.959% (4083/21536)\n",
            "Epoch 0 Step 673/1563 Loss: 2.187 | Acc: 18.977% (4093/21568)\n",
            "Epoch 0 Step 674/1563 Loss: 2.187 | Acc: 18.981% (4100/21600)\n",
            "Epoch 0 Step 675/1563 Loss: 2.187 | Acc: 18.986% (4107/21632)\n",
            "Epoch 0 Step 676/1563 Loss: 2.187 | Acc: 18.972% (4110/21664)\n",
            "Epoch 0 Step 677/1563 Loss: 2.186 | Acc: 18.976% (4117/21696)\n",
            "Epoch 0 Step 678/1563 Loss: 2.186 | Acc: 18.980% (4124/21728)\n",
            "Epoch 0 Step 679/1563 Loss: 2.186 | Acc: 18.984% (4131/21760)\n",
            "Epoch 0 Step 680/1563 Loss: 2.186 | Acc: 18.975% (4135/21792)\n",
            "Epoch 0 Step 681/1563 Loss: 2.186 | Acc: 18.965% (4139/21824)\n",
            "Epoch 0 Step 682/1563 Loss: 2.186 | Acc: 18.960% (4144/21856)\n",
            "Epoch 0 Step 683/1563 Loss: 2.186 | Acc: 18.951% (4148/21888)\n",
            "Epoch 0 Step 684/1563 Loss: 2.186 | Acc: 18.942% (4152/21920)\n",
            "Epoch 0 Step 685/1563 Loss: 2.186 | Acc: 18.932% (4156/21952)\n",
            "Epoch 0 Step 686/1563 Loss: 2.186 | Acc: 18.946% (4165/21984)\n",
            "Epoch 0 Step 687/1563 Loss: 2.186 | Acc: 18.950% (4172/22016)\n",
            "Epoch 0 Step 688/1563 Loss: 2.186 | Acc: 18.945% (4177/22048)\n",
            "Epoch 0 Step 689/1563 Loss: 2.185 | Acc: 18.927% (4179/22080)\n",
            "Epoch 0 Step 690/1563 Loss: 2.185 | Acc: 18.954% (4191/22112)\n",
            "Epoch 0 Step 691/1563 Loss: 2.185 | Acc: 18.971% (4201/22144)\n",
            "Epoch 0 Step 692/1563 Loss: 2.185 | Acc: 18.975% (4208/22176)\n",
            "Epoch 0 Step 693/1563 Loss: 2.185 | Acc: 18.989% (4217/22208)\n",
            "Epoch 0 Step 694/1563 Loss: 2.184 | Acc: 19.002% (4226/22240)\n",
            "Epoch 0 Step 695/1563 Loss: 2.184 | Acc: 19.019% (4236/22272)\n",
            "Epoch 0 Step 696/1563 Loss: 2.184 | Acc: 19.032% (4245/22304)\n",
            "Epoch 0 Step 697/1563 Loss: 2.184 | Acc: 19.028% (4250/22336)\n",
            "Epoch 0 Step 698/1563 Loss: 2.184 | Acc: 19.050% (4261/22368)\n",
            "Epoch 0 Step 699/1563 Loss: 2.184 | Acc: 19.040% (4265/22400)\n",
            "Epoch 0 Step 700/1563 Loss: 2.183 | Acc: 19.044% (4272/22432)\n",
            "Epoch 0 Step 701/1563 Loss: 2.183 | Acc: 19.044% (4278/22464)\n",
            "Epoch 0 Step 702/1563 Loss: 2.183 | Acc: 19.039% (4283/22496)\n",
            "Epoch 0 Step 703/1563 Loss: 2.183 | Acc: 19.039% (4289/22528)\n",
            "Epoch 0 Step 704/1563 Loss: 2.183 | Acc: 19.051% (4298/22560)\n",
            "Epoch 0 Step 705/1563 Loss: 2.183 | Acc: 19.055% (4305/22592)\n",
            "Epoch 0 Step 706/1563 Loss: 2.183 | Acc: 19.055% (4311/22624)\n",
            "Epoch 0 Step 707/1563 Loss: 2.182 | Acc: 19.041% (4314/22656)\n",
            "Epoch 0 Step 708/1563 Loss: 2.182 | Acc: 19.036% (4319/22688)\n",
            "Epoch 0 Step 709/1563 Loss: 2.182 | Acc: 19.045% (4327/22720)\n",
            "Epoch 0 Step 710/1563 Loss: 2.182 | Acc: 19.036% (4331/22752)\n",
            "Epoch 0 Step 711/1563 Loss: 2.182 | Acc: 19.044% (4339/22784)\n",
            "Epoch 0 Step 712/1563 Loss: 2.182 | Acc: 19.048% (4346/22816)\n",
            "Epoch 0 Step 713/1563 Loss: 2.182 | Acc: 19.043% (4351/22848)\n",
            "Epoch 0 Step 714/1563 Loss: 2.182 | Acc: 19.047% (4358/22880)\n",
            "Epoch 0 Step 715/1563 Loss: 2.182 | Acc: 19.060% (4367/22912)\n",
            "Epoch 0 Step 716/1563 Loss: 2.181 | Acc: 19.073% (4376/22944)\n",
            "Epoch 0 Step 717/1563 Loss: 2.181 | Acc: 19.085% (4385/22976)\n",
            "Epoch 0 Step 718/1563 Loss: 2.181 | Acc: 19.089% (4392/23008)\n",
            "Epoch 0 Step 719/1563 Loss: 2.181 | Acc: 19.102% (4401/23040)\n",
            "Epoch 0 Step 720/1563 Loss: 2.181 | Acc: 19.114% (4410/23072)\n",
            "Epoch 0 Step 721/1563 Loss: 2.180 | Acc: 19.118% (4417/23104)\n",
            "Epoch 0 Step 722/1563 Loss: 2.180 | Acc: 19.139% (4428/23136)\n",
            "Epoch 0 Step 723/1563 Loss: 2.180 | Acc: 19.147% (4436/23168)\n",
            "Epoch 0 Step 724/1563 Loss: 2.180 | Acc: 19.164% (4446/23200)\n",
            "Epoch 0 Step 725/1563 Loss: 2.180 | Acc: 19.150% (4449/23232)\n",
            "Epoch 0 Step 726/1563 Loss: 2.180 | Acc: 19.154% (4456/23264)\n",
            "Epoch 0 Step 727/1563 Loss: 2.179 | Acc: 19.166% (4465/23296)\n",
            "Epoch 0 Step 728/1563 Loss: 2.179 | Acc: 19.174% (4473/23328)\n",
            "Epoch 0 Step 729/1563 Loss: 2.179 | Acc: 19.178% (4480/23360)\n",
            "Epoch 0 Step 730/1563 Loss: 2.179 | Acc: 19.177% (4486/23392)\n",
            "Epoch 0 Step 731/1563 Loss: 2.178 | Acc: 19.190% (4495/23424)\n",
            "Epoch 0 Step 732/1563 Loss: 2.178 | Acc: 19.206% (4505/23456)\n",
            "Epoch 0 Step 733/1563 Loss: 2.178 | Acc: 19.218% (4514/23488)\n",
            "Epoch 0 Step 734/1563 Loss: 2.178 | Acc: 19.235% (4524/23520)\n",
            "Epoch 0 Step 735/1563 Loss: 2.177 | Acc: 19.260% (4536/23552)\n",
            "Epoch 0 Step 736/1563 Loss: 2.177 | Acc: 19.255% (4541/23584)\n",
            "Epoch 0 Step 737/1563 Loss: 2.177 | Acc: 19.271% (4551/23616)\n",
            "Epoch 0 Step 738/1563 Loss: 2.177 | Acc: 19.274% (4558/23648)\n",
            "Epoch 0 Step 739/1563 Loss: 2.177 | Acc: 19.282% (4566/23680)\n",
            "Epoch 0 Step 740/1563 Loss: 2.177 | Acc: 19.286% (4573/23712)\n",
            "Epoch 0 Step 741/1563 Loss: 2.177 | Acc: 19.306% (4584/23744)\n",
            "Epoch 0 Step 742/1563 Loss: 2.177 | Acc: 19.293% (4587/23776)\n",
            "Epoch 0 Step 743/1563 Loss: 2.177 | Acc: 19.296% (4594/23808)\n",
            "Epoch 0 Step 744/1563 Loss: 2.177 | Acc: 19.304% (4602/23840)\n",
            "Epoch 0 Step 745/1563 Loss: 2.177 | Acc: 19.299% (4607/23872)\n",
            "Epoch 0 Step 746/1563 Loss: 2.177 | Acc: 19.306% (4615/23904)\n",
            "Epoch 0 Step 747/1563 Loss: 2.176 | Acc: 19.297% (4619/23936)\n",
            "Epoch 0 Step 748/1563 Loss: 2.176 | Acc: 19.305% (4627/23968)\n",
            "Epoch 0 Step 749/1563 Loss: 2.176 | Acc: 19.312% (4635/24000)\n",
            "Epoch 0 Step 750/1563 Loss: 2.176 | Acc: 19.316% (4642/24032)\n",
            "Epoch 0 Step 751/1563 Loss: 2.176 | Acc: 19.328% (4651/24064)\n",
            "Epoch 0 Step 752/1563 Loss: 2.176 | Acc: 19.339% (4660/24096)\n",
            "Epoch 0 Step 753/1563 Loss: 2.176 | Acc: 19.363% (4672/24128)\n",
            "Epoch 0 Step 754/1563 Loss: 2.175 | Acc: 19.371% (4680/24160)\n",
            "Epoch 0 Step 755/1563 Loss: 2.175 | Acc: 19.362% (4684/24192)\n",
            "Epoch 0 Step 756/1563 Loss: 2.175 | Acc: 19.365% (4691/24224)\n",
            "Epoch 0 Step 757/1563 Loss: 2.175 | Acc: 19.385% (4702/24256)\n",
            "Epoch 0 Step 758/1563 Loss: 2.175 | Acc: 19.405% (4713/24288)\n",
            "Epoch 0 Step 759/1563 Loss: 2.174 | Acc: 19.412% (4721/24320)\n",
            "Epoch 0 Step 760/1563 Loss: 2.174 | Acc: 19.403% (4725/24352)\n",
            "Epoch 0 Step 761/1563 Loss: 2.174 | Acc: 19.410% (4733/24384)\n",
            "Epoch 0 Step 762/1563 Loss: 2.174 | Acc: 19.426% (4743/24416)\n",
            "Epoch 0 Step 763/1563 Loss: 2.173 | Acc: 19.421% (4748/24448)\n",
            "Epoch 0 Step 764/1563 Loss: 2.173 | Acc: 19.428% (4756/24480)\n",
            "Epoch 0 Step 765/1563 Loss: 2.173 | Acc: 19.419% (4760/24512)\n",
            "Epoch 0 Step 766/1563 Loss: 2.173 | Acc: 19.418% (4766/24544)\n",
            "Epoch 0 Step 767/1563 Loss: 2.173 | Acc: 19.438% (4777/24576)\n",
            "Epoch 0 Step 768/1563 Loss: 2.173 | Acc: 19.449% (4786/24608)\n",
            "Epoch 0 Step 769/1563 Loss: 2.173 | Acc: 19.456% (4794/24640)\n",
            "Epoch 0 Step 770/1563 Loss: 2.173 | Acc: 19.459% (4801/24672)\n",
            "Epoch 0 Step 771/1563 Loss: 2.172 | Acc: 19.475% (4811/24704)\n",
            "Epoch 0 Step 772/1563 Loss: 2.172 | Acc: 19.478% (4818/24736)\n",
            "Epoch 0 Step 773/1563 Loss: 2.172 | Acc: 19.481% (4825/24768)\n",
            "Epoch 0 Step 774/1563 Loss: 2.172 | Acc: 19.480% (4831/24800)\n",
            "Epoch 0 Step 775/1563 Loss: 2.171 | Acc: 19.487% (4839/24832)\n",
            "Epoch 0 Step 776/1563 Loss: 2.171 | Acc: 19.482% (4844/24864)\n",
            "Epoch 0 Step 777/1563 Loss: 2.171 | Acc: 19.481% (4850/24896)\n",
            "Epoch 0 Step 778/1563 Loss: 2.171 | Acc: 19.488% (4858/24928)\n",
            "Epoch 0 Step 779/1563 Loss: 2.171 | Acc: 19.507% (4869/24960)\n",
            "Epoch 0 Step 780/1563 Loss: 2.170 | Acc: 19.518% (4878/24992)\n",
            "Epoch 0 Step 781/1563 Loss: 2.170 | Acc: 19.509% (4882/25024)\n",
            "Epoch 0 Step 782/1563 Loss: 2.170 | Acc: 19.500% (4886/25056)\n",
            "Epoch 0 Step 783/1563 Loss: 2.170 | Acc: 19.491% (4890/25088)\n",
            "Epoch 0 Step 784/1563 Loss: 2.170 | Acc: 19.510% (4901/25120)\n",
            "Epoch 0 Step 785/1563 Loss: 2.170 | Acc: 19.509% (4907/25152)\n",
            "Epoch 0 Step 786/1563 Loss: 2.170 | Acc: 19.528% (4918/25184)\n",
            "Epoch 0 Step 787/1563 Loss: 2.170 | Acc: 19.527% (4924/25216)\n",
            "Epoch 0 Step 788/1563 Loss: 2.170 | Acc: 19.550% (4936/25248)\n",
            "Epoch 0 Step 789/1563 Loss: 2.169 | Acc: 19.557% (4944/25280)\n",
            "Epoch 0 Step 790/1563 Loss: 2.169 | Acc: 19.556% (4950/25312)\n",
            "Epoch 0 Step 791/1563 Loss: 2.169 | Acc: 19.551% (4955/25344)\n",
            "Epoch 0 Step 792/1563 Loss: 2.169 | Acc: 19.558% (4963/25376)\n",
            "Epoch 0 Step 793/1563 Loss: 2.169 | Acc: 19.553% (4968/25408)\n",
            "Epoch 0 Step 794/1563 Loss: 2.169 | Acc: 19.560% (4976/25440)\n",
            "Epoch 0 Step 795/1563 Loss: 2.169 | Acc: 19.559% (4982/25472)\n",
            "Epoch 0 Step 796/1563 Loss: 2.169 | Acc: 19.566% (4990/25504)\n",
            "Epoch 0 Step 797/1563 Loss: 2.169 | Acc: 19.576% (4999/25536)\n",
            "Epoch 0 Step 798/1563 Loss: 2.169 | Acc: 19.583% (5007/25568)\n",
            "Epoch 0 Step 799/1563 Loss: 2.169 | Acc: 19.598% (5017/25600)\n",
            "Epoch 0 Step 800/1563 Loss: 2.168 | Acc: 19.608% (5026/25632)\n",
            "Epoch 0 Step 801/1563 Loss: 2.168 | Acc: 19.607% (5032/25664)\n",
            "Epoch 0 Step 802/1563 Loss: 2.168 | Acc: 19.610% (5039/25696)\n",
            "Epoch 0 Step 803/1563 Loss: 2.168 | Acc: 19.625% (5049/25728)\n",
            "Epoch 0 Step 804/1563 Loss: 2.168 | Acc: 19.631% (5057/25760)\n",
            "Epoch 0 Step 805/1563 Loss: 2.168 | Acc: 19.653% (5069/25792)\n",
            "Epoch 0 Step 806/1563 Loss: 2.168 | Acc: 19.656% (5076/25824)\n",
            "Epoch 0 Step 807/1563 Loss: 2.168 | Acc: 19.659% (5083/25856)\n",
            "Epoch 0 Step 808/1563 Loss: 2.167 | Acc: 19.662% (5090/25888)\n",
            "Epoch 0 Step 809/1563 Loss: 2.167 | Acc: 19.664% (5097/25920)\n",
            "Epoch 0 Step 810/1563 Loss: 2.167 | Acc: 19.679% (5107/25952)\n",
            "Epoch 0 Step 811/1563 Loss: 2.167 | Acc: 19.666% (5110/25984)\n",
            "Epoch 0 Step 812/1563 Loss: 2.167 | Acc: 19.657% (5114/26016)\n",
            "Epoch 0 Step 813/1563 Loss: 2.167 | Acc: 19.656% (5120/26048)\n",
            "Epoch 0 Step 814/1563 Loss: 2.167 | Acc: 19.663% (5128/26080)\n",
            "Epoch 0 Step 815/1563 Loss: 2.167 | Acc: 19.669% (5136/26112)\n",
            "Epoch 0 Step 816/1563 Loss: 2.167 | Acc: 19.668% (5142/26144)\n",
            "Epoch 0 Step 817/1563 Loss: 2.166 | Acc: 19.671% (5149/26176)\n",
            "Epoch 0 Step 818/1563 Loss: 2.166 | Acc: 19.670% (5155/26208)\n",
            "Epoch 0 Step 819/1563 Loss: 2.166 | Acc: 19.668% (5161/26240)\n",
            "Epoch 0 Step 820/1563 Loss: 2.166 | Acc: 19.652% (5163/26272)\n",
            "Epoch 0 Step 821/1563 Loss: 2.166 | Acc: 19.662% (5172/26304)\n",
            "Epoch 0 Step 822/1563 Loss: 2.166 | Acc: 19.658% (5177/26336)\n",
            "Epoch 0 Step 823/1563 Loss: 2.166 | Acc: 19.641% (5179/26368)\n",
            "Epoch 0 Step 824/1563 Loss: 2.165 | Acc: 19.652% (5188/26400)\n",
            "Epoch 0 Step 825/1563 Loss: 2.165 | Acc: 19.662% (5197/26432)\n",
            "Epoch 0 Step 826/1563 Loss: 2.165 | Acc: 19.661% (5203/26464)\n",
            "Epoch 0 Step 827/1563 Loss: 2.165 | Acc: 19.652% (5207/26496)\n",
            "Epoch 0 Step 828/1563 Loss: 2.165 | Acc: 19.651% (5213/26528)\n",
            "Epoch 0 Step 829/1563 Loss: 2.165 | Acc: 19.657% (5221/26560)\n",
            "Epoch 0 Step 830/1563 Loss: 2.165 | Acc: 19.660% (5228/26592)\n",
            "Epoch 0 Step 831/1563 Loss: 2.165 | Acc: 19.648% (5231/26624)\n",
            "Epoch 0 Step 832/1563 Loss: 2.165 | Acc: 19.647% (5237/26656)\n",
            "Epoch 0 Step 833/1563 Loss: 2.165 | Acc: 19.653% (5245/26688)\n",
            "Epoch 0 Step 834/1563 Loss: 2.164 | Acc: 19.648% (5250/26720)\n",
            "Epoch 0 Step 835/1563 Loss: 2.164 | Acc: 19.643% (5255/26752)\n",
            "Epoch 0 Step 836/1563 Loss: 2.164 | Acc: 19.639% (5260/26784)\n",
            "Epoch 0 Step 837/1563 Loss: 2.164 | Acc: 19.649% (5269/26816)\n",
            "Epoch 0 Step 838/1563 Loss: 2.164 | Acc: 19.651% (5276/26848)\n",
            "Epoch 0 Step 839/1563 Loss: 2.163 | Acc: 19.673% (5288/26880)\n",
            "Epoch 0 Step 840/1563 Loss: 2.164 | Acc: 19.664% (5292/26912)\n",
            "Epoch 0 Step 841/1563 Loss: 2.163 | Acc: 19.682% (5303/26944)\n",
            "Epoch 0 Step 842/1563 Loss: 2.163 | Acc: 19.684% (5310/26976)\n",
            "Epoch 0 Step 843/1563 Loss: 2.163 | Acc: 19.672% (5313/27008)\n",
            "Epoch 0 Step 844/1563 Loss: 2.163 | Acc: 19.686% (5323/27040)\n",
            "Epoch 0 Step 845/1563 Loss: 2.163 | Acc: 19.703% (5334/27072)\n",
            "Epoch 0 Step 846/1563 Loss: 2.162 | Acc: 19.698% (5339/27104)\n",
            "Epoch 0 Step 847/1563 Loss: 2.162 | Acc: 19.712% (5349/27136)\n",
            "Epoch 0 Step 848/1563 Loss: 2.162 | Acc: 19.711% (5355/27168)\n",
            "Epoch 0 Step 849/1563 Loss: 2.162 | Acc: 19.717% (5363/27200)\n",
            "Epoch 0 Step 850/1563 Loss: 2.162 | Acc: 19.712% (5368/27232)\n",
            "Epoch 0 Step 851/1563 Loss: 2.162 | Acc: 19.700% (5371/27264)\n",
            "Epoch 0 Step 852/1563 Loss: 2.162 | Acc: 19.706% (5379/27296)\n",
            "Epoch 0 Step 853/1563 Loss: 2.162 | Acc: 19.698% (5383/27328)\n",
            "Epoch 0 Step 854/1563 Loss: 2.162 | Acc: 19.693% (5388/27360)\n",
            "Epoch 0 Step 855/1563 Loss: 2.162 | Acc: 19.692% (5394/27392)\n",
            "Epoch 0 Step 856/1563 Loss: 2.162 | Acc: 19.687% (5399/27424)\n",
            "Epoch 0 Step 857/1563 Loss: 2.162 | Acc: 19.679% (5403/27456)\n",
            "Epoch 0 Step 858/1563 Loss: 2.162 | Acc: 19.670% (5407/27488)\n",
            "Epoch 0 Step 859/1563 Loss: 2.162 | Acc: 19.658% (5410/27520)\n",
            "Epoch 0 Step 860/1563 Loss: 2.162 | Acc: 19.650% (5414/27552)\n",
            "Epoch 0 Step 861/1563 Loss: 2.162 | Acc: 19.645% (5419/27584)\n",
            "Epoch 0 Step 862/1563 Loss: 2.162 | Acc: 19.644% (5425/27616)\n",
            "Epoch 0 Step 863/1563 Loss: 2.162 | Acc: 19.651% (5433/27648)\n",
            "Epoch 0 Step 864/1563 Loss: 2.161 | Acc: 19.657% (5441/27680)\n",
            "Epoch 0 Step 865/1563 Loss: 2.161 | Acc: 19.674% (5452/27712)\n",
            "Epoch 0 Step 866/1563 Loss: 2.161 | Acc: 19.680% (5460/27744)\n",
            "Epoch 0 Step 867/1563 Loss: 2.161 | Acc: 19.675% (5465/27776)\n",
            "Epoch 0 Step 868/1563 Loss: 2.161 | Acc: 19.678% (5472/27808)\n",
            "Epoch 0 Step 869/1563 Loss: 2.161 | Acc: 19.670% (5476/27840)\n",
            "Epoch 0 Step 870/1563 Loss: 2.161 | Acc: 19.668% (5482/27872)\n",
            "Epoch 0 Step 871/1563 Loss: 2.161 | Acc: 19.685% (5493/27904)\n",
            "Epoch 0 Step 872/1563 Loss: 2.161 | Acc: 19.688% (5500/27936)\n",
            "Epoch 0 Step 873/1563 Loss: 2.161 | Acc: 19.676% (5503/27968)\n",
            "Epoch 0 Step 874/1563 Loss: 2.161 | Acc: 19.682% (5511/28000)\n",
            "Epoch 0 Step 875/1563 Loss: 2.161 | Acc: 19.702% (5523/28032)\n",
            "Epoch 0 Step 876/1563 Loss: 2.161 | Acc: 19.698% (5528/28064)\n",
            "Epoch 0 Step 877/1563 Loss: 2.161 | Acc: 19.704% (5536/28096)\n",
            "Epoch 0 Step 878/1563 Loss: 2.161 | Acc: 19.692% (5539/28128)\n",
            "Epoch 0 Step 879/1563 Loss: 2.161 | Acc: 19.688% (5544/28160)\n",
            "Epoch 0 Step 880/1563 Loss: 2.161 | Acc: 19.676% (5547/28192)\n",
            "Epoch 0 Step 881/1563 Loss: 2.161 | Acc: 19.689% (5557/28224)\n",
            "Epoch 0 Step 882/1563 Loss: 2.160 | Acc: 19.688% (5563/28256)\n",
            "Epoch 0 Step 883/1563 Loss: 2.160 | Acc: 19.708% (5575/28288)\n",
            "Epoch 0 Step 884/1563 Loss: 2.160 | Acc: 19.696% (5578/28320)\n",
            "Epoch 0 Step 885/1563 Loss: 2.160 | Acc: 19.695% (5584/28352)\n",
            "Epoch 0 Step 886/1563 Loss: 2.160 | Acc: 19.705% (5593/28384)\n",
            "Epoch 0 Step 887/1563 Loss: 2.160 | Acc: 19.714% (5602/28416)\n",
            "Epoch 0 Step 888/1563 Loss: 2.159 | Acc: 19.734% (5614/28448)\n",
            "Epoch 0 Step 889/1563 Loss: 2.159 | Acc: 19.751% (5625/28480)\n",
            "Epoch 0 Step 890/1563 Loss: 2.159 | Acc: 19.757% (5633/28512)\n",
            "Epoch 0 Step 891/1563 Loss: 2.159 | Acc: 19.769% (5643/28544)\n",
            "Epoch 0 Step 892/1563 Loss: 2.159 | Acc: 19.782% (5653/28576)\n",
            "Epoch 0 Step 893/1563 Loss: 2.159 | Acc: 19.799% (5664/28608)\n",
            "Epoch 0 Step 894/1563 Loss: 2.159 | Acc: 19.794% (5669/28640)\n",
            "Epoch 0 Step 895/1563 Loss: 2.158 | Acc: 19.803% (5678/28672)\n",
            "Epoch 0 Step 896/1563 Loss: 2.158 | Acc: 19.809% (5686/28704)\n",
            "Epoch 0 Step 897/1563 Loss: 2.158 | Acc: 19.825% (5697/28736)\n",
            "Epoch 0 Step 898/1563 Loss: 2.158 | Acc: 19.821% (5702/28768)\n",
            "Epoch 0 Step 899/1563 Loss: 2.158 | Acc: 19.826% (5710/28800)\n",
            "Epoch 0 Step 900/1563 Loss: 2.158 | Acc: 19.822% (5715/28832)\n",
            "Epoch 0 Step 901/1563 Loss: 2.158 | Acc: 19.824% (5722/28864)\n",
            "Epoch 0 Step 902/1563 Loss: 2.158 | Acc: 19.837% (5732/28896)\n",
            "Epoch 0 Step 903/1563 Loss: 2.158 | Acc: 19.839% (5739/28928)\n",
            "Epoch 0 Step 904/1563 Loss: 2.157 | Acc: 19.838% (5745/28960)\n",
            "Epoch 0 Step 905/1563 Loss: 2.157 | Acc: 19.840% (5752/28992)\n",
            "Epoch 0 Step 906/1563 Loss: 2.157 | Acc: 19.849% (5761/29024)\n",
            "Epoch 0 Step 907/1563 Loss: 2.157 | Acc: 19.848% (5767/29056)\n",
            "Epoch 0 Step 908/1563 Loss: 2.157 | Acc: 19.850% (5774/29088)\n",
            "Epoch 0 Step 909/1563 Loss: 2.157 | Acc: 19.845% (5779/29120)\n",
            "Epoch 0 Step 910/1563 Loss: 2.157 | Acc: 19.848% (5786/29152)\n",
            "Epoch 0 Step 911/1563 Loss: 2.157 | Acc: 19.843% (5791/29184)\n",
            "Epoch 0 Step 912/1563 Loss: 2.157 | Acc: 19.838% (5796/29216)\n",
            "Epoch 0 Step 913/1563 Loss: 2.156 | Acc: 19.844% (5804/29248)\n",
            "Epoch 0 Step 914/1563 Loss: 2.156 | Acc: 19.846% (5811/29280)\n",
            "Epoch 0 Step 915/1563 Loss: 2.156 | Acc: 19.859% (5821/29312)\n",
            "Epoch 0 Step 916/1563 Loss: 2.156 | Acc: 19.864% (5829/29344)\n",
            "Epoch 0 Step 917/1563 Loss: 2.156 | Acc: 19.856% (5833/29376)\n",
            "Epoch 0 Step 918/1563 Loss: 2.156 | Acc: 19.862% (5841/29408)\n",
            "Epoch 0 Step 919/1563 Loss: 2.156 | Acc: 19.874% (5851/29440)\n",
            "Epoch 0 Step 920/1563 Loss: 2.156 | Acc: 19.876% (5858/29472)\n",
            "Epoch 0 Step 921/1563 Loss: 2.156 | Acc: 19.872% (5863/29504)\n",
            "Epoch 0 Step 922/1563 Loss: 2.156 | Acc: 19.874% (5870/29536)\n",
            "Epoch 0 Step 923/1563 Loss: 2.155 | Acc: 19.900% (5884/29568)\n",
            "Epoch 0 Step 924/1563 Loss: 2.155 | Acc: 19.909% (5893/29600)\n",
            "Epoch 0 Step 925/1563 Loss: 2.155 | Acc: 19.901% (5897/29632)\n",
            "Epoch 0 Step 926/1563 Loss: 2.156 | Acc: 19.889% (5900/29664)\n",
            "Epoch 0 Step 927/1563 Loss: 2.156 | Acc: 19.892% (5907/29696)\n",
            "Epoch 0 Step 928/1563 Loss: 2.155 | Acc: 19.900% (5916/29728)\n",
            "Epoch 0 Step 929/1563 Loss: 2.155 | Acc: 19.916% (5927/29760)\n",
            "Epoch 0 Step 930/1563 Loss: 2.155 | Acc: 19.928% (5937/29792)\n",
            "Epoch 0 Step 931/1563 Loss: 2.155 | Acc: 19.944% (5948/29824)\n",
            "Epoch 0 Step 932/1563 Loss: 2.155 | Acc: 19.939% (5953/29856)\n",
            "Epoch 0 Step 933/1563 Loss: 2.155 | Acc: 19.931% (5957/29888)\n",
            "Epoch 0 Step 934/1563 Loss: 2.155 | Acc: 19.940% (5966/29920)\n",
            "Epoch 0 Step 935/1563 Loss: 2.154 | Acc: 19.949% (5975/29952)\n",
            "Epoch 0 Step 936/1563 Loss: 2.154 | Acc: 19.957% (5984/29984)\n",
            "Epoch 0 Step 937/1563 Loss: 2.154 | Acc: 19.966% (5993/30016)\n",
            "Epoch 0 Step 938/1563 Loss: 2.154 | Acc: 19.988% (6006/30048)\n",
            "Epoch 0 Step 939/1563 Loss: 2.154 | Acc: 19.997% (6015/30080)\n",
            "Epoch 0 Step 940/1563 Loss: 2.153 | Acc: 20.015% (6027/30112)\n",
            "Epoch 0 Step 941/1563 Loss: 2.153 | Acc: 20.037% (6040/30144)\n",
            "Epoch 0 Step 942/1563 Loss: 2.153 | Acc: 20.049% (6050/30176)\n",
            "Epoch 0 Step 943/1563 Loss: 2.153 | Acc: 20.038% (6053/30208)\n",
            "Epoch 0 Step 944/1563 Loss: 2.153 | Acc: 20.040% (6060/30240)\n",
            "Epoch 0 Step 945/1563 Loss: 2.153 | Acc: 20.038% (6066/30272)\n",
            "Epoch 0 Step 946/1563 Loss: 2.153 | Acc: 20.057% (6078/30304)\n",
            "Epoch 0 Step 947/1563 Loss: 2.152 | Acc: 20.075% (6090/30336)\n",
            "Epoch 0 Step 948/1563 Loss: 2.152 | Acc: 20.077% (6097/30368)\n",
            "Epoch 0 Step 949/1563 Loss: 2.152 | Acc: 20.082% (6105/30400)\n",
            "Epoch 0 Step 950/1563 Loss: 2.152 | Acc: 20.084% (6112/30432)\n",
            "Epoch 0 Step 951/1563 Loss: 2.152 | Acc: 20.079% (6117/30464)\n",
            "Epoch 0 Step 952/1563 Loss: 2.152 | Acc: 20.078% (6123/30496)\n",
            "Epoch 0 Step 953/1563 Loss: 2.152 | Acc: 20.080% (6130/30528)\n",
            "Epoch 0 Step 954/1563 Loss: 2.152 | Acc: 20.085% (6138/30560)\n",
            "Epoch 0 Step 955/1563 Loss: 2.152 | Acc: 20.084% (6144/30592)\n",
            "Epoch 0 Step 956/1563 Loss: 2.152 | Acc: 20.076% (6148/30624)\n",
            "Epoch 0 Step 957/1563 Loss: 2.152 | Acc: 20.087% (6158/30656)\n",
            "Epoch 0 Step 958/1563 Loss: 2.152 | Acc: 20.086% (6164/30688)\n",
            "Epoch 0 Step 959/1563 Loss: 2.151 | Acc: 20.091% (6172/30720)\n",
            "Epoch 0 Step 960/1563 Loss: 2.151 | Acc: 20.103% (6182/30752)\n",
            "Epoch 0 Step 961/1563 Loss: 2.151 | Acc: 20.121% (6194/30784)\n",
            "Epoch 0 Step 962/1563 Loss: 2.151 | Acc: 20.123% (6201/30816)\n",
            "Epoch 0 Step 963/1563 Loss: 2.151 | Acc: 20.141% (6213/30848)\n",
            "Epoch 0 Step 964/1563 Loss: 2.151 | Acc: 20.146% (6221/30880)\n",
            "Epoch 0 Step 965/1563 Loss: 2.151 | Acc: 20.151% (6229/30912)\n",
            "Epoch 0 Step 966/1563 Loss: 2.150 | Acc: 20.149% (6235/30944)\n",
            "Epoch 0 Step 967/1563 Loss: 2.150 | Acc: 20.161% (6245/30976)\n",
            "Epoch 0 Step 968/1563 Loss: 2.150 | Acc: 20.159% (6251/31008)\n",
            "Epoch 0 Step 969/1563 Loss: 2.150 | Acc: 20.151% (6255/31040)\n",
            "Epoch 0 Step 970/1563 Loss: 2.150 | Acc: 20.140% (6258/31072)\n",
            "Epoch 0 Step 971/1563 Loss: 2.150 | Acc: 20.155% (6269/31104)\n",
            "Epoch 0 Step 972/1563 Loss: 2.150 | Acc: 20.160% (6277/31136)\n",
            "Epoch 0 Step 973/1563 Loss: 2.150 | Acc: 20.152% (6281/31168)\n",
            "Epoch 0 Step 974/1563 Loss: 2.150 | Acc: 20.167% (6292/31200)\n",
            "Epoch 0 Step 975/1563 Loss: 2.149 | Acc: 20.175% (6301/31232)\n",
            "Epoch 0 Step 976/1563 Loss: 2.149 | Acc: 20.177% (6308/31264)\n",
            "Epoch 0 Step 977/1563 Loss: 2.149 | Acc: 20.188% (6318/31296)\n",
            "Epoch 0 Step 978/1563 Loss: 2.149 | Acc: 20.202% (6329/31328)\n",
            "Epoch 0 Step 979/1563 Loss: 2.149 | Acc: 20.204% (6336/31360)\n",
            "Epoch 0 Step 980/1563 Loss: 2.149 | Acc: 20.212% (6345/31392)\n",
            "Epoch 0 Step 981/1563 Loss: 2.149 | Acc: 20.214% (6352/31424)\n",
            "Epoch 0 Step 982/1563 Loss: 2.148 | Acc: 20.216% (6359/31456)\n",
            "Epoch 0 Step 983/1563 Loss: 2.148 | Acc: 20.224% (6368/31488)\n",
            "Epoch 0 Step 984/1563 Loss: 2.148 | Acc: 20.225% (6375/31520)\n",
            "Epoch 0 Step 985/1563 Loss: 2.148 | Acc: 20.230% (6383/31552)\n",
            "Epoch 0 Step 986/1563 Loss: 2.148 | Acc: 20.235% (6391/31584)\n",
            "Epoch 0 Step 987/1563 Loss: 2.148 | Acc: 20.252% (6403/31616)\n",
            "Epoch 0 Step 988/1563 Loss: 2.148 | Acc: 20.260% (6412/31648)\n",
            "Epoch 0 Step 989/1563 Loss: 2.148 | Acc: 20.259% (6418/31680)\n",
            "Epoch 0 Step 990/1563 Loss: 2.148 | Acc: 20.273% (6429/31712)\n",
            "Epoch 0 Step 991/1563 Loss: 2.148 | Acc: 20.272% (6435/31744)\n",
            "Epoch 0 Step 992/1563 Loss: 2.148 | Acc: 20.295% (6449/31776)\n",
            "Epoch 0 Step 993/1563 Loss: 2.147 | Acc: 20.300% (6457/31808)\n",
            "Epoch 0 Step 994/1563 Loss: 2.147 | Acc: 20.289% (6460/31840)\n",
            "Epoch 0 Step 995/1563 Loss: 2.147 | Acc: 20.287% (6466/31872)\n",
            "Epoch 0 Step 996/1563 Loss: 2.147 | Acc: 20.283% (6471/31904)\n",
            "Epoch 0 Step 997/1563 Loss: 2.147 | Acc: 20.278% (6476/31936)\n",
            "Epoch 0 Step 998/1563 Loss: 2.147 | Acc: 20.289% (6486/31968)\n",
            "Epoch 0 Step 999/1563 Loss: 2.147 | Acc: 20.288% (6492/32000)\n",
            "Epoch 0 Step 1000/1563 Loss: 2.146 | Acc: 20.295% (6501/32032)\n",
            "Epoch 0 Step 1001/1563 Loss: 2.146 | Acc: 20.300% (6509/32064)\n",
            "Epoch 0 Step 1002/1563 Loss: 2.146 | Acc: 20.308% (6518/32096)\n",
            "Epoch 0 Step 1003/1563 Loss: 2.146 | Acc: 20.306% (6524/32128)\n",
            "Epoch 0 Step 1004/1563 Loss: 2.146 | Acc: 20.311% (6532/32160)\n",
            "Epoch 0 Step 1005/1563 Loss: 2.146 | Acc: 20.316% (6540/32192)\n",
            "Epoch 0 Step 1006/1563 Loss: 2.146 | Acc: 20.326% (6550/32224)\n",
            "Epoch 0 Step 1007/1563 Loss: 2.145 | Acc: 20.331% (6558/32256)\n",
            "Epoch 0 Step 1008/1563 Loss: 2.145 | Acc: 20.330% (6564/32288)\n",
            "Epoch 0 Step 1009/1563 Loss: 2.145 | Acc: 20.340% (6574/32320)\n",
            "Epoch 0 Step 1010/1563 Loss: 2.145 | Acc: 20.354% (6585/32352)\n",
            "Epoch 0 Step 1011/1563 Loss: 2.145 | Acc: 20.362% (6594/32384)\n",
            "Epoch 0 Step 1012/1563 Loss: 2.145 | Acc: 20.360% (6600/32416)\n",
            "Epoch 0 Step 1013/1563 Loss: 2.145 | Acc: 20.362% (6607/32448)\n",
            "Epoch 0 Step 1014/1563 Loss: 2.145 | Acc: 20.363% (6614/32480)\n",
            "Epoch 0 Step 1015/1563 Loss: 2.145 | Acc: 20.365% (6621/32512)\n",
            "Epoch 0 Step 1016/1563 Loss: 2.145 | Acc: 20.369% (6629/32544)\n",
            "Epoch 0 Step 1017/1563 Loss: 2.145 | Acc: 20.365% (6634/32576)\n",
            "Epoch 0 Step 1018/1563 Loss: 2.145 | Acc: 20.366% (6641/32608)\n",
            "Epoch 0 Step 1019/1563 Loss: 2.145 | Acc: 20.383% (6653/32640)\n",
            "Epoch 0 Step 1020/1563 Loss: 2.145 | Acc: 20.381% (6659/32672)\n",
            "Epoch 0 Step 1021/1563 Loss: 2.144 | Acc: 20.392% (6669/32704)\n",
            "Epoch 0 Step 1022/1563 Loss: 2.144 | Acc: 20.390% (6675/32736)\n",
            "Epoch 0 Step 1023/1563 Loss: 2.144 | Acc: 20.416% (6690/32768)\n",
            "Epoch 0 Step 1024/1563 Loss: 2.144 | Acc: 20.421% (6698/32800)\n",
            "Epoch 0 Step 1025/1563 Loss: 2.144 | Acc: 20.416% (6703/32832)\n",
            "Epoch 0 Step 1026/1563 Loss: 2.144 | Acc: 20.408% (6707/32864)\n",
            "Epoch 0 Step 1027/1563 Loss: 2.144 | Acc: 20.425% (6719/32896)\n",
            "Epoch 0 Step 1028/1563 Loss: 2.144 | Acc: 20.420% (6724/32928)\n",
            "Epoch 0 Step 1029/1563 Loss: 2.144 | Acc: 20.407% (6726/32960)\n",
            "Epoch 0 Step 1030/1563 Loss: 2.144 | Acc: 20.423% (6738/32992)\n",
            "Epoch 0 Step 1031/1563 Loss: 2.143 | Acc: 20.431% (6747/33024)\n",
            "Epoch 0 Step 1032/1563 Loss: 2.143 | Acc: 20.423% (6751/33056)\n",
            "Epoch 0 Step 1033/1563 Loss: 2.143 | Acc: 20.436% (6762/33088)\n",
            "Epoch 0 Step 1034/1563 Loss: 2.143 | Acc: 20.441% (6770/33120)\n",
            "Epoch 0 Step 1035/1563 Loss: 2.143 | Acc: 20.439% (6776/33152)\n",
            "Epoch 0 Step 1036/1563 Loss: 2.143 | Acc: 20.435% (6781/33184)\n",
            "Epoch 0 Step 1037/1563 Loss: 2.143 | Acc: 20.436% (6788/33216)\n",
            "Epoch 0 Step 1038/1563 Loss: 2.143 | Acc: 20.437% (6795/33248)\n",
            "Epoch 0 Step 1039/1563 Loss: 2.142 | Acc: 20.439% (6802/33280)\n",
            "Epoch 0 Step 1040/1563 Loss: 2.143 | Acc: 20.428% (6805/33312)\n",
            "Epoch 0 Step 1041/1563 Loss: 2.142 | Acc: 20.435% (6814/33344)\n",
            "Epoch 0 Step 1042/1563 Loss: 2.142 | Acc: 20.425% (6817/33376)\n",
            "Epoch 0 Step 1043/1563 Loss: 2.142 | Acc: 20.423% (6823/33408)\n",
            "Epoch 0 Step 1044/1563 Loss: 2.142 | Acc: 20.431% (6832/33440)\n",
            "Epoch 0 Step 1045/1563 Loss: 2.142 | Acc: 20.429% (6838/33472)\n",
            "Epoch 0 Step 1046/1563 Loss: 2.142 | Acc: 20.430% (6845/33504)\n",
            "Epoch 0 Step 1047/1563 Loss: 2.142 | Acc: 20.432% (6852/33536)\n",
            "Epoch 0 Step 1048/1563 Loss: 2.142 | Acc: 20.436% (6860/33568)\n",
            "Epoch 0 Step 1049/1563 Loss: 2.142 | Acc: 20.440% (6868/33600)\n",
            "Epoch 0 Step 1050/1563 Loss: 2.142 | Acc: 20.454% (6879/33632)\n",
            "Epoch 0 Step 1051/1563 Loss: 2.142 | Acc: 20.458% (6887/33664)\n",
            "Epoch 0 Step 1052/1563 Loss: 2.142 | Acc: 20.465% (6896/33696)\n",
            "Epoch 0 Step 1053/1563 Loss: 2.141 | Acc: 20.464% (6902/33728)\n",
            "Epoch 0 Step 1054/1563 Loss: 2.142 | Acc: 20.468% (6910/33760)\n",
            "Epoch 0 Step 1055/1563 Loss: 2.141 | Acc: 20.460% (6914/33792)\n",
            "Epoch 0 Step 1056/1563 Loss: 2.141 | Acc: 20.471% (6924/33824)\n",
            "Epoch 0 Step 1057/1563 Loss: 2.141 | Acc: 20.478% (6933/33856)\n",
            "Epoch 0 Step 1058/1563 Loss: 2.141 | Acc: 20.491% (6944/33888)\n",
            "Epoch 0 Step 1059/1563 Loss: 2.141 | Acc: 20.516% (6959/33920)\n",
            "Epoch 0 Step 1060/1563 Loss: 2.141 | Acc: 20.529% (6970/33952)\n",
            "Epoch 0 Step 1061/1563 Loss: 2.141 | Acc: 20.533% (6978/33984)\n",
            "Epoch 0 Step 1062/1563 Loss: 2.140 | Acc: 20.549% (6990/34016)\n",
            "Epoch 0 Step 1063/1563 Loss: 2.140 | Acc: 20.556% (6999/34048)\n",
            "Epoch 0 Step 1064/1563 Loss: 2.140 | Acc: 20.555% (7005/34080)\n",
            "Epoch 0 Step 1065/1563 Loss: 2.140 | Acc: 20.568% (7016/34112)\n",
            "Epoch 0 Step 1066/1563 Loss: 2.140 | Acc: 20.566% (7022/34144)\n",
            "Epoch 0 Step 1067/1563 Loss: 2.140 | Acc: 20.564% (7028/34176)\n",
            "Epoch 0 Step 1068/1563 Loss: 2.140 | Acc: 20.557% (7032/34208)\n",
            "Epoch 0 Step 1069/1563 Loss: 2.140 | Acc: 20.555% (7038/34240)\n",
            "Epoch 0 Step 1070/1563 Loss: 2.140 | Acc: 20.571% (7050/34272)\n",
            "Epoch 0 Step 1071/1563 Loss: 2.139 | Acc: 20.584% (7061/34304)\n",
            "Epoch 0 Step 1072/1563 Loss: 2.139 | Acc: 20.585% (7068/34336)\n",
            "Epoch 0 Step 1073/1563 Loss: 2.139 | Acc: 20.586% (7075/34368)\n",
            "Epoch 0 Step 1074/1563 Loss: 2.139 | Acc: 20.590% (7083/34400)\n",
            "Epoch 0 Step 1075/1563 Loss: 2.139 | Acc: 20.588% (7089/34432)\n",
            "Epoch 0 Step 1076/1563 Loss: 2.139 | Acc: 20.598% (7099/34464)\n",
            "Epoch 0 Step 1077/1563 Loss: 2.139 | Acc: 20.602% (7107/34496)\n",
            "Epoch 0 Step 1078/1563 Loss: 2.139 | Acc: 20.598% (7112/34528)\n",
            "Epoch 0 Step 1079/1563 Loss: 2.139 | Acc: 20.605% (7121/34560)\n",
            "Epoch 0 Step 1080/1563 Loss: 2.139 | Acc: 20.600% (7126/34592)\n",
            "Epoch 0 Step 1081/1563 Loss: 2.139 | Acc: 20.604% (7134/34624)\n",
            "Epoch 0 Step 1082/1563 Loss: 2.139 | Acc: 20.602% (7140/34656)\n",
            "Epoch 0 Step 1083/1563 Loss: 2.139 | Acc: 20.601% (7146/34688)\n",
            "Epoch 0 Step 1084/1563 Loss: 2.139 | Acc: 20.608% (7155/34720)\n",
            "Epoch 0 Step 1085/1563 Loss: 2.139 | Acc: 20.603% (7160/34752)\n",
            "Epoch 0 Step 1086/1563 Loss: 2.139 | Acc: 20.599% (7165/34784)\n",
            "Epoch 0 Step 1087/1563 Loss: 2.139 | Acc: 20.608% (7175/34816)\n",
            "Epoch 0 Step 1088/1563 Loss: 2.139 | Acc: 20.612% (7183/34848)\n",
            "Epoch 0 Step 1089/1563 Loss: 2.139 | Acc: 20.619% (7192/34880)\n",
            "Epoch 0 Step 1090/1563 Loss: 2.139 | Acc: 20.626% (7201/34912)\n",
            "Epoch 0 Step 1091/1563 Loss: 2.139 | Acc: 20.642% (7213/34944)\n",
            "Epoch 0 Step 1092/1563 Loss: 2.138 | Acc: 20.654% (7224/34976)\n",
            "Epoch 0 Step 1093/1563 Loss: 2.138 | Acc: 20.655% (7231/35008)\n",
            "Epoch 0 Step 1094/1563 Loss: 2.138 | Acc: 20.665% (7241/35040)\n",
            "Epoch 0 Step 1095/1563 Loss: 2.138 | Acc: 20.672% (7250/35072)\n",
            "Epoch 0 Step 1096/1563 Loss: 2.138 | Acc: 20.670% (7256/35104)\n",
            "Epoch 0 Step 1097/1563 Loss: 2.138 | Acc: 20.668% (7262/35136)\n",
            "Epoch 0 Step 1098/1563 Loss: 2.138 | Acc: 20.675% (7271/35168)\n",
            "Epoch 0 Step 1099/1563 Loss: 2.138 | Acc: 20.670% (7276/35200)\n",
            "Epoch 0 Step 1100/1563 Loss: 2.138 | Acc: 20.672% (7283/35232)\n",
            "Epoch 0 Step 1101/1563 Loss: 2.138 | Acc: 20.661% (7286/35264)\n",
            "Epoch 0 Step 1102/1563 Loss: 2.138 | Acc: 20.665% (7294/35296)\n",
            "Epoch 0 Step 1103/1563 Loss: 2.138 | Acc: 20.675% (7304/35328)\n",
            "Epoch 0 Step 1104/1563 Loss: 2.137 | Acc: 20.684% (7314/35360)\n",
            "Epoch 0 Step 1105/1563 Loss: 2.137 | Acc: 20.685% (7321/35392)\n",
            "Epoch 0 Step 1106/1563 Loss: 2.137 | Acc: 20.692% (7330/35424)\n",
            "Epoch 0 Step 1107/1563 Loss: 2.137 | Acc: 20.699% (7339/35456)\n",
            "Epoch 0 Step 1108/1563 Loss: 2.137 | Acc: 20.708% (7349/35488)\n",
            "Epoch 0 Step 1109/1563 Loss: 2.137 | Acc: 20.709% (7356/35520)\n",
            "Epoch 0 Step 1110/1563 Loss: 2.137 | Acc: 20.705% (7361/35552)\n",
            "Epoch 0 Step 1111/1563 Loss: 2.137 | Acc: 20.698% (7365/35584)\n",
            "Epoch 0 Step 1112/1563 Loss: 2.137 | Acc: 20.713% (7377/35616)\n",
            "Epoch 0 Step 1113/1563 Loss: 2.137 | Acc: 20.714% (7384/35648)\n",
            "Epoch 0 Step 1114/1563 Loss: 2.136 | Acc: 20.715% (7391/35680)\n",
            "Epoch 0 Step 1115/1563 Loss: 2.136 | Acc: 20.721% (7400/35712)\n",
            "Epoch 0 Step 1116/1563 Loss: 2.136 | Acc: 20.734% (7411/35744)\n",
            "Epoch 0 Step 1117/1563 Loss: 2.136 | Acc: 20.746% (7422/35776)\n",
            "Epoch 0 Step 1118/1563 Loss: 2.136 | Acc: 20.750% (7430/35808)\n",
            "Epoch 0 Step 1119/1563 Loss: 2.136 | Acc: 20.745% (7435/35840)\n",
            "Epoch 0 Step 1120/1563 Loss: 2.136 | Acc: 20.740% (7440/35872)\n",
            "Epoch 0 Step 1121/1563 Loss: 2.136 | Acc: 20.736% (7445/35904)\n",
            "Epoch 0 Step 1122/1563 Loss: 2.136 | Acc: 20.737% (7452/35936)\n",
            "Epoch 0 Step 1123/1563 Loss: 2.136 | Acc: 20.735% (7458/35968)\n",
            "Epoch 0 Step 1124/1563 Loss: 2.136 | Acc: 20.725% (7461/36000)\n",
            "Epoch 0 Step 1125/1563 Loss: 2.136 | Acc: 20.726% (7468/36032)\n",
            "Epoch 0 Step 1126/1563 Loss: 2.136 | Acc: 20.719% (7472/36064)\n",
            "Epoch 0 Step 1127/1563 Loss: 2.136 | Acc: 20.714% (7477/36096)\n",
            "Epoch 0 Step 1128/1563 Loss: 2.136 | Acc: 20.715% (7484/36128)\n",
            "Epoch 0 Step 1129/1563 Loss: 2.136 | Acc: 20.716% (7491/36160)\n",
            "Epoch 0 Step 1130/1563 Loss: 2.136 | Acc: 20.709% (7495/36192)\n",
            "Epoch 0 Step 1131/1563 Loss: 2.136 | Acc: 20.702% (7499/36224)\n",
            "Epoch 0 Step 1132/1563 Loss: 2.136 | Acc: 20.700% (7505/36256)\n",
            "Epoch 0 Step 1133/1563 Loss: 2.136 | Acc: 20.709% (7515/36288)\n",
            "Epoch 0 Step 1134/1563 Loss: 2.136 | Acc: 20.719% (7525/36320)\n",
            "Epoch 0 Step 1135/1563 Loss: 2.136 | Acc: 20.720% (7532/36352)\n",
            "Epoch 0 Step 1136/1563 Loss: 2.136 | Acc: 20.734% (7544/36384)\n",
            "Epoch 0 Step 1137/1563 Loss: 2.135 | Acc: 20.744% (7554/36416)\n",
            "Epoch 0 Step 1138/1563 Loss: 2.136 | Acc: 20.736% (7558/36448)\n",
            "Epoch 0 Step 1139/1563 Loss: 2.135 | Acc: 20.751% (7570/36480)\n",
            "Epoch 0 Step 1140/1563 Loss: 2.135 | Acc: 20.755% (7578/36512)\n",
            "Epoch 0 Step 1141/1563 Loss: 2.135 | Acc: 20.753% (7584/36544)\n",
            "Epoch 0 Step 1142/1563 Loss: 2.135 | Acc: 20.754% (7591/36576)\n",
            "Epoch 0 Step 1143/1563 Loss: 2.135 | Acc: 20.760% (7600/36608)\n",
            "Epoch 0 Step 1144/1563 Loss: 2.135 | Acc: 20.764% (7608/36640)\n",
            "Epoch 0 Step 1145/1563 Loss: 2.135 | Acc: 20.762% (7614/36672)\n",
            "Epoch 0 Step 1146/1563 Loss: 2.135 | Acc: 20.761% (7620/36704)\n",
            "Epoch 0 Step 1147/1563 Loss: 2.135 | Acc: 20.759% (7626/36736)\n",
            "Epoch 0 Step 1148/1563 Loss: 2.135 | Acc: 20.763% (7634/36768)\n",
            "Epoch 0 Step 1149/1563 Loss: 2.135 | Acc: 20.764% (7641/36800)\n",
            "Epoch 0 Step 1150/1563 Loss: 2.135 | Acc: 20.767% (7649/36832)\n",
            "Epoch 0 Step 1151/1563 Loss: 2.135 | Acc: 20.766% (7655/36864)\n",
            "Epoch 0 Step 1152/1563 Loss: 2.135 | Acc: 20.775% (7665/36896)\n",
            "Epoch 0 Step 1153/1563 Loss: 2.135 | Acc: 20.776% (7672/36928)\n",
            "Epoch 0 Step 1154/1563 Loss: 2.135 | Acc: 20.774% (7678/36960)\n",
            "Epoch 0 Step 1155/1563 Loss: 2.135 | Acc: 20.769% (7683/36992)\n",
            "Epoch 0 Step 1156/1563 Loss: 2.134 | Acc: 20.778% (7693/37024)\n",
            "Epoch 0 Step 1157/1563 Loss: 2.134 | Acc: 20.774% (7698/37056)\n",
            "Epoch 0 Step 1158/1563 Loss: 2.134 | Acc: 20.786% (7709/37088)\n",
            "Epoch 0 Step 1159/1563 Loss: 2.134 | Acc: 20.781% (7714/37120)\n",
            "Epoch 0 Step 1160/1563 Loss: 2.134 | Acc: 20.785% (7722/37152)\n",
            "Epoch 0 Step 1161/1563 Loss: 2.134 | Acc: 20.789% (7730/37184)\n",
            "Epoch 0 Step 1162/1563 Loss: 2.134 | Acc: 20.792% (7738/37216)\n",
            "Epoch 0 Step 1163/1563 Loss: 2.134 | Acc: 20.793% (7745/37248)\n",
            "Epoch 0 Step 1164/1563 Loss: 2.134 | Acc: 20.807% (7757/37280)\n",
            "Epoch 0 Step 1165/1563 Loss: 2.134 | Acc: 20.816% (7767/37312)\n",
            "Epoch 0 Step 1166/1563 Loss: 2.134 | Acc: 20.812% (7772/37344)\n",
            "Epoch 0 Step 1167/1563 Loss: 2.134 | Acc: 20.826% (7784/37376)\n",
            "Epoch 0 Step 1168/1563 Loss: 2.134 | Acc: 20.824% (7790/37408)\n",
            "Epoch 0 Step 1169/1563 Loss: 2.134 | Acc: 20.817% (7794/37440)\n",
            "Epoch 0 Step 1170/1563 Loss: 2.134 | Acc: 20.816% (7800/37472)\n",
            "Epoch 0 Step 1171/1563 Loss: 2.134 | Acc: 20.822% (7809/37504)\n",
            "Epoch 0 Step 1172/1563 Loss: 2.134 | Acc: 20.825% (7817/37536)\n",
            "Epoch 0 Step 1173/1563 Loss: 2.134 | Acc: 20.818% (7821/37568)\n",
            "Epoch 0 Step 1174/1563 Loss: 2.134 | Acc: 20.822% (7829/37600)\n",
            "Epoch 0 Step 1175/1563 Loss: 2.133 | Acc: 20.839% (7842/37632)\n",
            "Epoch 0 Step 1176/1563 Loss: 2.133 | Acc: 20.837% (7848/37664)\n",
            "Epoch 0 Step 1177/1563 Loss: 2.133 | Acc: 20.843% (7857/37696)\n",
            "Epoch 0 Step 1178/1563 Loss: 2.133 | Acc: 20.847% (7865/37728)\n",
            "Epoch 0 Step 1179/1563 Loss: 2.133 | Acc: 20.855% (7875/37760)\n",
            "Epoch 0 Step 1180/1563 Loss: 2.133 | Acc: 20.864% (7885/37792)\n",
            "Epoch 0 Step 1181/1563 Loss: 2.133 | Acc: 20.868% (7893/37824)\n",
            "Epoch 0 Step 1182/1563 Loss: 2.133 | Acc: 20.863% (7898/37856)\n",
            "Epoch 0 Step 1183/1563 Loss: 2.133 | Acc: 20.869% (7907/37888)\n",
            "Epoch 0 Step 1184/1563 Loss: 2.133 | Acc: 20.876% (7916/37920)\n",
            "Epoch 0 Step 1185/1563 Loss: 2.132 | Acc: 20.879% (7924/37952)\n",
            "Epoch 0 Step 1186/1563 Loss: 2.132 | Acc: 20.882% (7932/37984)\n",
            "Epoch 0 Step 1187/1563 Loss: 2.132 | Acc: 20.875% (7936/38016)\n",
            "Epoch 0 Step 1188/1563 Loss: 2.132 | Acc: 20.868% (7940/38048)\n",
            "Epoch 0 Step 1189/1563 Loss: 2.132 | Acc: 20.874% (7949/38080)\n",
            "Epoch 0 Step 1190/1563 Loss: 2.132 | Acc: 20.881% (7958/38112)\n",
            "Epoch 0 Step 1191/1563 Loss: 2.132 | Acc: 20.900% (7972/38144)\n",
            "Epoch 0 Step 1192/1563 Loss: 2.131 | Acc: 20.911% (7983/38176)\n",
            "Epoch 0 Step 1193/1563 Loss: 2.131 | Acc: 20.912% (7990/38208)\n",
            "Epoch 0 Step 1194/1563 Loss: 2.131 | Acc: 20.926% (8002/38240)\n",
            "Epoch 0 Step 1195/1563 Loss: 2.131 | Acc: 20.927% (8009/38272)\n",
            "Epoch 0 Step 1196/1563 Loss: 2.131 | Acc: 20.925% (8015/38304)\n",
            "Epoch 0 Step 1197/1563 Loss: 2.131 | Acc: 20.926% (8022/38336)\n",
            "Epoch 0 Step 1198/1563 Loss: 2.131 | Acc: 20.921% (8027/38368)\n",
            "Epoch 0 Step 1199/1563 Loss: 2.131 | Acc: 20.930% (8037/38400)\n",
            "Epoch 0 Step 1200/1563 Loss: 2.131 | Acc: 20.936% (8046/38432)\n",
            "Epoch 0 Step 1201/1563 Loss: 2.131 | Acc: 20.934% (8052/38464)\n",
            "Epoch 0 Step 1202/1563 Loss: 2.131 | Acc: 20.940% (8061/38496)\n",
            "Epoch 0 Step 1203/1563 Loss: 2.131 | Acc: 20.948% (8071/38528)\n",
            "Epoch 0 Step 1204/1563 Loss: 2.131 | Acc: 20.941% (8075/38560)\n",
            "Epoch 0 Step 1205/1563 Loss: 2.131 | Acc: 20.942% (8082/38592)\n",
            "Epoch 0 Step 1206/1563 Loss: 2.131 | Acc: 20.953% (8093/38624)\n",
            "Epoch 0 Step 1207/1563 Loss: 2.131 | Acc: 20.951% (8099/38656)\n",
            "Epoch 0 Step 1208/1563 Loss: 2.131 | Acc: 20.963% (8110/38688)\n",
            "Epoch 0 Step 1209/1563 Loss: 2.131 | Acc: 20.963% (8117/38720)\n",
            "Epoch 0 Step 1210/1563 Loss: 2.131 | Acc: 20.969% (8126/38752)\n",
            "Epoch 0 Step 1211/1563 Loss: 2.130 | Acc: 20.983% (8138/38784)\n",
            "Epoch 0 Step 1212/1563 Loss: 2.130 | Acc: 20.986% (8146/38816)\n",
            "Epoch 0 Step 1213/1563 Loss: 2.130 | Acc: 20.984% (8152/38848)\n",
            "Epoch 0 Step 1214/1563 Loss: 2.130 | Acc: 20.990% (8161/38880)\n",
            "Epoch 0 Step 1215/1563 Loss: 2.130 | Acc: 20.983% (8165/38912)\n",
            "Epoch 0 Step 1216/1563 Loss: 2.130 | Acc: 20.989% (8174/38944)\n",
            "Epoch 0 Step 1217/1563 Loss: 2.130 | Acc: 20.990% (8181/38976)\n",
            "Epoch 0 Step 1218/1563 Loss: 2.130 | Acc: 20.998% (8191/39008)\n",
            "Epoch 0 Step 1219/1563 Loss: 2.130 | Acc: 21.007% (8201/39040)\n",
            "Epoch 0 Step 1220/1563 Loss: 2.130 | Acc: 21.002% (8206/39072)\n",
            "Epoch 0 Step 1221/1563 Loss: 2.130 | Acc: 21.003% (8213/39104)\n",
            "Epoch 0 Step 1222/1563 Loss: 2.130 | Acc: 21.009% (8222/39136)\n",
            "Epoch 0 Step 1223/1563 Loss: 2.130 | Acc: 21.004% (8227/39168)\n",
            "Epoch 0 Step 1224/1563 Loss: 2.130 | Acc: 20.997% (8231/39200)\n",
            "Epoch 0 Step 1225/1563 Loss: 2.130 | Acc: 20.998% (8238/39232)\n",
            "Epoch 0 Step 1226/1563 Loss: 2.130 | Acc: 21.004% (8247/39264)\n",
            "Epoch 0 Step 1227/1563 Loss: 2.130 | Acc: 20.992% (8249/39296)\n",
            "Epoch 0 Step 1228/1563 Loss: 2.130 | Acc: 20.990% (8255/39328)\n",
            "Epoch 0 Step 1229/1563 Loss: 2.130 | Acc: 21.004% (8267/39360)\n",
            "Epoch 0 Step 1230/1563 Loss: 2.130 | Acc: 20.994% (8270/39392)\n",
            "Epoch 0 Step 1231/1563 Loss: 2.130 | Acc: 21.002% (8280/39424)\n",
            "Epoch 0 Step 1232/1563 Loss: 2.129 | Acc: 21.006% (8288/39456)\n",
            "Epoch 0 Step 1233/1563 Loss: 2.129 | Acc: 21.014% (8298/39488)\n",
            "Epoch 0 Step 1234/1563 Loss: 2.129 | Acc: 21.020% (8307/39520)\n",
            "Epoch 0 Step 1235/1563 Loss: 2.129 | Acc: 21.020% (8314/39552)\n",
            "Epoch 0 Step 1236/1563 Loss: 2.129 | Acc: 21.016% (8319/39584)\n",
            "Epoch 0 Step 1237/1563 Loss: 2.129 | Acc: 21.014% (8325/39616)\n",
            "Epoch 0 Step 1238/1563 Loss: 2.129 | Acc: 21.012% (8331/39648)\n",
            "Epoch 0 Step 1239/1563 Loss: 2.129 | Acc: 21.021% (8341/39680)\n",
            "Epoch 0 Step 1240/1563 Loss: 2.129 | Acc: 21.024% (8349/39712)\n",
            "Epoch 0 Step 1241/1563 Loss: 2.129 | Acc: 21.017% (8353/39744)\n",
            "Epoch 0 Step 1242/1563 Loss: 2.129 | Acc: 21.025% (8363/39776)\n",
            "Epoch 0 Step 1243/1563 Loss: 2.129 | Acc: 21.023% (8369/39808)\n",
            "Epoch 0 Step 1244/1563 Loss: 2.129 | Acc: 21.022% (8375/39840)\n",
            "Epoch 0 Step 1245/1563 Loss: 2.129 | Acc: 21.022% (8382/39872)\n",
            "Epoch 0 Step 1246/1563 Loss: 2.129 | Acc: 21.018% (8387/39904)\n",
            "Epoch 0 Step 1247/1563 Loss: 2.129 | Acc: 21.024% (8396/39936)\n",
            "Epoch 0 Step 1248/1563 Loss: 2.128 | Acc: 21.027% (8404/39968)\n",
            "Epoch 0 Step 1249/1563 Loss: 2.128 | Acc: 21.030% (8412/40000)\n",
            "Epoch 0 Step 1250/1563 Loss: 2.128 | Acc: 21.036% (8421/40032)\n",
            "Epoch 0 Step 1251/1563 Loss: 2.128 | Acc: 21.046% (8432/40064)\n",
            "Epoch 0 Step 1252/1563 Loss: 2.128 | Acc: 21.044% (8438/40096)\n",
            "Epoch 0 Step 1253/1563 Loss: 2.128 | Acc: 21.053% (8448/40128)\n",
            "Epoch 0 Step 1254/1563 Loss: 2.128 | Acc: 21.056% (8456/40160)\n",
            "Epoch 0 Step 1255/1563 Loss: 2.128 | Acc: 21.069% (8468/40192)\n",
            "Epoch 0 Step 1256/1563 Loss: 2.128 | Acc: 21.072% (8476/40224)\n",
            "Epoch 0 Step 1257/1563 Loss: 2.128 | Acc: 21.070% (8482/40256)\n",
            "Epoch 0 Step 1258/1563 Loss: 2.128 | Acc: 21.078% (8492/40288)\n",
            "Epoch 0 Step 1259/1563 Loss: 2.128 | Acc: 21.071% (8496/40320)\n",
            "Epoch 0 Step 1260/1563 Loss: 2.128 | Acc: 21.072% (8503/40352)\n",
            "Epoch 0 Step 1261/1563 Loss: 2.128 | Acc: 21.068% (8508/40384)\n",
            "Epoch 0 Step 1262/1563 Loss: 2.128 | Acc: 21.068% (8515/40416)\n",
            "Epoch 0 Step 1263/1563 Loss: 2.128 | Acc: 21.071% (8523/40448)\n",
            "Epoch 0 Step 1264/1563 Loss: 2.128 | Acc: 21.082% (8534/40480)\n",
            "Epoch 0 Step 1265/1563 Loss: 2.128 | Acc: 21.083% (8541/40512)\n",
            "Epoch 0 Step 1266/1563 Loss: 2.128 | Acc: 21.078% (8546/40544)\n",
            "Epoch 0 Step 1267/1563 Loss: 2.128 | Acc: 21.079% (8553/40576)\n",
            "Epoch 0 Step 1268/1563 Loss: 2.128 | Acc: 21.077% (8559/40608)\n",
            "Epoch 0 Step 1269/1563 Loss: 2.128 | Acc: 21.090% (8571/40640)\n",
            "Epoch 0 Step 1270/1563 Loss: 2.127 | Acc: 21.088% (8577/40672)\n",
            "Epoch 0 Step 1271/1563 Loss: 2.127 | Acc: 21.106% (8591/40704)\n",
            "Epoch 0 Step 1272/1563 Loss: 2.127 | Acc: 21.116% (8602/40736)\n",
            "Epoch 0 Step 1273/1563 Loss: 2.127 | Acc: 21.117% (8609/40768)\n",
            "Epoch 0 Step 1274/1563 Loss: 2.127 | Acc: 21.123% (8618/40800)\n",
            "Epoch 0 Step 1275/1563 Loss: 2.127 | Acc: 21.133% (8629/40832)\n",
            "Epoch 0 Step 1276/1563 Loss: 2.127 | Acc: 21.134% (8636/40864)\n",
            "Epoch 0 Step 1277/1563 Loss: 2.127 | Acc: 21.139% (8645/40896)\n",
            "Epoch 0 Step 1278/1563 Loss: 2.127 | Acc: 21.135% (8650/40928)\n",
            "Epoch 0 Step 1279/1563 Loss: 2.127 | Acc: 21.145% (8661/40960)\n",
            "Epoch 0 Step 1280/1563 Loss: 2.127 | Acc: 21.155% (8672/40992)\n",
            "Epoch 0 Step 1281/1563 Loss: 2.127 | Acc: 21.156% (8679/41024)\n",
            "Epoch 0 Step 1282/1563 Loss: 2.127 | Acc: 21.159% (8687/41056)\n",
            "Epoch 0 Step 1283/1563 Loss: 2.127 | Acc: 21.164% (8696/41088)\n",
            "Epoch 0 Step 1284/1563 Loss: 2.126 | Acc: 21.162% (8702/41120)\n",
            "Epoch 0 Step 1285/1563 Loss: 2.126 | Acc: 21.163% (8709/41152)\n",
            "Epoch 0 Step 1286/1563 Loss: 2.126 | Acc: 21.171% (8719/41184)\n",
            "Epoch 0 Step 1287/1563 Loss: 2.126 | Acc: 21.174% (8727/41216)\n",
            "Epoch 0 Step 1288/1563 Loss: 2.126 | Acc: 21.184% (8738/41248)\n",
            "Epoch 0 Step 1289/1563 Loss: 2.126 | Acc: 21.194% (8749/41280)\n",
            "Epoch 0 Step 1290/1563 Loss: 2.126 | Acc: 21.195% (8756/41312)\n",
            "Epoch 0 Step 1291/1563 Loss: 2.125 | Acc: 21.198% (8764/41344)\n",
            "Epoch 0 Step 1292/1563 Loss: 2.125 | Acc: 21.213% (8777/41376)\n",
            "Epoch 0 Step 1293/1563 Loss: 2.125 | Acc: 21.213% (8784/41408)\n",
            "Epoch 0 Step 1294/1563 Loss: 2.125 | Acc: 21.216% (8792/41440)\n",
            "Epoch 0 Step 1295/1563 Loss: 2.125 | Acc: 21.207% (8795/41472)\n",
            "Epoch 0 Step 1296/1563 Loss: 2.125 | Acc: 21.208% (8802/41504)\n",
            "Epoch 0 Step 1297/1563 Loss: 2.125 | Acc: 21.206% (8808/41536)\n",
            "Epoch 0 Step 1298/1563 Loss: 2.125 | Acc: 21.216% (8819/41568)\n",
            "Epoch 0 Step 1299/1563 Loss: 2.125 | Acc: 21.219% (8827/41600)\n",
            "Epoch 0 Step 1300/1563 Loss: 2.125 | Acc: 21.212% (8831/41632)\n",
            "Epoch 0 Step 1301/1563 Loss: 2.125 | Acc: 21.210% (8837/41664)\n",
            "Epoch 0 Step 1302/1563 Loss: 2.125 | Acc: 21.220% (8848/41696)\n",
            "Epoch 0 Step 1303/1563 Loss: 2.125 | Acc: 21.228% (8858/41728)\n",
            "Epoch 0 Step 1304/1563 Loss: 2.125 | Acc: 21.228% (8865/41760)\n",
            "Epoch 0 Step 1305/1563 Loss: 2.125 | Acc: 21.231% (8873/41792)\n",
            "Epoch 0 Step 1306/1563 Loss: 2.125 | Acc: 21.232% (8880/41824)\n",
            "Epoch 0 Step 1307/1563 Loss: 2.125 | Acc: 21.230% (8886/41856)\n",
            "Epoch 0 Step 1308/1563 Loss: 2.125 | Acc: 21.226% (8891/41888)\n",
            "Epoch 0 Step 1309/1563 Loss: 2.124 | Acc: 21.233% (8901/41920)\n",
            "Epoch 0 Step 1310/1563 Loss: 2.125 | Acc: 21.231% (8907/41952)\n",
            "Epoch 0 Step 1311/1563 Loss: 2.124 | Acc: 21.241% (8918/41984)\n",
            "Epoch 0 Step 1312/1563 Loss: 2.124 | Acc: 21.247% (8927/42016)\n",
            "Epoch 0 Step 1313/1563 Loss: 2.124 | Acc: 21.242% (8932/42048)\n",
            "Epoch 0 Step 1314/1563 Loss: 2.124 | Acc: 21.243% (8939/42080)\n",
            "Epoch 0 Step 1315/1563 Loss: 2.124 | Acc: 21.236% (8943/42112)\n",
            "Epoch 0 Step 1316/1563 Loss: 2.124 | Acc: 21.249% (8955/42144)\n",
            "Epoch 0 Step 1317/1563 Loss: 2.124 | Acc: 21.247% (8961/42176)\n",
            "Epoch 0 Step 1318/1563 Loss: 2.124 | Acc: 21.254% (8971/42208)\n",
            "Epoch 0 Step 1319/1563 Loss: 2.124 | Acc: 21.259% (8980/42240)\n",
            "Epoch 0 Step 1320/1563 Loss: 2.124 | Acc: 21.258% (8986/42272)\n",
            "Epoch 0 Step 1321/1563 Loss: 2.124 | Acc: 21.263% (8995/42304)\n",
            "Epoch 0 Step 1322/1563 Loss: 2.124 | Acc: 21.263% (9002/42336)\n",
            "Epoch 0 Step 1323/1563 Loss: 2.124 | Acc: 21.268% (9011/42368)\n",
            "Epoch 0 Step 1324/1563 Loss: 2.124 | Acc: 21.276% (9021/42400)\n",
            "Epoch 0 Step 1325/1563 Loss: 2.124 | Acc: 21.283% (9031/42432)\n",
            "Epoch 0 Step 1326/1563 Loss: 2.124 | Acc: 21.284% (9038/42464)\n",
            "Epoch 0 Step 1327/1563 Loss: 2.123 | Acc: 21.291% (9048/42496)\n",
            "Epoch 0 Step 1328/1563 Loss: 2.124 | Acc: 21.290% (9054/42528)\n",
            "Epoch 0 Step 1329/1563 Loss: 2.124 | Acc: 21.290% (9061/42560)\n",
            "Epoch 0 Step 1330/1563 Loss: 2.124 | Acc: 21.288% (9067/42592)\n",
            "Epoch 0 Step 1331/1563 Loss: 2.123 | Acc: 21.298% (9078/42624)\n",
            "Epoch 0 Step 1332/1563 Loss: 2.123 | Acc: 21.312% (9091/42656)\n",
            "Epoch 0 Step 1333/1563 Loss: 2.123 | Acc: 21.317% (9100/42688)\n",
            "Epoch 0 Step 1334/1563 Loss: 2.123 | Acc: 21.323% (9109/42720)\n",
            "Epoch 0 Step 1335/1563 Loss: 2.123 | Acc: 21.325% (9117/42752)\n",
            "Epoch 0 Step 1336/1563 Loss: 2.123 | Acc: 21.330% (9126/42784)\n",
            "Epoch 0 Step 1337/1563 Loss: 2.123 | Acc: 21.331% (9133/42816)\n",
            "Epoch 0 Step 1338/1563 Loss: 2.123 | Acc: 21.341% (9144/42848)\n",
            "Epoch 0 Step 1339/1563 Loss: 2.122 | Acc: 21.341% (9151/42880)\n",
            "Epoch 0 Step 1340/1563 Loss: 2.122 | Acc: 21.346% (9160/42912)\n",
            "Epoch 0 Step 1341/1563 Loss: 2.122 | Acc: 21.337% (9163/42944)\n",
            "Epoch 0 Step 1342/1563 Loss: 2.122 | Acc: 21.351% (9176/42976)\n",
            "Epoch 0 Step 1343/1563 Loss: 2.122 | Acc: 21.361% (9187/43008)\n",
            "Epoch 0 Step 1344/1563 Loss: 2.122 | Acc: 21.362% (9194/43040)\n",
            "Epoch 0 Step 1345/1563 Loss: 2.122 | Acc: 21.367% (9203/43072)\n",
            "Epoch 0 Step 1346/1563 Loss: 2.122 | Acc: 21.369% (9211/43104)\n",
            "Epoch 0 Step 1347/1563 Loss: 2.122 | Acc: 21.374% (9220/43136)\n",
            "Epoch 0 Step 1348/1563 Loss: 2.122 | Acc: 21.377% (9228/43168)\n",
            "Epoch 0 Step 1349/1563 Loss: 2.122 | Acc: 21.375% (9234/43200)\n",
            "Epoch 0 Step 1350/1563 Loss: 2.122 | Acc: 21.387% (9246/43232)\n",
            "Epoch 0 Step 1351/1563 Loss: 2.122 | Acc: 21.385% (9252/43264)\n",
            "Epoch 0 Step 1352/1563 Loss: 2.122 | Acc: 21.385% (9259/43296)\n",
            "Epoch 0 Step 1353/1563 Loss: 2.122 | Acc: 21.386% (9266/43328)\n",
            "Epoch 0 Step 1354/1563 Loss: 2.121 | Acc: 21.391% (9275/43360)\n",
            "Epoch 0 Step 1355/1563 Loss: 2.121 | Acc: 21.400% (9286/43392)\n",
            "Epoch 0 Step 1356/1563 Loss: 2.121 | Acc: 21.396% (9291/43424)\n",
            "Epoch 0 Step 1357/1563 Loss: 2.121 | Acc: 21.392% (9296/43456)\n",
            "Epoch 0 Step 1358/1563 Loss: 2.121 | Acc: 21.388% (9301/43488)\n",
            "Epoch 0 Step 1359/1563 Loss: 2.121 | Acc: 21.386% (9307/43520)\n",
            "Epoch 0 Step 1360/1563 Loss: 2.121 | Acc: 21.397% (9319/43552)\n",
            "Epoch 0 Step 1361/1563 Loss: 2.121 | Acc: 21.398% (9326/43584)\n",
            "Epoch 0 Step 1362/1563 Loss: 2.121 | Acc: 21.396% (9332/43616)\n",
            "Epoch 0 Step 1363/1563 Loss: 2.121 | Acc: 21.401% (9341/43648)\n",
            "Epoch 0 Step 1364/1563 Loss: 2.121 | Acc: 21.406% (9350/43680)\n",
            "Epoch 0 Step 1365/1563 Loss: 2.121 | Acc: 21.401% (9355/43712)\n",
            "Epoch 0 Step 1366/1563 Loss: 2.121 | Acc: 21.416% (9368/43744)\n",
            "Epoch 0 Step 1367/1563 Loss: 2.121 | Acc: 21.414% (9374/43776)\n",
            "Epoch 0 Step 1368/1563 Loss: 2.120 | Acc: 21.418% (9383/43808)\n",
            "Epoch 0 Step 1369/1563 Loss: 2.120 | Acc: 21.428% (9394/43840)\n",
            "Epoch 0 Step 1370/1563 Loss: 2.120 | Acc: 21.437% (9405/43872)\n",
            "Epoch 0 Step 1371/1563 Loss: 2.120 | Acc: 21.435% (9411/43904)\n",
            "Epoch 0 Step 1372/1563 Loss: 2.120 | Acc: 21.440% (9420/43936)\n",
            "Epoch 0 Step 1373/1563 Loss: 2.120 | Acc: 21.436% (9425/43968)\n",
            "Epoch 0 Step 1374/1563 Loss: 2.120 | Acc: 21.439% (9433/44000)\n",
            "Epoch 0 Step 1375/1563 Loss: 2.120 | Acc: 21.437% (9439/44032)\n",
            "Epoch 0 Step 1376/1563 Loss: 2.120 | Acc: 21.444% (9449/44064)\n",
            "Epoch 0 Step 1377/1563 Loss: 2.120 | Acc: 21.451% (9459/44096)\n",
            "Epoch 0 Step 1378/1563 Loss: 2.120 | Acc: 21.447% (9464/44128)\n",
            "Epoch 0 Step 1379/1563 Loss: 2.120 | Acc: 21.456% (9475/44160)\n",
            "Epoch 0 Step 1380/1563 Loss: 2.119 | Acc: 21.463% (9485/44192)\n",
            "Epoch 0 Step 1381/1563 Loss: 2.119 | Acc: 21.466% (9493/44224)\n",
            "Epoch 0 Step 1382/1563 Loss: 2.119 | Acc: 21.473% (9503/44256)\n",
            "Epoch 0 Step 1383/1563 Loss: 2.119 | Acc: 21.471% (9509/44288)\n",
            "Epoch 0 Step 1384/1563 Loss: 2.119 | Acc: 21.476% (9518/44320)\n",
            "Epoch 0 Step 1385/1563 Loss: 2.119 | Acc: 21.476% (9525/44352)\n",
            "Epoch 0 Step 1386/1563 Loss: 2.119 | Acc: 21.476% (9532/44384)\n",
            "Epoch 0 Step 1387/1563 Loss: 2.119 | Acc: 21.474% (9538/44416)\n",
            "Epoch 0 Step 1388/1563 Loss: 2.119 | Acc: 21.477% (9546/44448)\n",
            "Epoch 0 Step 1389/1563 Loss: 2.119 | Acc: 21.484% (9556/44480)\n",
            "Epoch 0 Step 1390/1563 Loss: 2.118 | Acc: 21.486% (9564/44512)\n",
            "Epoch 0 Step 1391/1563 Loss: 2.118 | Acc: 21.493% (9574/44544)\n",
            "Epoch 0 Step 1392/1563 Loss: 2.119 | Acc: 21.494% (9581/44576)\n",
            "Epoch 0 Step 1393/1563 Loss: 2.118 | Acc: 21.507% (9594/44608)\n",
            "Epoch 0 Step 1394/1563 Loss: 2.118 | Acc: 21.517% (9605/44640)\n",
            "Epoch 0 Step 1395/1563 Loss: 2.118 | Acc: 21.519% (9613/44672)\n",
            "Epoch 0 Step 1396/1563 Loss: 2.118 | Acc: 21.519% (9620/44704)\n",
            "Epoch 0 Step 1397/1563 Loss: 2.118 | Acc: 21.524% (9629/44736)\n",
            "Epoch 0 Step 1398/1563 Loss: 2.118 | Acc: 21.520% (9634/44768)\n",
            "Epoch 0 Step 1399/1563 Loss: 2.118 | Acc: 21.522% (9642/44800)\n",
            "Epoch 0 Step 1400/1563 Loss: 2.118 | Acc: 21.527% (9651/44832)\n",
            "Epoch 0 Step 1401/1563 Loss: 2.118 | Acc: 21.521% (9655/44864)\n",
            "Epoch 0 Step 1402/1563 Loss: 2.118 | Acc: 21.521% (9662/44896)\n",
            "Epoch 0 Step 1403/1563 Loss: 2.118 | Acc: 21.530% (9673/44928)\n",
            "Epoch 0 Step 1404/1563 Loss: 2.118 | Acc: 21.535% (9682/44960)\n",
            "Epoch 0 Step 1405/1563 Loss: 2.117 | Acc: 21.546% (9694/44992)\n",
            "Epoch 0 Step 1406/1563 Loss: 2.117 | Acc: 21.542% (9699/45024)\n",
            "Epoch 0 Step 1407/1563 Loss: 2.117 | Acc: 21.542% (9706/45056)\n",
            "Epoch 0 Step 1408/1563 Loss: 2.117 | Acc: 21.538% (9711/45088)\n",
            "Epoch 0 Step 1409/1563 Loss: 2.117 | Acc: 21.549% (9723/45120)\n",
            "Epoch 0 Step 1410/1563 Loss: 2.117 | Acc: 21.554% (9732/45152)\n",
            "Epoch 0 Step 1411/1563 Loss: 2.117 | Acc: 21.561% (9742/45184)\n",
            "Epoch 0 Step 1412/1563 Loss: 2.117 | Acc: 21.557% (9747/45216)\n",
            "Epoch 0 Step 1413/1563 Loss: 2.117 | Acc: 21.563% (9757/45248)\n",
            "Epoch 0 Step 1414/1563 Loss: 2.117 | Acc: 21.575% (9769/45280)\n",
            "Epoch 0 Step 1415/1563 Loss: 2.117 | Acc: 21.581% (9779/45312)\n",
            "Epoch 0 Step 1416/1563 Loss: 2.117 | Acc: 21.591% (9790/45344)\n",
            "Epoch 0 Step 1417/1563 Loss: 2.117 | Acc: 21.586% (9795/45376)\n",
            "Epoch 0 Step 1418/1563 Loss: 2.117 | Acc: 21.587% (9802/45408)\n",
            "Epoch 0 Step 1419/1563 Loss: 2.117 | Acc: 21.587% (9809/45440)\n",
            "Epoch 0 Step 1420/1563 Loss: 2.117 | Acc: 21.589% (9817/45472)\n",
            "Epoch 0 Step 1421/1563 Loss: 2.116 | Acc: 21.592% (9825/45504)\n",
            "Epoch 0 Step 1422/1563 Loss: 2.116 | Acc: 21.590% (9831/45536)\n",
            "Epoch 0 Step 1423/1563 Loss: 2.116 | Acc: 21.592% (9839/45568)\n",
            "Epoch 0 Step 1424/1563 Loss: 2.116 | Acc: 21.594% (9847/45600)\n",
            "Epoch 0 Step 1425/1563 Loss: 2.116 | Acc: 21.592% (9853/45632)\n",
            "Epoch 0 Step 1426/1563 Loss: 2.116 | Acc: 21.584% (9856/45664)\n",
            "Epoch 0 Step 1427/1563 Loss: 2.116 | Acc: 21.584% (9863/45696)\n",
            "Epoch 0 Step 1428/1563 Loss: 2.116 | Acc: 21.586% (9871/45728)\n",
            "Epoch 0 Step 1429/1563 Loss: 2.115 | Acc: 21.595% (9882/45760)\n",
            "Epoch 0 Step 1430/1563 Loss: 2.115 | Acc: 21.595% (9889/45792)\n",
            "Epoch 0 Step 1431/1563 Loss: 2.115 | Acc: 21.596% (9896/45824)\n",
            "Epoch 0 Step 1432/1563 Loss: 2.115 | Acc: 21.605% (9907/45856)\n",
            "Epoch 0 Step 1433/1563 Loss: 2.115 | Acc: 21.605% (9914/45888)\n",
            "Epoch 0 Step 1434/1563 Loss: 2.115 | Acc: 21.609% (9923/45920)\n",
            "Epoch 0 Step 1435/1563 Loss: 2.115 | Acc: 21.614% (9932/45952)\n",
            "Epoch 0 Step 1436/1563 Loss: 2.115 | Acc: 21.614% (9939/45984)\n",
            "Epoch 0 Step 1437/1563 Loss: 2.115 | Acc: 21.616% (9947/46016)\n",
            "Epoch 0 Step 1438/1563 Loss: 2.115 | Acc: 21.619% (9955/46048)\n",
            "Epoch 0 Step 1439/1563 Loss: 2.115 | Acc: 21.632% (9968/46080)\n",
            "Epoch 0 Step 1440/1563 Loss: 2.115 | Acc: 21.636% (9977/46112)\n",
            "Epoch 0 Step 1441/1563 Loss: 2.115 | Acc: 21.650% (9990/46144)\n",
            "Epoch 0 Step 1442/1563 Loss: 2.114 | Acc: 21.658% (10001/46176)\n",
            "Epoch 0 Step 1443/1563 Loss: 2.114 | Acc: 21.669% (10013/46208)\n",
            "Epoch 0 Step 1444/1563 Loss: 2.114 | Acc: 21.665% (10018/46240)\n",
            "Epoch 0 Step 1445/1563 Loss: 2.114 | Acc: 21.659% (10022/46272)\n",
            "Epoch 0 Step 1446/1563 Loss: 2.114 | Acc: 21.657% (10028/46304)\n",
            "Epoch 0 Step 1447/1563 Loss: 2.114 | Acc: 21.661% (10037/46336)\n",
            "Epoch 0 Step 1448/1563 Loss: 2.114 | Acc: 21.670% (10048/46368)\n",
            "Epoch 0 Step 1449/1563 Loss: 2.114 | Acc: 21.668% (10054/46400)\n",
            "Epoch 0 Step 1450/1563 Loss: 2.114 | Acc: 21.675% (10064/46432)\n",
            "Epoch 0 Step 1451/1563 Loss: 2.114 | Acc: 21.677% (10072/46464)\n",
            "Epoch 0 Step 1452/1563 Loss: 2.114 | Acc: 21.681% (10081/46496)\n",
            "Epoch 0 Step 1453/1563 Loss: 2.113 | Acc: 21.686% (10090/46528)\n",
            "Epoch 0 Step 1454/1563 Loss: 2.113 | Acc: 21.690% (10099/46560)\n",
            "Epoch 0 Step 1455/1563 Loss: 2.113 | Acc: 21.690% (10106/46592)\n",
            "Epoch 0 Step 1456/1563 Loss: 2.113 | Acc: 21.686% (10111/46624)\n",
            "Epoch 0 Step 1457/1563 Loss: 2.113 | Acc: 21.691% (10120/46656)\n",
            "Epoch 0 Step 1458/1563 Loss: 2.113 | Acc: 21.684% (10124/46688)\n",
            "Epoch 0 Step 1459/1563 Loss: 2.113 | Acc: 21.687% (10132/46720)\n",
            "Epoch 0 Step 1460/1563 Loss: 2.113 | Acc: 21.702% (10146/46752)\n",
            "Epoch 0 Step 1461/1563 Loss: 2.113 | Acc: 21.700% (10152/46784)\n",
            "Epoch 0 Step 1462/1563 Loss: 2.113 | Acc: 21.708% (10163/46816)\n",
            "Epoch 0 Step 1463/1563 Loss: 2.113 | Acc: 21.706% (10169/46848)\n",
            "Epoch 0 Step 1464/1563 Loss: 2.113 | Acc: 21.706% (10176/46880)\n",
            "Epoch 0 Step 1465/1563 Loss: 2.113 | Acc: 21.713% (10186/46912)\n",
            "Epoch 0 Step 1466/1563 Loss: 2.113 | Acc: 21.722% (10197/46944)\n",
            "Epoch 0 Step 1467/1563 Loss: 2.113 | Acc: 21.724% (10205/46976)\n",
            "Epoch 0 Step 1468/1563 Loss: 2.113 | Acc: 21.730% (10215/47008)\n",
            "Epoch 0 Step 1469/1563 Loss: 2.113 | Acc: 21.733% (10223/47040)\n",
            "Epoch 0 Step 1470/1563 Loss: 2.113 | Acc: 21.737% (10232/47072)\n",
            "Epoch 0 Step 1471/1563 Loss: 2.113 | Acc: 21.739% (10240/47104)\n",
            "Epoch 0 Step 1472/1563 Loss: 2.113 | Acc: 21.733% (10244/47136)\n",
            "Epoch 0 Step 1473/1563 Loss: 2.113 | Acc: 21.733% (10251/47168)\n",
            "Epoch 0 Step 1474/1563 Loss: 2.113 | Acc: 21.733% (10258/47200)\n",
            "Epoch 0 Step 1475/1563 Loss: 2.113 | Acc: 21.737% (10267/47232)\n",
            "Epoch 0 Step 1476/1563 Loss: 2.112 | Acc: 21.742% (10276/47264)\n",
            "Epoch 0 Step 1477/1563 Loss: 2.112 | Acc: 21.748% (10286/47296)\n",
            "Epoch 0 Step 1478/1563 Loss: 2.112 | Acc: 21.757% (10297/47328)\n",
            "Epoch 0 Step 1479/1563 Loss: 2.112 | Acc: 21.765% (10308/47360)\n",
            "Epoch 0 Step 1480/1563 Loss: 2.112 | Acc: 21.761% (10313/47392)\n",
            "Epoch 0 Step 1481/1563 Loss: 2.112 | Acc: 21.759% (10319/47424)\n",
            "Epoch 0 Step 1482/1563 Loss: 2.112 | Acc: 21.759% (10326/47456)\n",
            "Epoch 0 Step 1483/1563 Loss: 2.112 | Acc: 21.757% (10332/47488)\n",
            "Epoch 0 Step 1484/1563 Loss: 2.112 | Acc: 21.753% (10337/47520)\n",
            "Epoch 0 Step 1485/1563 Loss: 2.112 | Acc: 21.761% (10348/47552)\n",
            "Epoch 0 Step 1486/1563 Loss: 2.112 | Acc: 21.762% (10355/47584)\n",
            "Epoch 0 Step 1487/1563 Loss: 2.112 | Acc: 21.753% (10358/47616)\n",
            "Epoch 0 Step 1488/1563 Loss: 2.111 | Acc: 21.755% (10366/47648)\n",
            "Epoch 0 Step 1489/1563 Loss: 2.111 | Acc: 21.760% (10375/47680)\n",
            "Epoch 0 Step 1490/1563 Loss: 2.111 | Acc: 21.764% (10384/47712)\n",
            "Epoch 0 Step 1491/1563 Loss: 2.111 | Acc: 21.766% (10392/47744)\n",
            "Epoch 0 Step 1492/1563 Loss: 2.111 | Acc: 21.768% (10400/47776)\n",
            "Epoch 0 Step 1493/1563 Loss: 2.111 | Acc: 21.773% (10409/47808)\n",
            "Epoch 0 Step 1494/1563 Loss: 2.111 | Acc: 21.770% (10415/47840)\n",
            "Epoch 0 Step 1495/1563 Loss: 2.111 | Acc: 21.775% (10424/47872)\n",
            "Epoch 0 Step 1496/1563 Loss: 2.111 | Acc: 21.771% (10429/47904)\n",
            "Epoch 0 Step 1497/1563 Loss: 2.111 | Acc: 21.775% (10438/47936)\n",
            "Epoch 0 Step 1498/1563 Loss: 2.111 | Acc: 21.773% (10444/47968)\n",
            "Epoch 0 Step 1499/1563 Loss: 2.111 | Acc: 21.779% (10454/48000)\n",
            "Epoch 0 Step 1500/1563 Loss: 2.111 | Acc: 21.788% (10465/48032)\n",
            "Epoch 0 Step 1501/1563 Loss: 2.111 | Acc: 21.788% (10472/48064)\n",
            "Epoch 0 Step 1502/1563 Loss: 2.111 | Acc: 21.784% (10477/48096)\n",
            "Epoch 0 Step 1503/1563 Loss: 2.111 | Acc: 21.775% (10480/48128)\n",
            "Epoch 0 Step 1504/1563 Loss: 2.111 | Acc: 21.777% (10488/48160)\n",
            "Epoch 0 Step 1505/1563 Loss: 2.111 | Acc: 21.784% (10498/48192)\n",
            "Epoch 0 Step 1506/1563 Loss: 2.111 | Acc: 21.786% (10506/48224)\n",
            "Epoch 0 Step 1507/1563 Loss: 2.111 | Acc: 21.786% (10513/48256)\n",
            "Epoch 0 Step 1508/1563 Loss: 2.111 | Acc: 21.796% (10525/48288)\n",
            "Epoch 0 Step 1509/1563 Loss: 2.111 | Acc: 21.792% (10530/48320)\n",
            "Epoch 0 Step 1510/1563 Loss: 2.111 | Acc: 21.788% (10535/48352)\n",
            "Epoch 0 Step 1511/1563 Loss: 2.111 | Acc: 21.796% (10546/48384)\n",
            "Epoch 0 Step 1512/1563 Loss: 2.111 | Acc: 21.794% (10552/48416)\n",
            "Epoch 0 Step 1513/1563 Loss: 2.111 | Acc: 21.792% (10558/48448)\n",
            "Epoch 0 Step 1514/1563 Loss: 2.111 | Acc: 21.799% (10568/48480)\n",
            "Epoch 0 Step 1515/1563 Loss: 2.110 | Acc: 21.793% (10572/48512)\n",
            "Epoch 0 Step 1516/1563 Loss: 2.110 | Acc: 21.795% (10580/48544)\n",
            "Epoch 0 Step 1517/1563 Loss: 2.110 | Acc: 21.795% (10587/48576)\n",
            "Epoch 0 Step 1518/1563 Loss: 2.110 | Acc: 21.791% (10592/48608)\n",
            "Epoch 0 Step 1519/1563 Loss: 2.110 | Acc: 21.793% (10600/48640)\n",
            "Epoch 0 Step 1520/1563 Loss: 2.110 | Acc: 21.795% (10608/48672)\n",
            "Epoch 0 Step 1521/1563 Loss: 2.110 | Acc: 21.787% (10611/48704)\n",
            "Epoch 0 Step 1522/1563 Loss: 2.110 | Acc: 21.783% (10616/48736)\n",
            "Epoch 0 Step 1523/1563 Loss: 2.110 | Acc: 21.789% (10626/48768)\n",
            "Epoch 0 Step 1524/1563 Loss: 2.110 | Acc: 21.781% (10629/48800)\n",
            "Epoch 0 Step 1525/1563 Loss: 2.110 | Acc: 21.781% (10636/48832)\n",
            "Epoch 0 Step 1526/1563 Loss: 2.110 | Acc: 21.787% (10646/48864)\n",
            "Epoch 0 Step 1527/1563 Loss: 2.110 | Acc: 21.785% (10652/48896)\n",
            "Epoch 0 Step 1528/1563 Loss: 2.110 | Acc: 21.783% (10658/48928)\n",
            "Epoch 0 Step 1529/1563 Loss: 2.110 | Acc: 21.779% (10663/48960)\n",
            "Epoch 0 Step 1530/1563 Loss: 2.110 | Acc: 21.781% (10671/48992)\n",
            "Epoch 0 Step 1531/1563 Loss: 2.110 | Acc: 21.777% (10676/49024)\n",
            "Epoch 0 Step 1532/1563 Loss: 2.110 | Acc: 21.787% (10688/49056)\n",
            "Epoch 0 Step 1533/1563 Loss: 2.110 | Acc: 21.791% (10697/49088)\n",
            "Epoch 0 Step 1534/1563 Loss: 2.109 | Acc: 21.798% (10707/49120)\n",
            "Epoch 0 Step 1535/1563 Loss: 2.110 | Acc: 21.792% (10711/49152)\n",
            "Epoch 0 Step 1536/1563 Loss: 2.110 | Acc: 21.792% (10718/49184)\n",
            "Epoch 0 Step 1537/1563 Loss: 2.109 | Acc: 21.788% (10723/49216)\n",
            "Epoch 0 Step 1538/1563 Loss: 2.109 | Acc: 21.792% (10732/49248)\n",
            "Epoch 0 Step 1539/1563 Loss: 2.109 | Acc: 21.802% (10744/49280)\n",
            "Epoch 0 Step 1540/1563 Loss: 2.109 | Acc: 21.796% (10748/49312)\n",
            "Epoch 0 Step 1541/1563 Loss: 2.109 | Acc: 21.800% (10757/49344)\n",
            "Epoch 0 Step 1542/1563 Loss: 2.109 | Acc: 21.808% (10768/49376)\n",
            "Epoch 0 Step 1543/1563 Loss: 2.109 | Acc: 21.812% (10777/49408)\n",
            "Epoch 0 Step 1544/1563 Loss: 2.109 | Acc: 21.816% (10786/49440)\n",
            "Epoch 0 Step 1545/1563 Loss: 2.109 | Acc: 21.814% (10792/49472)\n",
            "Epoch 0 Step 1546/1563 Loss: 2.109 | Acc: 21.818% (10801/49504)\n",
            "Epoch 0 Step 1547/1563 Loss: 2.109 | Acc: 21.814% (10806/49536)\n",
            "Epoch 0 Step 1548/1563 Loss: 2.109 | Acc: 21.812% (10812/49568)\n",
            "Epoch 0 Step 1549/1563 Loss: 2.110 | Acc: 21.800% (10813/49600)\n",
            "Epoch 0 Step 1550/1563 Loss: 2.110 | Acc: 21.798% (10819/49632)\n",
            "Epoch 0 Step 1551/1563 Loss: 2.110 | Acc: 21.807% (10830/49664)\n",
            "Epoch 0 Step 1552/1563 Loss: 2.110 | Acc: 21.811% (10839/49696)\n",
            "Epoch 0 Step 1553/1563 Loss: 2.109 | Acc: 21.809% (10845/49728)\n",
            "Epoch 0 Step 1554/1563 Loss: 2.109 | Acc: 21.817% (10856/49760)\n",
            "Epoch 0 Step 1555/1563 Loss: 2.109 | Acc: 21.815% (10862/49792)\n",
            "Epoch 0 Step 1556/1563 Loss: 2.109 | Acc: 21.813% (10868/49824)\n",
            "Epoch 0 Step 1557/1563 Loss: 2.109 | Acc: 21.811% (10874/49856)\n",
            "Epoch 0 Step 1558/1563 Loss: 2.109 | Acc: 21.815% (10883/49888)\n",
            "Epoch 0 Step 1559/1563 Loss: 2.109 | Acc: 21.817% (10891/49920)\n",
            "Epoch 0 Step 1560/1563 Loss: 2.109 | Acc: 21.817% (10898/49952)\n",
            "Epoch 0 Step 1561/1563 Loss: 2.109 | Acc: 21.821% (10907/49984)\n",
            "Epoch 0 Step 1562/1563 Loss: 2.109 | Acc: 21.822% (10911/50000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/opacus/privacy_engine.py:283: UserWarning: PrivacyEngine expected a batch of size 32 but the last step received a batch of size 16. This means that the privacy analysis will be a bit more pessimistic. You can set `drop_last = True` in your PyTorch dataloader to avoid this problem completely\n",
            "  f\"PrivacyEngine expected a batch of size {self.batch_size} \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 47 Step 632/1563 Loss: 2.083 | Acc: 32.469% (6577/20256)\n",
            "Epoch 47 Step 633/1563 Loss: 2.083 | Acc: 32.472% (6588/20288)\n",
            "Epoch 47 Step 634/1563 Loss: 2.082 | Acc: 32.500% (6604/20320)\n",
            "Epoch 47 Step 635/1563 Loss: 2.083 | Acc: 32.488% (6612/20352)\n",
            "Epoch 47 Step 636/1563 Loss: 2.083 | Acc: 32.501% (6625/20384)\n",
            "Epoch 47 Step 637/1563 Loss: 2.082 | Acc: 32.509% (6637/20416)\n",
            "Epoch 47 Step 638/1563 Loss: 2.082 | Acc: 32.526% (6651/20448)\n",
            "Epoch 47 Step 639/1563 Loss: 2.082 | Acc: 32.510% (6658/20480)\n",
            "Epoch 47 Step 640/1563 Loss: 2.082 | Acc: 32.518% (6670/20512)\n",
            "Epoch 47 Step 641/1563 Loss: 2.082 | Acc: 32.511% (6679/20544)\n",
            "Epoch 47 Step 642/1563 Loss: 2.083 | Acc: 32.528% (6693/20576)\n",
            "Epoch 47 Step 643/1563 Loss: 2.083 | Acc: 32.512% (6700/20608)\n",
            "Epoch 47 Step 644/1563 Loss: 2.083 | Acc: 32.524% (6713/20640)\n",
            "Epoch 47 Step 645/1563 Loss: 2.084 | Acc: 32.508% (6720/20672)\n",
            "Epoch 47 Step 646/1563 Loss: 2.084 | Acc: 32.501% (6729/20704)\n",
            "Epoch 47 Step 647/1563 Loss: 2.085 | Acc: 32.489% (6737/20736)\n",
            "Epoch 47 Step 648/1563 Loss: 2.085 | Acc: 32.487% (6747/20768)\n",
            "Epoch 47 Step 649/1563 Loss: 2.085 | Acc: 32.495% (6759/20800)\n",
            "Epoch 47 Step 650/1563 Loss: 2.084 | Acc: 32.517% (6774/20832)\n",
            "Epoch 47 Step 651/1563 Loss: 2.083 | Acc: 32.515% (6784/20864)\n",
            "Epoch 47 Step 652/1563 Loss: 2.083 | Acc: 32.528% (6797/20896)\n",
            "Epoch 47 Step 653/1563 Loss: 2.084 | Acc: 32.531% (6808/20928)\n",
            "Epoch 47 Step 654/1563 Loss: 2.083 | Acc: 32.524% (6817/20960)\n",
            "Epoch 47 Step 655/1563 Loss: 2.084 | Acc: 32.498% (6822/20992)\n",
            "Epoch 47 Step 656/1563 Loss: 2.084 | Acc: 32.487% (6830/21024)\n",
            "Epoch 47 Step 657/1563 Loss: 2.084 | Acc: 32.485% (6840/21056)\n",
            "Epoch 47 Step 658/1563 Loss: 2.084 | Acc: 32.488% (6851/21088)\n",
            "Epoch 47 Step 659/1563 Loss: 2.084 | Acc: 32.495% (6863/21120)\n",
            "Epoch 47 Step 660/1563 Loss: 2.084 | Acc: 32.498% (6874/21152)\n",
            "Epoch 47 Step 661/1563 Loss: 2.085 | Acc: 32.487% (6882/21184)\n",
            "Epoch 47 Step 662/1563 Loss: 2.085 | Acc: 32.475% (6890/21216)\n",
            "Epoch 47 Step 663/1563 Loss: 2.085 | Acc: 32.483% (6902/21248)\n",
            "Epoch 47 Step 664/1563 Loss: 2.085 | Acc: 32.462% (6908/21280)\n",
            "Epoch 47 Step 665/1563 Loss: 2.085 | Acc: 32.461% (6918/21312)\n",
            "Epoch 47 Step 666/1563 Loss: 2.086 | Acc: 32.463% (6929/21344)\n",
            "Epoch 47 Step 667/1563 Loss: 2.086 | Acc: 32.457% (6938/21376)\n",
            "Epoch 47 Step 668/1563 Loss: 2.086 | Acc: 32.469% (6951/21408)\n",
            "Epoch 47 Step 669/1563 Loss: 2.086 | Acc: 32.467% (6961/21440)\n",
            "Epoch 47 Step 670/1563 Loss: 2.087 | Acc: 32.466% (6971/21472)\n",
            "Epoch 47 Step 671/1563 Loss: 2.087 | Acc: 32.450% (6978/21504)\n",
            "Epoch 47 Step 672/1563 Loss: 2.087 | Acc: 32.439% (6986/21536)\n",
            "Epoch 47 Step 673/1563 Loss: 2.088 | Acc: 32.437% (6996/21568)\n",
            "Epoch 47 Step 674/1563 Loss: 2.087 | Acc: 32.444% (7008/21600)\n",
            "Epoch 47 Step 675/1563 Loss: 2.087 | Acc: 32.461% (7022/21632)\n",
            "Epoch 47 Step 676/1563 Loss: 2.087 | Acc: 32.446% (7029/21664)\n",
            "Epoch 47 Step 677/1563 Loss: 2.087 | Acc: 32.435% (7037/21696)\n",
            "Epoch 47 Step 678/1563 Loss: 2.087 | Acc: 32.437% (7048/21728)\n",
            "Epoch 47 Step 679/1563 Loss: 2.087 | Acc: 32.454% (7062/21760)\n",
            "Epoch 47 Step 680/1563 Loss: 2.087 | Acc: 32.448% (7071/21792)\n",
            "Epoch 47 Step 681/1563 Loss: 2.086 | Acc: 32.451% (7082/21824)\n",
            "Epoch 47 Step 682/1563 Loss: 2.086 | Acc: 32.444% (7091/21856)\n",
            "Epoch 47 Step 683/1563 Loss: 2.086 | Acc: 32.433% (7099/21888)\n",
            "Epoch 47 Step 684/1563 Loss: 2.086 | Acc: 32.459% (7115/21920)\n",
            "Epoch 47 Step 685/1563 Loss: 2.086 | Acc: 32.457% (7125/21952)\n",
            "Epoch 47 Step 686/1563 Loss: 2.086 | Acc: 32.460% (7136/21984)\n",
            "Epoch 47 Step 687/1563 Loss: 2.087 | Acc: 32.454% (7145/22016)\n",
            "Epoch 47 Step 688/1563 Loss: 2.087 | Acc: 32.443% (7153/22048)\n",
            "Epoch 47 Step 689/1563 Loss: 2.087 | Acc: 32.432% (7161/22080)\n",
            "Epoch 47 Step 690/1563 Loss: 2.087 | Acc: 32.435% (7172/22112)\n",
            "Epoch 47 Step 691/1563 Loss: 2.086 | Acc: 32.447% (7185/22144)\n",
            "Epoch 47 Step 692/1563 Loss: 2.086 | Acc: 32.436% (7193/22176)\n",
            "Epoch 47 Step 693/1563 Loss: 2.086 | Acc: 32.434% (7203/22208)\n",
            "Epoch 47 Step 694/1563 Loss: 2.086 | Acc: 32.446% (7216/22240)\n",
            "Epoch 47 Step 695/1563 Loss: 2.086 | Acc: 32.440% (7225/22272)\n",
            "Epoch 47 Step 696/1563 Loss: 2.086 | Acc: 32.434% (7234/22304)\n",
            "Epoch 47 Step 697/1563 Loss: 2.086 | Acc: 32.427% (7243/22336)\n",
            "Epoch 47 Step 698/1563 Loss: 2.086 | Acc: 32.444% (7257/22368)\n",
            "Epoch 47 Step 699/1563 Loss: 2.086 | Acc: 32.442% (7267/22400)\n",
            "Epoch 47 Step 700/1563 Loss: 2.086 | Acc: 32.449% (7279/22432)\n",
            "Epoch 47 Step 701/1563 Loss: 2.086 | Acc: 32.447% (7289/22464)\n",
            "Epoch 47 Step 702/1563 Loss: 2.086 | Acc: 32.446% (7299/22496)\n",
            "Epoch 47 Step 703/1563 Loss: 2.086 | Acc: 32.440% (7308/22528)\n",
            "Epoch 47 Step 704/1563 Loss: 2.086 | Acc: 32.442% (7319/22560)\n",
            "Epoch 47 Step 705/1563 Loss: 2.087 | Acc: 32.423% (7325/22592)\n",
            "Epoch 47 Step 706/1563 Loss: 2.087 | Acc: 32.412% (7333/22624)\n",
            "Epoch 47 Step 707/1563 Loss: 2.087 | Acc: 32.411% (7343/22656)\n",
            "Epoch 47 Step 708/1563 Loss: 2.087 | Acc: 32.409% (7353/22688)\n",
            "Epoch 47 Step 709/1563 Loss: 2.088 | Acc: 32.408% (7363/22720)\n",
            "Epoch 47 Step 710/1563 Loss: 2.088 | Acc: 32.397% (7371/22752)\n",
            "Epoch 47 Step 711/1563 Loss: 2.089 | Acc: 32.387% (7379/22784)\n",
            "Epoch 47 Step 712/1563 Loss: 2.088 | Acc: 32.420% (7397/22816)\n",
            "Epoch 47 Step 713/1563 Loss: 2.087 | Acc: 32.436% (7411/22848)\n",
            "Epoch 47 Step 714/1563 Loss: 2.087 | Acc: 32.439% (7422/22880)\n",
            "Epoch 47 Step 715/1563 Loss: 2.088 | Acc: 32.424% (7429/22912)\n",
            "Epoch 47 Step 716/1563 Loss: 2.088 | Acc: 32.409% (7436/22944)\n",
            "Epoch 47 Step 717/1563 Loss: 2.088 | Acc: 32.403% (7445/22976)\n",
            "Epoch 47 Step 718/1563 Loss: 2.088 | Acc: 32.428% (7461/23008)\n",
            "Epoch 47 Step 719/1563 Loss: 2.088 | Acc: 32.444% (7475/23040)\n",
            "Epoch 47 Step 720/1563 Loss: 2.088 | Acc: 32.438% (7484/23072)\n",
            "Epoch 47 Step 721/1563 Loss: 2.089 | Acc: 32.432% (7493/23104)\n",
            "Epoch 47 Step 722/1563 Loss: 2.088 | Acc: 32.443% (7506/23136)\n",
            "Epoch 47 Step 723/1563 Loss: 2.088 | Acc: 32.454% (7519/23168)\n",
            "Epoch 47 Step 724/1563 Loss: 2.088 | Acc: 32.448% (7528/23200)\n",
            "Epoch 47 Step 725/1563 Loss: 2.088 | Acc: 32.464% (7542/23232)\n",
            "Epoch 47 Step 726/1563 Loss: 2.087 | Acc: 32.466% (7553/23264)\n",
            "Epoch 47 Step 727/1563 Loss: 2.087 | Acc: 32.486% (7568/23296)\n",
            "Epoch 47 Step 728/1563 Loss: 2.087 | Acc: 32.480% (7577/23328)\n",
            "Epoch 47 Step 729/1563 Loss: 2.087 | Acc: 32.496% (7591/23360)\n",
            "Epoch 47 Step 730/1563 Loss: 2.087 | Acc: 32.490% (7600/23392)\n",
            "Epoch 47 Step 731/1563 Loss: 2.087 | Acc: 32.488% (7610/23424)\n",
            "Epoch 47 Step 732/1563 Loss: 2.087 | Acc: 32.478% (7618/23456)\n",
            "Epoch 47 Step 733/1563 Loss: 2.087 | Acc: 32.502% (7634/23488)\n",
            "Epoch 47 Step 734/1563 Loss: 2.087 | Acc: 32.491% (7642/23520)\n",
            "Epoch 47 Step 735/1563 Loss: 2.087 | Acc: 32.498% (7654/23552)\n",
            "Epoch 47 Step 736/1563 Loss: 2.087 | Acc: 32.497% (7664/23584)\n",
            "Epoch 47 Step 737/1563 Loss: 2.087 | Acc: 32.499% (7675/23616)\n",
            "Epoch 47 Step 738/1563 Loss: 2.086 | Acc: 32.493% (7684/23648)\n",
            "Epoch 47 Step 739/1563 Loss: 2.086 | Acc: 32.500% (7696/23680)\n",
            "Epoch 47 Step 740/1563 Loss: 2.086 | Acc: 32.503% (7707/23712)\n",
            "Epoch 47 Step 741/1563 Loss: 2.086 | Acc: 32.509% (7719/23744)\n",
            "Epoch 47 Step 742/1563 Loss: 2.086 | Acc: 32.495% (7726/23776)\n",
            "Epoch 47 Step 743/1563 Loss: 2.086 | Acc: 32.485% (7734/23808)\n",
            "Epoch 47 Step 744/1563 Loss: 2.086 | Acc: 32.479% (7743/23840)\n",
            "Epoch 47 Step 745/1563 Loss: 2.086 | Acc: 32.456% (7748/23872)\n",
            "Epoch 47 Step 746/1563 Loss: 2.086 | Acc: 32.459% (7759/23904)\n",
            "Epoch 47 Step 747/1563 Loss: 2.085 | Acc: 32.457% (7769/23936)\n",
            "Epoch 47 Step 748/1563 Loss: 2.085 | Acc: 32.477% (7784/23968)\n",
            "Epoch 47 Step 749/1563 Loss: 2.085 | Acc: 32.467% (7792/24000)\n",
            "Epoch 47 Step 750/1563 Loss: 2.085 | Acc: 32.486% (7807/24032)\n",
            "Epoch 47 Step 751/1563 Loss: 2.086 | Acc: 32.476% (7815/24064)\n",
            "Epoch 47 Step 752/1563 Loss: 2.086 | Acc: 32.454% (7820/24096)\n",
            "Epoch 47 Step 753/1563 Loss: 2.086 | Acc: 32.460% (7832/24128)\n",
            "Epoch 47 Step 754/1563 Loss: 2.086 | Acc: 32.471% (7845/24160)\n",
            "Epoch 47 Step 755/1563 Loss: 2.086 | Acc: 32.494% (7861/24192)\n",
            "Epoch 47 Step 756/1563 Loss: 2.085 | Acc: 32.501% (7873/24224)\n",
            "Epoch 47 Step 757/1563 Loss: 2.085 | Acc: 32.495% (7882/24256)\n",
            "Epoch 47 Step 758/1563 Loss: 2.085 | Acc: 32.493% (7892/24288)\n",
            "Epoch 47 Step 759/1563 Loss: 2.085 | Acc: 32.500% (7904/24320)\n",
            "Epoch 47 Step 760/1563 Loss: 2.085 | Acc: 32.502% (7915/24352)\n",
            "Epoch 47 Step 761/1563 Loss: 2.086 | Acc: 32.505% (7926/24384)\n",
            "Epoch 47 Step 762/1563 Loss: 2.086 | Acc: 32.507% (7937/24416)\n",
            "Epoch 47 Step 763/1563 Loss: 2.086 | Acc: 32.514% (7949/24448)\n",
            "Epoch 47 Step 764/1563 Loss: 2.086 | Acc: 32.516% (7960/24480)\n",
            "Epoch 47 Step 765/1563 Loss: 2.086 | Acc: 32.523% (7972/24512)\n",
            "Epoch 47 Step 766/1563 Loss: 2.086 | Acc: 32.521% (7982/24544)\n",
            "Epoch 47 Step 767/1563 Loss: 2.086 | Acc: 32.536% (7996/24576)\n",
            "Epoch 47 Step 768/1563 Loss: 2.085 | Acc: 32.554% (8011/24608)\n",
            "Epoch 47 Step 769/1563 Loss: 2.085 | Acc: 32.557% (8022/24640)\n",
            "Epoch 47 Step 770/1563 Loss: 2.086 | Acc: 32.547% (8030/24672)\n",
            "Epoch 47 Step 771/1563 Loss: 2.086 | Acc: 32.553% (8042/24704)\n",
            "Epoch 47 Step 772/1563 Loss: 2.086 | Acc: 32.560% (8054/24736)\n",
            "Epoch 47 Step 773/1563 Loss: 2.086 | Acc: 32.554% (8063/24768)\n",
            "Epoch 47 Step 774/1563 Loss: 2.086 | Acc: 32.569% (8077/24800)\n",
            "Epoch 47 Step 775/1563 Loss: 2.086 | Acc: 32.579% (8090/24832)\n",
            "Epoch 47 Step 776/1563 Loss: 2.086 | Acc: 32.569% (8098/24864)\n",
            "Epoch 47 Step 777/1563 Loss: 2.086 | Acc: 32.567% (8108/24896)\n",
            "Epoch 47 Step 778/1563 Loss: 2.085 | Acc: 32.598% (8126/24928)\n",
            "Epoch 47 Step 779/1563 Loss: 2.085 | Acc: 32.592% (8135/24960)\n",
            "Epoch 47 Step 780/1563 Loss: 2.086 | Acc: 32.578% (8142/24992)\n",
            "Epoch 47 Step 781/1563 Loss: 2.085 | Acc: 32.585% (8154/25024)\n",
            "Epoch 47 Step 782/1563 Loss: 2.086 | Acc: 32.583% (8164/25056)\n",
            "Epoch 47 Step 783/1563 Loss: 2.087 | Acc: 32.573% (8172/25088)\n",
            "Epoch 47 Step 784/1563 Loss: 2.087 | Acc: 32.572% (8182/25120)\n",
            "Epoch 47 Step 785/1563 Loss: 2.087 | Acc: 32.574% (8193/25152)\n",
            "Epoch 47 Step 786/1563 Loss: 2.088 | Acc: 32.568% (8202/25184)\n",
            "Epoch 47 Step 787/1563 Loss: 2.087 | Acc: 32.575% (8214/25216)\n",
            "Epoch 47 Step 788/1563 Loss: 2.087 | Acc: 32.561% (8221/25248)\n",
            "Epoch 47 Step 789/1563 Loss: 2.087 | Acc: 32.547% (8228/25280)\n",
            "Epoch 47 Step 790/1563 Loss: 2.088 | Acc: 32.554% (8240/25312)\n",
            "Epoch 47 Step 791/1563 Loss: 2.088 | Acc: 32.556% (8251/25344)\n",
            "Epoch 47 Step 792/1563 Loss: 2.087 | Acc: 32.547% (8259/25376)\n",
            "Epoch 47 Step 793/1563 Loss: 2.088 | Acc: 32.553% (8271/25408)\n",
            "Epoch 47 Step 794/1563 Loss: 2.087 | Acc: 32.571% (8286/25440)\n",
            "Epoch 47 Step 795/1563 Loss: 2.087 | Acc: 32.581% (8299/25472)\n",
            "Epoch 47 Step 796/1563 Loss: 2.086 | Acc: 32.591% (8312/25504)\n",
            "Epoch 47 Step 797/1563 Loss: 2.087 | Acc: 32.593% (8323/25536)\n",
            "Epoch 47 Step 798/1563 Loss: 2.086 | Acc: 32.611% (8338/25568)\n",
            "Epoch 47 Step 799/1563 Loss: 2.086 | Acc: 32.617% (8350/25600)\n",
            "Epoch 47 Step 800/1563 Loss: 2.086 | Acc: 32.615% (8360/25632)\n",
            "Epoch 47 Step 801/1563 Loss: 2.086 | Acc: 32.614% (8370/25664)\n",
            "Epoch 47 Step 802/1563 Loss: 2.087 | Acc: 32.600% (8377/25696)\n",
            "Epoch 47 Step 803/1563 Loss: 2.087 | Acc: 32.603% (8388/25728)\n",
            "Epoch 47 Step 804/1563 Loss: 2.087 | Acc: 32.593% (8396/25760)\n",
            "Epoch 47 Step 805/1563 Loss: 2.087 | Acc: 32.595% (8407/25792)\n",
            "Epoch 47 Step 806/1563 Loss: 2.086 | Acc: 32.601% (8419/25824)\n",
            "Epoch 47 Step 807/1563 Loss: 2.086 | Acc: 32.604% (8430/25856)\n",
            "Epoch 47 Step 808/1563 Loss: 2.086 | Acc: 32.594% (8438/25888)\n",
            "Epoch 47 Step 809/1563 Loss: 2.086 | Acc: 32.589% (8447/25920)\n",
            "Epoch 47 Step 810/1563 Loss: 2.086 | Acc: 32.587% (8457/25952)\n",
            "Epoch 47 Step 811/1563 Loss: 2.086 | Acc: 32.597% (8470/25984)\n",
            "Epoch 47 Step 812/1563 Loss: 2.087 | Acc: 32.588% (8478/26016)\n",
            "Epoch 47 Step 813/1563 Loss: 2.087 | Acc: 32.598% (8491/26048)\n",
            "Epoch 47 Step 814/1563 Loss: 2.086 | Acc: 32.611% (8505/26080)\n",
            "Epoch 47 Step 815/1563 Loss: 2.086 | Acc: 32.617% (8517/26112)\n",
            "Epoch 47 Step 816/1563 Loss: 2.086 | Acc: 32.616% (8527/26144)\n",
            "Epoch 47 Step 817/1563 Loss: 2.085 | Acc: 32.621% (8539/26176)\n",
            "Epoch 47 Step 818/1563 Loss: 2.085 | Acc: 32.620% (8549/26208)\n",
            "Epoch 47 Step 819/1563 Loss: 2.085 | Acc: 32.618% (8559/26240)\n",
            "Epoch 47 Step 820/1563 Loss: 2.086 | Acc: 32.624% (8571/26272)\n",
            "Epoch 47 Step 821/1563 Loss: 2.086 | Acc: 32.626% (8582/26304)\n",
            "Epoch 47 Step 822/1563 Loss: 2.085 | Acc: 32.632% (8594/26336)\n",
            "Epoch 47 Step 823/1563 Loss: 2.085 | Acc: 32.627% (8603/26368)\n",
            "Epoch 47 Step 824/1563 Loss: 2.086 | Acc: 32.621% (8612/26400)\n",
            "Epoch 47 Step 825/1563 Loss: 2.086 | Acc: 32.623% (8623/26432)\n",
            "Epoch 47 Step 826/1563 Loss: 2.085 | Acc: 32.641% (8638/26464)\n",
            "Epoch 47 Step 827/1563 Loss: 2.086 | Acc: 32.631% (8646/26496)\n",
            "Epoch 47 Step 828/1563 Loss: 2.086 | Acc: 32.622% (8654/26528)\n",
            "Epoch 47 Step 829/1563 Loss: 2.086 | Acc: 32.620% (8664/26560)\n",
            "Epoch 47 Step 830/1563 Loss: 2.086 | Acc: 32.611% (8672/26592)\n",
            "Epoch 47 Step 831/1563 Loss: 2.086 | Acc: 32.602% (8680/26624)\n",
            "Epoch 47 Step 832/1563 Loss: 2.086 | Acc: 32.616% (8694/26656)\n",
            "Epoch 47 Step 833/1563 Loss: 2.086 | Acc: 32.603% (8701/26688)\n",
            "Epoch 47 Step 834/1563 Loss: 2.086 | Acc: 32.605% (8712/26720)\n",
            "Epoch 47 Step 835/1563 Loss: 2.086 | Acc: 32.596% (8720/26752)\n",
            "Epoch 47 Step 836/1563 Loss: 2.086 | Acc: 32.598% (8731/26784)\n",
            "Epoch 47 Step 837/1563 Loss: 2.086 | Acc: 32.619% (8747/26816)\n",
            "Epoch 47 Step 838/1563 Loss: 2.086 | Acc: 32.617% (8757/26848)\n",
            "Epoch 47 Step 839/1563 Loss: 2.086 | Acc: 32.608% (8765/26880)\n",
            "Epoch 47 Step 840/1563 Loss: 2.087 | Acc: 32.595% (8772/26912)\n",
            "Epoch 47 Step 841/1563 Loss: 2.087 | Acc: 32.590% (8781/26944)\n",
            "Epoch 47 Step 842/1563 Loss: 2.088 | Acc: 32.585% (8790/26976)\n",
            "Epoch 47 Step 843/1563 Loss: 2.088 | Acc: 32.583% (8800/27008)\n",
            "Epoch 47 Step 844/1563 Loss: 2.088 | Acc: 32.585% (8811/27040)\n",
            "Epoch 47 Step 845/1563 Loss: 2.087 | Acc: 32.587% (8822/27072)\n",
            "Epoch 47 Step 846/1563 Loss: 2.087 | Acc: 32.586% (8832/27104)\n",
            "Epoch 47 Step 847/1563 Loss: 2.088 | Acc: 32.580% (8841/27136)\n",
            "Epoch 47 Step 848/1563 Loss: 2.088 | Acc: 32.575% (8850/27168)\n",
            "Epoch 47 Step 849/1563 Loss: 2.087 | Acc: 32.592% (8865/27200)\n",
            "Epoch 47 Step 850/1563 Loss: 2.087 | Acc: 32.590% (8875/27232)\n",
            "Epoch 47 Step 851/1563 Loss: 2.087 | Acc: 32.585% (8884/27264)\n",
            "Epoch 47 Step 852/1563 Loss: 2.087 | Acc: 32.587% (8895/27296)\n",
            "Epoch 47 Step 853/1563 Loss: 2.087 | Acc: 32.589% (8906/27328)\n",
            "Epoch 47 Step 854/1563 Loss: 2.087 | Acc: 32.610% (8922/27360)\n",
            "Epoch 47 Step 855/1563 Loss: 2.087 | Acc: 32.608% (8932/27392)\n",
            "Epoch 47 Step 856/1563 Loss: 2.087 | Acc: 32.614% (8944/27424)\n",
            "Epoch 47 Step 857/1563 Loss: 2.087 | Acc: 32.623% (8957/27456)\n",
            "Epoch 47 Step 858/1563 Loss: 2.087 | Acc: 32.622% (8967/27488)\n",
            "Epoch 47 Step 859/1563 Loss: 2.087 | Acc: 32.624% (8978/27520)\n",
            "Epoch 47 Step 860/1563 Loss: 2.087 | Acc: 32.618% (8987/27552)\n",
            "Epoch 47 Step 861/1563 Loss: 2.088 | Acc: 32.613% (8996/27584)\n",
            "Epoch 47 Step 862/1563 Loss: 2.088 | Acc: 32.608% (9005/27616)\n",
            "Epoch 47 Step 863/1563 Loss: 2.088 | Acc: 32.617% (9018/27648)\n",
            "Epoch 47 Step 864/1563 Loss: 2.087 | Acc: 32.626% (9031/27680)\n",
            "Epoch 47 Step 865/1563 Loss: 2.088 | Acc: 32.610% (9037/27712)\n",
            "Epoch 47 Step 866/1563 Loss: 2.087 | Acc: 32.634% (9054/27744)\n",
            "Epoch 47 Step 867/1563 Loss: 2.087 | Acc: 32.636% (9065/27776)\n",
            "Epoch 47 Step 868/1563 Loss: 2.087 | Acc: 32.627% (9073/27808)\n",
            "Epoch 47 Step 869/1563 Loss: 2.087 | Acc: 32.626% (9083/27840)\n",
            "Epoch 47 Step 870/1563 Loss: 2.087 | Acc: 32.638% (9097/27872)\n",
            "Epoch 47 Step 871/1563 Loss: 2.087 | Acc: 32.640% (9108/27904)\n",
            "Epoch 47 Step 872/1563 Loss: 2.087 | Acc: 32.628% (9115/27936)\n",
            "Epoch 47 Step 873/1563 Loss: 2.087 | Acc: 32.644% (9130/27968)\n",
            "Epoch 47 Step 874/1563 Loss: 2.087 | Acc: 32.657% (9144/28000)\n",
            "Epoch 47 Step 875/1563 Loss: 2.087 | Acc: 32.659% (9155/28032)\n",
            "Epoch 47 Step 876/1563 Loss: 2.087 | Acc: 32.668% (9168/28064)\n",
            "Epoch 47 Step 877/1563 Loss: 2.087 | Acc: 32.670% (9179/28096)\n",
            "Epoch 47 Step 878/1563 Loss: 2.087 | Acc: 32.676% (9191/28128)\n",
            "Epoch 47 Step 879/1563 Loss: 2.087 | Acc: 32.670% (9200/28160)\n",
            "Epoch 47 Step 880/1563 Loss: 2.087 | Acc: 32.669% (9210/28192)\n",
            "Epoch 47 Step 881/1563 Loss: 2.087 | Acc: 32.671% (9221/28224)\n",
            "Epoch 47 Step 882/1563 Loss: 2.087 | Acc: 32.659% (9228/28256)\n",
            "Epoch 47 Step 883/1563 Loss: 2.087 | Acc: 32.657% (9238/28288)\n",
            "Epoch 47 Step 884/1563 Loss: 2.087 | Acc: 32.659% (9249/28320)\n",
            "Epoch 47 Step 885/1563 Loss: 2.087 | Acc: 32.657% (9259/28352)\n",
            "Epoch 47 Step 886/1563 Loss: 2.087 | Acc: 32.652% (9268/28384)\n",
            "Epoch 47 Step 887/1563 Loss: 2.087 | Acc: 32.668% (9283/28416)\n",
            "Epoch 47 Step 888/1563 Loss: 2.086 | Acc: 32.674% (9295/28448)\n",
            "Epoch 47 Step 889/1563 Loss: 2.086 | Acc: 32.693% (9311/28480)\n",
            "Epoch 47 Step 890/1563 Loss: 2.086 | Acc: 32.699% (9323/28512)\n",
            "Epoch 47 Step 891/1563 Loss: 2.086 | Acc: 32.714% (9338/28544)\n",
            "Epoch 47 Step 892/1563 Loss: 2.086 | Acc: 32.720% (9350/28576)\n",
            "Epoch 47 Step 893/1563 Loss: 2.086 | Acc: 32.736% (9365/28608)\n",
            "Epoch 47 Step 894/1563 Loss: 2.086 | Acc: 32.737% (9376/28640)\n",
            "Epoch 47 Step 895/1563 Loss: 2.085 | Acc: 32.743% (9388/28672)\n",
            "Epoch 47 Step 896/1563 Loss: 2.086 | Acc: 32.731% (9395/28704)\n",
            "Epoch 47 Step 897/1563 Loss: 2.086 | Acc: 32.726% (9404/28736)\n",
            "Epoch 47 Step 898/1563 Loss: 2.086 | Acc: 32.727% (9415/28768)\n",
            "Epoch 47 Step 899/1563 Loss: 2.086 | Acc: 32.729% (9426/28800)\n",
            "Epoch 47 Step 900/1563 Loss: 2.086 | Acc: 32.752% (9443/28832)\n",
            "Epoch 47 Step 901/1563 Loss: 2.086 | Acc: 32.740% (9450/28864)\n",
            "Epoch 47 Step 902/1563 Loss: 2.086 | Acc: 32.742% (9461/28896)\n",
            "Epoch 47 Step 903/1563 Loss: 2.086 | Acc: 32.747% (9473/28928)\n",
            "Epoch 47 Step 904/1563 Loss: 2.085 | Acc: 32.759% (9487/28960)\n",
            "Epoch 47 Step 905/1563 Loss: 2.085 | Acc: 32.761% (9498/28992)\n",
            "Epoch 47 Step 906/1563 Loss: 2.085 | Acc: 32.759% (9508/29024)\n",
            "Epoch 47 Step 907/1563 Loss: 2.085 | Acc: 32.754% (9517/29056)\n",
            "Epoch 47 Step 908/1563 Loss: 2.085 | Acc: 32.756% (9528/29088)\n",
            "Epoch 47 Step 909/1563 Loss: 2.085 | Acc: 32.730% (9531/29120)\n",
            "Epoch 47 Step 910/1563 Loss: 2.085 | Acc: 32.728% (9541/29152)\n",
            "Epoch 47 Step 911/1563 Loss: 2.085 | Acc: 32.734% (9553/29184)\n",
            "Epoch 47 Step 912/1563 Loss: 2.086 | Acc: 32.735% (9564/29216)\n",
            "Epoch 47 Step 913/1563 Loss: 2.085 | Acc: 32.741% (9576/29248)\n",
            "Epoch 47 Step 914/1563 Loss: 2.085 | Acc: 32.746% (9588/29280)\n",
            "Epoch 47 Step 915/1563 Loss: 2.085 | Acc: 32.751% (9600/29312)\n",
            "Epoch 47 Step 916/1563 Loss: 2.085 | Acc: 32.749% (9610/29344)\n",
            "Epoch 47 Step 917/1563 Loss: 2.085 | Acc: 32.748% (9620/29376)\n",
            "Epoch 47 Step 918/1563 Loss: 2.085 | Acc: 32.753% (9632/29408)\n",
            "Epoch 47 Step 919/1563 Loss: 2.085 | Acc: 32.755% (9643/29440)\n",
            "Epoch 47 Step 920/1563 Loss: 2.085 | Acc: 32.753% (9653/29472)\n",
            "Epoch 47 Step 921/1563 Loss: 2.084 | Acc: 32.758% (9665/29504)\n",
            "Epoch 47 Step 922/1563 Loss: 2.084 | Acc: 32.763% (9677/29536)\n",
            "Epoch 47 Step 923/1563 Loss: 2.084 | Acc: 32.762% (9687/29568)\n",
            "Epoch 47 Step 924/1563 Loss: 2.084 | Acc: 32.750% (9694/29600)\n",
            "Epoch 47 Step 925/1563 Loss: 2.084 | Acc: 32.752% (9705/29632)\n",
            "Epoch 47 Step 926/1563 Loss: 2.085 | Acc: 32.743% (9713/29664)\n",
            "Epoch 47 Step 927/1563 Loss: 2.085 | Acc: 32.742% (9723/29696)\n",
            "Epoch 47 Step 928/1563 Loss: 2.085 | Acc: 32.727% (9729/29728)\n",
            "Epoch 47 Step 929/1563 Loss: 2.085 | Acc: 32.735% (9742/29760)\n",
            "Epoch 47 Step 930/1563 Loss: 2.085 | Acc: 32.730% (9751/29792)\n",
            "Epoch 47 Step 931/1563 Loss: 2.085 | Acc: 32.739% (9764/29824)\n",
            "Epoch 47 Step 932/1563 Loss: 2.086 | Acc: 32.734% (9773/29856)\n",
            "Epoch 47 Step 933/1563 Loss: 2.086 | Acc: 32.742% (9786/29888)\n",
            "Epoch 47 Step 934/1563 Loss: 2.086 | Acc: 32.734% (9794/29920)\n",
            "Epoch 47 Step 935/1563 Loss: 2.086 | Acc: 32.742% (9807/29952)\n",
            "Epoch 47 Step 936/1563 Loss: 2.085 | Acc: 32.757% (9822/29984)\n",
            "Epoch 47 Step 937/1563 Loss: 2.086 | Acc: 32.753% (9831/30016)\n",
            "Epoch 47 Step 938/1563 Loss: 2.086 | Acc: 32.751% (9841/30048)\n",
            "Epoch 47 Step 939/1563 Loss: 2.086 | Acc: 32.733% (9846/30080)\n",
            "Epoch 47 Step 940/1563 Loss: 2.086 | Acc: 32.731% (9856/30112)\n",
            "Epoch 47 Step 941/1563 Loss: 2.087 | Acc: 32.736% (9868/30144)\n",
            "Epoch 47 Step 942/1563 Loss: 2.086 | Acc: 32.751% (9883/30176)\n",
            "Epoch 47 Step 943/1563 Loss: 2.087 | Acc: 32.743% (9891/30208)\n",
            "Epoch 47 Step 944/1563 Loss: 2.086 | Acc: 32.758% (9906/30240)\n",
            "Epoch 47 Step 945/1563 Loss: 2.086 | Acc: 32.766% (9919/30272)\n",
            "Epoch 47 Step 946/1563 Loss: 2.086 | Acc: 32.768% (9930/30304)\n",
            "Epoch 47 Step 947/1563 Loss: 2.086 | Acc: 32.766% (9940/30336)\n",
            "Epoch 47 Step 948/1563 Loss: 2.086 | Acc: 32.765% (9950/30368)\n",
            "Epoch 47 Step 949/1563 Loss: 2.086 | Acc: 32.760% (9959/30400)\n",
            "Epoch 47 Step 950/1563 Loss: 2.086 | Acc: 32.755% (9968/30432)\n",
            "Epoch 47 Step 951/1563 Loss: 2.086 | Acc: 32.747% (9976/30464)\n",
            "Epoch 47 Step 952/1563 Loss: 2.086 | Acc: 32.752% (9988/30496)\n",
            "Epoch 47 Step 953/1563 Loss: 2.086 | Acc: 32.757% (10000/30528)\n",
            "Epoch 47 Step 954/1563 Loss: 2.086 | Acc: 32.762% (10012/30560)\n",
            "Epoch 47 Step 955/1563 Loss: 2.086 | Acc: 32.757% (10021/30592)\n",
            "Epoch 47 Step 956/1563 Loss: 2.086 | Acc: 32.742% (10027/30624)\n",
            "Epoch 47 Step 957/1563 Loss: 2.087 | Acc: 32.728% (10033/30656)\n",
            "Epoch 47 Step 958/1563 Loss: 2.087 | Acc: 32.726% (10043/30688)\n",
            "Epoch 47 Step 959/1563 Loss: 2.087 | Acc: 32.712% (10049/30720)\n",
            "Epoch 47 Step 960/1563 Loss: 2.087 | Acc: 32.700% (10056/30752)\n",
            "Epoch 47 Step 961/1563 Loss: 2.087 | Acc: 32.702% (10067/30784)\n",
            "Epoch 47 Step 962/1563 Loss: 2.087 | Acc: 32.697% (10076/30816)\n",
            "Epoch 47 Step 963/1563 Loss: 2.087 | Acc: 32.699% (10087/30848)\n",
            "Epoch 47 Step 964/1563 Loss: 2.088 | Acc: 32.688% (10094/30880)\n",
            "Epoch 47 Step 965/1563 Loss: 2.088 | Acc: 32.686% (10104/30912)\n",
            "Epoch 47 Step 966/1563 Loss: 2.088 | Acc: 32.685% (10114/30944)\n",
            "Epoch 47 Step 967/1563 Loss: 2.088 | Acc: 32.677% (10122/30976)\n",
            "Epoch 47 Step 968/1563 Loss: 2.088 | Acc: 32.672% (10131/31008)\n",
            "Epoch 47 Step 969/1563 Loss: 2.088 | Acc: 32.674% (10142/31040)\n",
            "Epoch 47 Step 970/1563 Loss: 2.088 | Acc: 32.673% (10152/31072)\n",
            "Epoch 47 Step 971/1563 Loss: 2.088 | Acc: 32.665% (10160/31104)\n",
            "Epoch 47 Step 972/1563 Loss: 2.088 | Acc: 32.660% (10169/31136)\n",
            "Epoch 47 Step 973/1563 Loss: 2.088 | Acc: 32.665% (10181/31168)\n",
            "Epoch 47 Step 974/1563 Loss: 2.088 | Acc: 32.663% (10191/31200)\n",
            "Epoch 47 Step 975/1563 Loss: 2.088 | Acc: 32.652% (10198/31232)\n",
            "Epoch 47 Step 976/1563 Loss: 2.089 | Acc: 32.651% (10208/31264)\n",
            "Epoch 47 Step 977/1563 Loss: 2.089 | Acc: 32.656% (10220/31296)\n",
            "Epoch 47 Step 978/1563 Loss: 2.088 | Acc: 32.661% (10232/31328)\n",
            "Epoch 47 Step 979/1563 Loss: 2.088 | Acc: 32.659% (10242/31360)\n",
            "Epoch 47 Step 980/1563 Loss: 2.088 | Acc: 32.652% (10250/31392)\n",
            "Epoch 47 Step 981/1563 Loss: 2.088 | Acc: 32.644% (10258/31424)\n",
            "Epoch 47 Step 982/1563 Loss: 2.088 | Acc: 32.630% (10264/31456)\n",
            "Epoch 47 Step 983/1563 Loss: 2.088 | Acc: 32.631% (10275/31488)\n",
            "Epoch 47 Step 984/1563 Loss: 2.089 | Acc: 32.624% (10283/31520)\n",
            "Epoch 47 Step 985/1563 Loss: 2.089 | Acc: 32.616% (10291/31552)\n",
            "Epoch 47 Step 986/1563 Loss: 2.089 | Acc: 32.615% (10301/31584)\n",
            "Epoch 47 Step 987/1563 Loss: 2.089 | Acc: 32.610% (10310/31616)\n",
            "Epoch 47 Step 988/1563 Loss: 2.089 | Acc: 32.609% (10320/31648)\n",
            "Epoch 47 Step 989/1563 Loss: 2.089 | Acc: 32.601% (10328/31680)\n",
            "Epoch 47 Step 990/1563 Loss: 2.089 | Acc: 32.603% (10339/31712)\n",
            "Epoch 47 Step 991/1563 Loss: 2.090 | Acc: 32.601% (10349/31744)\n",
            "Epoch 47 Step 992/1563 Loss: 2.090 | Acc: 32.610% (10362/31776)\n",
            "Epoch 47 Step 993/1563 Loss: 2.089 | Acc: 32.608% (10372/31808)\n",
            "Epoch 47 Step 994/1563 Loss: 2.089 | Acc: 32.610% (10383/31840)\n",
            "Epoch 47 Step 995/1563 Loss: 2.089 | Acc: 32.605% (10392/31872)\n",
            "Epoch 47 Step 996/1563 Loss: 2.089 | Acc: 32.604% (10402/31904)\n",
            "Epoch 47 Step 997/1563 Loss: 2.089 | Acc: 32.612% (10415/31936)\n",
            "Epoch 47 Step 998/1563 Loss: 2.089 | Acc: 32.614% (10426/31968)\n",
            "Epoch 47 Step 999/1563 Loss: 2.089 | Acc: 32.622% (10439/32000)\n",
            "Epoch 47 Step 1000/1563 Loss: 2.090 | Acc: 32.602% (10443/32032)\n",
            "Epoch 47 Step 1001/1563 Loss: 2.090 | Acc: 32.607% (10455/32064)\n",
            "Epoch 47 Step 1002/1563 Loss: 2.090 | Acc: 32.599% (10463/32096)\n",
            "Epoch 47 Step 1003/1563 Loss: 2.090 | Acc: 32.613% (10478/32128)\n",
            "Epoch 47 Step 1004/1563 Loss: 2.090 | Acc: 32.609% (10487/32160)\n",
            "Epoch 47 Step 1005/1563 Loss: 2.090 | Acc: 32.601% (10495/32192)\n",
            "Epoch 47 Step 1006/1563 Loss: 2.090 | Acc: 32.591% (10502/32224)\n",
            "Epoch 47 Step 1007/1563 Loss: 2.090 | Acc: 32.595% (10514/32256)\n",
            "Epoch 47 Step 1008/1563 Loss: 2.090 | Acc: 32.585% (10521/32288)\n",
            "Epoch 47 Step 1009/1563 Loss: 2.089 | Acc: 32.596% (10535/32320)\n",
            "Epoch 47 Step 1010/1563 Loss: 2.089 | Acc: 32.595% (10545/32352)\n",
            "Epoch 47 Step 1011/1563 Loss: 2.089 | Acc: 32.606% (10559/32384)\n",
            "Epoch 47 Step 1012/1563 Loss: 2.089 | Acc: 32.604% (10569/32416)\n",
            "Epoch 47 Step 1013/1563 Loss: 2.089 | Acc: 32.606% (10580/32448)\n",
            "Epoch 47 Step 1014/1563 Loss: 2.090 | Acc: 32.595% (10587/32480)\n",
            "Epoch 47 Step 1015/1563 Loss: 2.090 | Acc: 32.603% (10600/32512)\n",
            "Epoch 47 Step 1016/1563 Loss: 2.090 | Acc: 32.602% (10610/32544)\n",
            "Epoch 47 Step 1017/1563 Loss: 2.089 | Acc: 32.598% (10619/32576)\n",
            "Epoch 47 Step 1018/1563 Loss: 2.089 | Acc: 32.584% (10625/32608)\n",
            "Epoch 47 Step 1019/1563 Loss: 2.089 | Acc: 32.586% (10636/32640)\n",
            "Epoch 47 Step 1020/1563 Loss: 2.089 | Acc: 32.584% (10646/32672)\n",
            "Epoch 47 Step 1021/1563 Loss: 2.089 | Acc: 32.574% (10653/32704)\n",
            "Epoch 47 Step 1022/1563 Loss: 2.089 | Acc: 32.585% (10667/32736)\n",
            "Epoch 47 Step 1023/1563 Loss: 2.089 | Acc: 32.599% (10682/32768)\n",
            "Epoch 47 Step 1024/1563 Loss: 2.088 | Acc: 32.607% (10695/32800)\n",
            "Epoch 47 Step 1025/1563 Loss: 2.088 | Acc: 32.608% (10706/32832)\n",
            "Epoch 47 Step 1026/1563 Loss: 2.088 | Acc: 32.607% (10716/32864)\n",
            "Epoch 47 Step 1027/1563 Loss: 2.088 | Acc: 32.612% (10728/32896)\n",
            "Epoch 47 Step 1028/1563 Loss: 2.088 | Acc: 32.608% (10737/32928)\n",
            "Epoch 47 Step 1029/1563 Loss: 2.088 | Acc: 32.600% (10745/32960)\n",
            "Epoch 47 Step 1030/1563 Loss: 2.088 | Acc: 32.599% (10755/32992)\n",
            "Epoch 47 Step 1031/1563 Loss: 2.088 | Acc: 32.598% (10765/33024)\n",
            "Epoch 47 Step 1032/1563 Loss: 2.088 | Acc: 32.605% (10778/33056)\n",
            "Epoch 47 Step 1033/1563 Loss: 2.088 | Acc: 32.598% (10786/33088)\n",
            "Epoch 47 Step 1034/1563 Loss: 2.088 | Acc: 32.585% (10792/33120)\n",
            "Epoch 47 Step 1035/1563 Loss: 2.088 | Acc: 32.580% (10801/33152)\n",
            "Epoch 47 Step 1036/1563 Loss: 2.089 | Acc: 32.564% (10806/33184)\n",
            "Epoch 47 Step 1037/1563 Loss: 2.089 | Acc: 32.569% (10818/33216)\n",
            "Epoch 47 Step 1038/1563 Loss: 2.089 | Acc: 32.567% (10828/33248)\n",
            "Epoch 47 Step 1039/1563 Loss: 2.089 | Acc: 32.563% (10837/33280)\n",
            "Epoch 47 Step 1040/1563 Loss: 2.089 | Acc: 32.556% (10845/33312)\n",
            "Epoch 47 Step 1041/1563 Loss: 2.089 | Acc: 32.570% (10860/33344)\n",
            "Epoch 47 Step 1042/1563 Loss: 2.089 | Acc: 32.571% (10871/33376)\n",
            "Epoch 47 Step 1043/1563 Loss: 2.088 | Acc: 32.588% (10887/33408)\n",
            "Epoch 47 Step 1044/1563 Loss: 2.089 | Acc: 32.587% (10897/33440)\n",
            "Epoch 47 Step 1045/1563 Loss: 2.088 | Acc: 32.585% (10907/33472)\n",
            "Epoch 47 Step 1046/1563 Loss: 2.089 | Acc: 32.581% (10916/33504)\n",
            "Epoch 47 Step 1047/1563 Loss: 2.089 | Acc: 32.583% (10927/33536)\n",
            "Epoch 47 Step 1048/1563 Loss: 2.089 | Acc: 32.585% (10938/33568)\n",
            "Epoch 47 Step 1049/1563 Loss: 2.089 | Acc: 32.595% (10952/33600)\n",
            "Epoch 47 Step 1050/1563 Loss: 2.088 | Acc: 32.606% (10966/33632)\n",
            "Epoch 47 Step 1051/1563 Loss: 2.088 | Acc: 32.608% (10977/33664)\n",
            "Epoch 47 Step 1052/1563 Loss: 2.089 | Acc: 32.603% (10986/33696)\n",
            "Epoch 47 Step 1053/1563 Loss: 2.089 | Acc: 32.602% (10996/33728)\n",
            "Epoch 47 Step 1054/1563 Loss: 2.089 | Acc: 32.601% (11006/33760)\n",
            "Epoch 47 Step 1055/1563 Loss: 2.089 | Acc: 32.596% (11015/33792)\n",
            "Epoch 47 Step 1056/1563 Loss: 2.089 | Acc: 32.595% (11025/33824)\n",
            "Epoch 47 Step 1057/1563 Loss: 2.089 | Acc: 32.591% (11034/33856)\n",
            "Epoch 47 Step 1058/1563 Loss: 2.089 | Acc: 32.602% (11048/33888)\n",
            "Epoch 47 Step 1059/1563 Loss: 2.089 | Acc: 32.606% (11060/33920)\n",
            "Epoch 47 Step 1060/1563 Loss: 2.089 | Acc: 32.608% (11071/33952)\n",
            "Epoch 47 Step 1061/1563 Loss: 2.089 | Acc: 32.612% (11083/33984)\n",
            "Epoch 47 Step 1062/1563 Loss: 2.089 | Acc: 32.629% (11099/34016)\n",
            "Epoch 47 Step 1063/1563 Loss: 2.089 | Acc: 32.630% (11110/34048)\n",
            "Epoch 47 Step 1064/1563 Loss: 2.089 | Acc: 32.617% (11116/34080)\n",
            "Epoch 47 Step 1065/1563 Loss: 2.089 | Acc: 32.625% (11129/34112)\n",
            "Epoch 47 Step 1066/1563 Loss: 2.089 | Acc: 32.638% (11144/34144)\n",
            "Epoch 47 Step 1067/1563 Loss: 2.089 | Acc: 32.631% (11152/34176)\n",
            "Epoch 47 Step 1068/1563 Loss: 2.089 | Acc: 32.627% (11161/34208)\n",
            "Epoch 47 Step 1069/1563 Loss: 2.089 | Acc: 32.631% (11173/34240)\n",
            "Epoch 47 Step 1070/1563 Loss: 2.089 | Acc: 32.633% (11184/34272)\n",
            "Epoch 47 Step 1071/1563 Loss: 2.088 | Acc: 32.652% (11201/34304)\n",
            "Epoch 47 Step 1072/1563 Loss: 2.088 | Acc: 32.651% (11211/34336)\n",
            "Epoch 47 Step 1073/1563 Loss: 2.088 | Acc: 32.641% (11218/34368)\n",
            "Epoch 47 Step 1074/1563 Loss: 2.089 | Acc: 32.637% (11227/34400)\n",
            "Epoch 47 Step 1075/1563 Loss: 2.089 | Acc: 32.630% (11235/34432)\n",
            "Epoch 47 Step 1076/1563 Loss: 2.089 | Acc: 32.620% (11242/34464)\n",
            "Epoch 47 Step 1077/1563 Loss: 2.089 | Acc: 32.618% (11252/34496)\n",
            "Epoch 47 Step 1078/1563 Loss: 2.089 | Acc: 32.603% (11257/34528)\n",
            "Epoch 47 Step 1079/1563 Loss: 2.089 | Acc: 32.610% (11270/34560)\n",
            "Epoch 47 Step 1080/1563 Loss: 2.089 | Acc: 32.597% (11276/34592)\n",
            "Epoch 47 Step 1081/1563 Loss: 2.089 | Acc: 32.602% (11288/34624)\n",
            "Epoch 47 Step 1082/1563 Loss: 2.089 | Acc: 32.606% (11300/34656)\n",
            "Epoch 47 Step 1083/1563 Loss: 2.089 | Acc: 32.614% (11313/34688)\n",
            "Epoch 47 Step 1084/1563 Loss: 2.089 | Acc: 32.598% (11318/34720)\n",
            "Epoch 47 Step 1085/1563 Loss: 2.090 | Acc: 32.588% (11325/34752)\n",
            "Epoch 47 Step 1086/1563 Loss: 2.090 | Acc: 32.578% (11332/34784)\n",
            "Epoch 47 Step 1087/1563 Loss: 2.090 | Acc: 32.568% (11339/34816)\n",
            "Epoch 47 Step 1088/1563 Loss: 2.090 | Acc: 32.564% (11348/34848)\n",
            "Epoch 47 Step 1089/1563 Loss: 2.090 | Acc: 32.577% (11363/34880)\n",
            "Epoch 47 Step 1090/1563 Loss: 2.090 | Acc: 32.579% (11374/34912)\n",
            "Epoch 47 Step 1091/1563 Loss: 2.090 | Acc: 32.575% (11383/34944)\n",
            "Epoch 47 Step 1092/1563 Loss: 2.090 | Acc: 32.574% (11393/34976)\n",
            "Epoch 47 Step 1093/1563 Loss: 2.090 | Acc: 32.573% (11403/35008)\n",
            "Epoch 47 Step 1094/1563 Loss: 2.090 | Acc: 32.563% (11410/35040)\n",
            "Epoch 47 Step 1095/1563 Loss: 2.091 | Acc: 32.559% (11419/35072)\n",
            "Epoch 47 Step 1096/1563 Loss: 2.091 | Acc: 32.560% (11430/35104)\n",
            "Epoch 47 Step 1097/1563 Loss: 2.092 | Acc: 32.545% (11435/35136)\n",
            "Epoch 47 Step 1098/1563 Loss: 2.092 | Acc: 32.541% (11444/35168)\n",
            "Epoch 47 Step 1099/1563 Loss: 2.092 | Acc: 32.543% (11455/35200)\n",
            "Epoch 47 Step 1100/1563 Loss: 2.092 | Acc: 32.544% (11466/35232)\n",
            "Epoch 47 Step 1101/1563 Loss: 2.092 | Acc: 32.543% (11476/35264)\n",
            "Epoch 47 Step 1102/1563 Loss: 2.092 | Acc: 32.533% (11483/35296)\n",
            "Epoch 47 Step 1103/1563 Loss: 2.092 | Acc: 32.529% (11492/35328)\n",
            "Epoch 47 Step 1104/1563 Loss: 2.092 | Acc: 32.534% (11504/35360)\n",
            "Epoch 47 Step 1105/1563 Loss: 2.093 | Acc: 32.533% (11514/35392)\n",
            "Epoch 47 Step 1106/1563 Loss: 2.092 | Acc: 32.529% (11523/35424)\n",
            "Epoch 47 Step 1107/1563 Loss: 2.092 | Acc: 32.539% (11537/35456)\n",
            "Epoch 47 Step 1108/1563 Loss: 2.092 | Acc: 32.526% (11543/35488)\n",
            "Epoch 47 Step 1109/1563 Loss: 2.093 | Acc: 32.525% (11553/35520)\n",
            "Epoch 47 Step 1110/1563 Loss: 2.093 | Acc: 32.513% (11559/35552)\n",
            "Epoch 47 Step 1111/1563 Loss: 2.092 | Acc: 32.517% (11571/35584)\n",
            "Epoch 47 Step 1112/1563 Loss: 2.092 | Acc: 32.519% (11582/35616)\n",
            "Epoch 47 Step 1113/1563 Loss: 2.092 | Acc: 32.515% (11591/35648)\n",
            "Epoch 47 Step 1114/1563 Loss: 2.092 | Acc: 32.511% (11600/35680)\n",
            "Epoch 47 Step 1115/1563 Loss: 2.092 | Acc: 32.510% (11610/35712)\n",
            "Epoch 47 Step 1116/1563 Loss: 2.092 | Acc: 32.515% (11622/35744)\n",
            "Epoch 47 Step 1117/1563 Loss: 2.092 | Acc: 32.511% (11631/35776)\n",
            "Epoch 47 Step 1118/1563 Loss: 2.092 | Acc: 32.501% (11638/35808)\n",
            "Epoch 47 Step 1119/1563 Loss: 2.093 | Acc: 32.494% (11646/35840)\n",
            "Epoch 47 Step 1120/1563 Loss: 2.093 | Acc: 32.488% (11654/35872)\n",
            "Epoch 47 Step 1121/1563 Loss: 2.093 | Acc: 32.484% (11663/35904)\n",
            "Epoch 47 Step 1122/1563 Loss: 2.093 | Acc: 32.480% (11672/35936)\n",
            "Epoch 47 Step 1123/1563 Loss: 2.093 | Acc: 32.476% (11681/35968)\n",
            "Epoch 47 Step 1124/1563 Loss: 2.093 | Acc: 32.469% (11689/36000)\n",
            "Epoch 47 Step 1125/1563 Loss: 2.092 | Acc: 32.477% (11702/36032)\n",
            "Epoch 47 Step 1126/1563 Loss: 2.092 | Acc: 32.478% (11713/36064)\n",
            "Epoch 47 Step 1127/1563 Loss: 2.092 | Acc: 32.477% (11723/36096)\n",
            "Epoch 47 Step 1128/1563 Loss: 2.093 | Acc: 32.454% (11725/36128)\n",
            "Epoch 47 Step 1129/1563 Loss: 2.093 | Acc: 32.450% (11734/36160)\n",
            "Epoch 47 Step 1130/1563 Loss: 2.092 | Acc: 32.452% (11745/36192)\n",
            "Epoch 47 Step 1131/1563 Loss: 2.093 | Acc: 32.451% (11755/36224)\n",
            "Epoch 47 Step 1132/1563 Loss: 2.093 | Acc: 32.453% (11766/36256)\n",
            "Epoch 47 Step 1133/1563 Loss: 2.093 | Acc: 32.460% (11779/36288)\n",
            "Epoch 47 Step 1134/1563 Loss: 2.092 | Acc: 32.464% (11791/36320)\n",
            "Epoch 47 Step 1135/1563 Loss: 2.093 | Acc: 32.463% (11801/36352)\n",
            "Epoch 47 Step 1136/1563 Loss: 2.092 | Acc: 32.481% (11818/36384)\n",
            "Epoch 47 Step 1137/1563 Loss: 2.092 | Acc: 32.494% (11833/36416)\n",
            "Epoch 47 Step 1138/1563 Loss: 2.092 | Acc: 32.493% (11843/36448)\n",
            "Epoch 47 Step 1139/1563 Loss: 2.092 | Acc: 32.495% (11854/36480)\n",
            "Epoch 47 Step 1140/1563 Loss: 2.092 | Acc: 32.496% (11865/36512)\n",
            "Epoch 47 Step 1141/1563 Loss: 2.092 | Acc: 32.498% (11876/36544)\n",
            "Epoch 47 Step 1142/1563 Loss: 2.092 | Acc: 32.494% (11885/36576)\n",
            "Epoch 47 Step 1143/1563 Loss: 2.092 | Acc: 32.490% (11894/36608)\n",
            "Epoch 47 Step 1144/1563 Loss: 2.092 | Acc: 32.492% (11905/36640)\n",
            "Epoch 47 Step 1145/1563 Loss: 2.092 | Acc: 32.499% (11918/36672)\n",
            "Epoch 47 Step 1146/1563 Loss: 2.092 | Acc: 32.498% (11928/36704)\n",
            "Epoch 47 Step 1147/1563 Loss: 2.092 | Acc: 32.510% (11943/36736)\n",
            "Epoch 47 Step 1148/1563 Loss: 2.092 | Acc: 32.509% (11953/36768)\n",
            "Epoch 47 Step 1149/1563 Loss: 2.092 | Acc: 32.514% (11965/36800)\n",
            "Epoch 47 Step 1150/1563 Loss: 2.093 | Acc: 32.507% (11973/36832)\n",
            "Epoch 47 Step 1151/1563 Loss: 2.093 | Acc: 32.509% (11984/36864)\n",
            "Epoch 47 Step 1152/1563 Loss: 2.093 | Acc: 32.508% (11994/36896)\n",
            "Epoch 47 Step 1153/1563 Loss: 2.093 | Acc: 32.509% (12005/36928)\n",
            "Epoch 47 Step 1154/1563 Loss: 2.093 | Acc: 32.500% (12012/36960)\n",
            "Epoch 47 Step 1155/1563 Loss: 2.093 | Acc: 32.488% (12018/36992)\n",
            "Epoch 47 Step 1156/1563 Loss: 2.093 | Acc: 32.498% (12032/37024)\n",
            "Epoch 47 Step 1157/1563 Loss: 2.093 | Acc: 32.491% (12040/37056)\n",
            "Epoch 47 Step 1158/1563 Loss: 2.093 | Acc: 32.488% (12049/37088)\n",
            "Epoch 47 Step 1159/1563 Loss: 2.093 | Acc: 32.492% (12061/37120)\n",
            "Epoch 47 Step 1160/1563 Loss: 2.093 | Acc: 32.491% (12071/37152)\n",
            "Epoch 47 Step 1161/1563 Loss: 2.093 | Acc: 32.484% (12079/37184)\n",
            "Epoch 47 Step 1162/1563 Loss: 2.093 | Acc: 32.486% (12090/37216)\n",
            "Epoch 47 Step 1163/1563 Loss: 2.093 | Acc: 32.488% (12101/37248)\n",
            "Epoch 47 Step 1164/1563 Loss: 2.093 | Acc: 32.492% (12113/37280)\n",
            "Epoch 47 Step 1165/1563 Loss: 2.093 | Acc: 32.496% (12125/37312)\n",
            "Epoch 47 Step 1166/1563 Loss: 2.093 | Acc: 32.498% (12136/37344)\n",
            "Epoch 47 Step 1167/1563 Loss: 2.093 | Acc: 32.499% (12147/37376)\n",
            "Epoch 47 Step 1168/1563 Loss: 2.092 | Acc: 32.504% (12159/37408)\n",
            "Epoch 47 Step 1169/1563 Loss: 2.093 | Acc: 32.503% (12169/37440)\n",
            "Epoch 47 Step 1170/1563 Loss: 2.093 | Acc: 32.499% (12178/37472)\n",
            "Epoch 47 Step 1171/1563 Loss: 2.093 | Acc: 32.487% (12184/37504)\n",
            "Epoch 47 Step 1172/1563 Loss: 2.093 | Acc: 32.510% (12203/37536)\n",
            "Epoch 47 Step 1173/1563 Loss: 2.093 | Acc: 32.509% (12213/37568)\n",
            "Epoch 47 Step 1174/1563 Loss: 2.093 | Acc: 32.503% (12221/37600)\n",
            "Epoch 47 Step 1175/1563 Loss: 2.093 | Acc: 32.510% (12234/37632)\n",
            "Epoch 47 Step 1176/1563 Loss: 2.093 | Acc: 32.506% (12243/37664)\n",
            "Epoch 47 Step 1177/1563 Loss: 2.093 | Acc: 32.505% (12253/37696)\n",
            "Epoch 47 Step 1178/1563 Loss: 2.094 | Acc: 32.493% (12259/37728)\n",
            "Epoch 47 Step 1179/1563 Loss: 2.093 | Acc: 32.497% (12271/37760)\n",
            "Epoch 47 Step 1180/1563 Loss: 2.094 | Acc: 32.496% (12281/37792)\n",
            "Epoch 47 Step 1181/1563 Loss: 2.094 | Acc: 32.485% (12287/37824)\n",
            "Epoch 47 Step 1182/1563 Loss: 2.094 | Acc: 32.484% (12297/37856)\n",
            "Epoch 47 Step 1183/1563 Loss: 2.094 | Acc: 32.477% (12305/37888)\n",
            "Epoch 47 Step 1184/1563 Loss: 2.094 | Acc: 32.482% (12317/37920)\n",
            "Epoch 47 Step 1185/1563 Loss: 2.094 | Acc: 32.486% (12329/37952)\n",
            "Epoch 47 Step 1186/1563 Loss: 2.094 | Acc: 32.493% (12342/37984)\n",
            "Epoch 47 Step 1187/1563 Loss: 2.094 | Acc: 32.510% (12359/38016)\n",
            "Epoch 47 Step 1188/1563 Loss: 2.094 | Acc: 32.512% (12370/38048)\n",
            "Epoch 47 Step 1189/1563 Loss: 2.094 | Acc: 32.524% (12385/38080)\n",
            "Epoch 47 Step 1190/1563 Loss: 2.094 | Acc: 32.520% (12394/38112)\n",
            "Epoch 47 Step 1191/1563 Loss: 2.093 | Acc: 32.527% (12407/38144)\n",
            "Epoch 47 Step 1192/1563 Loss: 2.093 | Acc: 32.531% (12419/38176)\n",
            "Epoch 47 Step 1193/1563 Loss: 2.093 | Acc: 32.532% (12430/38208)\n",
            "Epoch 47 Step 1194/1563 Loss: 2.093 | Acc: 32.544% (12445/38240)\n",
            "Epoch 47 Step 1195/1563 Loss: 2.093 | Acc: 32.546% (12456/38272)\n",
            "Epoch 47 Step 1196/1563 Loss: 2.093 | Acc: 32.537% (12463/38304)\n",
            "Epoch 47 Step 1197/1563 Loss: 2.093 | Acc: 32.531% (12471/38336)\n",
            "Epoch 47 Step 1198/1563 Loss: 2.094 | Acc: 32.509% (12473/38368)\n",
            "Epoch 47 Step 1199/1563 Loss: 2.094 | Acc: 32.508% (12483/38400)\n",
            "Epoch 47 Step 1200/1563 Loss: 2.094 | Acc: 32.509% (12494/38432)\n",
            "Epoch 47 Step 1201/1563 Loss: 2.094 | Acc: 32.511% (12505/38464)\n",
            "Epoch 47 Step 1202/1563 Loss: 2.094 | Acc: 32.515% (12517/38496)\n",
            "Epoch 47 Step 1203/1563 Loss: 2.094 | Acc: 32.522% (12530/38528)\n",
            "Epoch 47 Step 1204/1563 Loss: 2.094 | Acc: 32.526% (12542/38560)\n",
            "Epoch 47 Step 1205/1563 Loss: 2.094 | Acc: 32.522% (12551/38592)\n",
            "Epoch 47 Step 1206/1563 Loss: 2.094 | Acc: 32.529% (12564/38624)\n",
            "Epoch 47 Step 1207/1563 Loss: 2.094 | Acc: 32.528% (12574/38656)\n",
            "Epoch 47 Step 1208/1563 Loss: 2.094 | Acc: 32.535% (12587/38688)\n",
            "Epoch 47 Step 1209/1563 Loss: 2.094 | Acc: 32.541% (12600/38720)\n",
            "Epoch 47 Step 1210/1563 Loss: 2.093 | Acc: 32.553% (12615/38752)\n",
            "Epoch 47 Step 1211/1563 Loss: 2.093 | Acc: 32.557% (12627/38784)\n",
            "Epoch 47 Step 1212/1563 Loss: 2.093 | Acc: 32.561% (12639/38816)\n",
            "Epoch 47 Step 1213/1563 Loss: 2.094 | Acc: 32.547% (12644/38848)\n",
            "Epoch 47 Step 1214/1563 Loss: 2.094 | Acc: 32.544% (12653/38880)\n",
            "Epoch 47 Step 1215/1563 Loss: 2.094 | Acc: 32.543% (12663/38912)\n",
            "Epoch 47 Step 1216/1563 Loss: 2.093 | Acc: 32.549% (12676/38944)\n",
            "Epoch 47 Step 1217/1563 Loss: 2.093 | Acc: 32.566% (12693/38976)\n",
            "Epoch 47 Step 1218/1563 Loss: 2.093 | Acc: 32.565% (12703/39008)\n",
            "Epoch 47 Step 1219/1563 Loss: 2.093 | Acc: 32.574% (12717/39040)\n",
            "Epoch 47 Step 1220/1563 Loss: 2.093 | Acc: 32.573% (12727/39072)\n",
            "Epoch 47 Step 1221/1563 Loss: 2.093 | Acc: 32.582% (12741/39104)\n",
            "Epoch 47 Step 1222/1563 Loss: 2.093 | Acc: 32.581% (12751/39136)\n",
            "Epoch 47 Step 1223/1563 Loss: 2.094 | Acc: 32.580% (12761/39168)\n",
            "Epoch 47 Step 1224/1563 Loss: 2.094 | Acc: 32.584% (12773/39200)\n",
            "Epoch 47 Step 1225/1563 Loss: 2.094 | Acc: 32.586% (12784/39232)\n",
            "Epoch 47 Step 1226/1563 Loss: 2.094 | Acc: 32.579% (12792/39264)\n",
            "Epoch 47 Step 1227/1563 Loss: 2.094 | Acc: 32.578% (12802/39296)\n",
            "Epoch 47 Step 1228/1563 Loss: 2.094 | Acc: 32.580% (12813/39328)\n",
            "Epoch 47 Step 1229/1563 Loss: 2.094 | Acc: 32.594% (12829/39360)\n",
            "Epoch 47 Step 1230/1563 Loss: 2.094 | Acc: 32.585% (12836/39392)\n",
            "Epoch 47 Step 1231/1563 Loss: 2.094 | Acc: 32.577% (12843/39424)\n",
            "Epoch 47 Step 1232/1563 Loss: 2.094 | Acc: 32.578% (12854/39456)\n",
            "Epoch 47 Step 1233/1563 Loss: 2.094 | Acc: 32.574% (12863/39488)\n",
            "Epoch 47 Step 1234/1563 Loss: 2.094 | Acc: 32.571% (12872/39520)\n",
            "Epoch 47 Step 1235/1563 Loss: 2.095 | Acc: 32.550% (12874/39552)\n",
            "Epoch 47 Step 1236/1563 Loss: 2.095 | Acc: 32.556% (12887/39584)\n",
            "Epoch 47 Step 1237/1563 Loss: 2.096 | Acc: 32.553% (12896/39616)\n",
            "Epoch 47 Step 1238/1563 Loss: 2.095 | Acc: 32.559% (12909/39648)\n",
            "Epoch 47 Step 1239/1563 Loss: 2.096 | Acc: 32.555% (12918/39680)\n",
            "Epoch 47 Step 1240/1563 Loss: 2.096 | Acc: 32.549% (12926/39712)\n",
            "Epoch 47 Step 1241/1563 Loss: 2.095 | Acc: 32.563% (12942/39744)\n",
            "Epoch 47 Step 1242/1563 Loss: 2.096 | Acc: 32.550% (12947/39776)\n",
            "Epoch 47 Step 1243/1563 Loss: 2.096 | Acc: 32.554% (12959/39808)\n",
            "Epoch 47 Step 1244/1563 Loss: 2.096 | Acc: 32.548% (12967/39840)\n",
            "Epoch 47 Step 1245/1563 Loss: 2.096 | Acc: 32.572% (12987/39872)\n",
            "Epoch 47 Step 1246/1563 Loss: 2.096 | Acc: 32.566% (12995/39904)\n",
            "Epoch 47 Step 1247/1563 Loss: 2.096 | Acc: 32.570% (13007/39936)\n",
            "Epoch 47 Step 1248/1563 Loss: 2.095 | Acc: 32.569% (13017/39968)\n",
            "Epoch 47 Step 1249/1563 Loss: 2.096 | Acc: 32.565% (13026/40000)\n",
            "Epoch 47 Step 1250/1563 Loss: 2.096 | Acc: 32.569% (13038/40032)\n",
            "Epoch 47 Step 1251/1563 Loss: 2.095 | Acc: 32.565% (13047/40064)\n",
            "Epoch 47 Step 1252/1563 Loss: 2.095 | Acc: 32.567% (13058/40096)\n",
            "Epoch 47 Step 1253/1563 Loss: 2.095 | Acc: 32.571% (13070/40128)\n",
            "Epoch 47 Step 1254/1563 Loss: 2.095 | Acc: 32.575% (13082/40160)\n",
            "Epoch 47 Step 1255/1563 Loss: 2.095 | Acc: 32.576% (13093/40192)\n",
            "Epoch 47 Step 1256/1563 Loss: 2.095 | Acc: 32.585% (13107/40224)\n",
            "Epoch 47 Step 1257/1563 Loss: 2.095 | Acc: 32.581% (13116/40256)\n",
            "Epoch 47 Step 1258/1563 Loss: 2.095 | Acc: 32.583% (13127/40288)\n",
            "Epoch 47 Step 1259/1563 Loss: 2.095 | Acc: 32.579% (13136/40320)\n",
            "Epoch 47 Step 1260/1563 Loss: 2.095 | Acc: 32.583% (13148/40352)\n",
            "Epoch 47 Step 1261/1563 Loss: 2.095 | Acc: 32.587% (13160/40384)\n",
            "Epoch 47 Step 1262/1563 Loss: 2.095 | Acc: 32.589% (13171/40416)\n",
            "Epoch 47 Step 1263/1563 Loss: 2.095 | Acc: 32.585% (13180/40448)\n",
            "Epoch 47 Step 1264/1563 Loss: 2.095 | Acc: 32.596% (13195/40480)\n",
            "Epoch 47 Step 1265/1563 Loss: 2.095 | Acc: 32.598% (13206/40512)\n",
            "Epoch 47 Step 1266/1563 Loss: 2.094 | Acc: 32.611% (13222/40544)\n",
            "Epoch 47 Step 1267/1563 Loss: 2.094 | Acc: 32.618% (13235/40576)\n",
            "Epoch 47 Step 1268/1563 Loss: 2.094 | Acc: 32.619% (13246/40608)\n",
            "Epoch 47 Step 1269/1563 Loss: 2.094 | Acc: 32.625% (13259/40640)\n",
            "Epoch 47 Step 1270/1563 Loss: 2.095 | Acc: 32.617% (13266/40672)\n",
            "Epoch 47 Step 1271/1563 Loss: 2.095 | Acc: 32.616% (13276/40704)\n",
            "Epoch 47 Step 1272/1563 Loss: 2.095 | Acc: 32.617% (13287/40736)\n",
            "Epoch 47 Step 1273/1563 Loss: 2.095 | Acc: 32.626% (13301/40768)\n",
            "Epoch 47 Step 1274/1563 Loss: 2.095 | Acc: 32.620% (13309/40800)\n",
            "Epoch 47 Step 1275/1563 Loss: 2.094 | Acc: 32.617% (13318/40832)\n",
            "Epoch 47 Step 1276/1563 Loss: 2.095 | Acc: 32.613% (13327/40864)\n",
            "Epoch 47 Step 1277/1563 Loss: 2.095 | Acc: 32.610% (13336/40896)\n",
            "Epoch 47 Step 1278/1563 Loss: 2.095 | Acc: 32.611% (13347/40928)\n",
            "Epoch 47 Step 1279/1563 Loss: 2.094 | Acc: 32.612% (13358/40960)\n",
            "Epoch 47 Step 1280/1563 Loss: 2.094 | Acc: 32.619% (13371/40992)\n",
            "Epoch 47 Step 1281/1563 Loss: 2.094 | Acc: 32.613% (13379/41024)\n",
            "Epoch 47 Step 1282/1563 Loss: 2.094 | Acc: 32.609% (13388/41056)\n",
            "Epoch 47 Step 1283/1563 Loss: 2.094 | Acc: 32.613% (13400/41088)\n",
            "Epoch 47 Step 1284/1563 Loss: 2.094 | Acc: 32.609% (13409/41120)\n",
            "Epoch 47 Step 1285/1563 Loss: 2.094 | Acc: 32.616% (13422/41152)\n",
            "Epoch 47 Step 1286/1563 Loss: 2.094 | Acc: 32.605% (13428/41184)\n",
            "Epoch 47 Step 1287/1563 Loss: 2.094 | Acc: 32.597% (13435/41216)\n",
            "Epoch 47 Step 1288/1563 Loss: 2.094 | Acc: 32.605% (13449/41248)\n",
            "Epoch 47 Step 1289/1563 Loss: 2.094 | Acc: 32.614% (13463/41280)\n",
            "Epoch 47 Step 1290/1563 Loss: 2.094 | Acc: 32.610% (13472/41312)\n",
            "Epoch 47 Step 1291/1563 Loss: 2.094 | Acc: 32.621% (13487/41344)\n",
            "Epoch 47 Step 1292/1563 Loss: 2.094 | Acc: 32.623% (13498/41376)\n",
            "Epoch 47 Step 1293/1563 Loss: 2.094 | Acc: 32.627% (13510/41408)\n",
            "Epoch 47 Step 1294/1563 Loss: 2.094 | Acc: 32.623% (13519/41440)\n",
            "Epoch 47 Step 1295/1563 Loss: 2.095 | Acc: 32.610% (13524/41472)\n",
            "Epoch 47 Step 1296/1563 Loss: 2.095 | Acc: 32.616% (13537/41504)\n",
            "Epoch 47 Step 1297/1563 Loss: 2.095 | Acc: 32.613% (13546/41536)\n",
            "Epoch 47 Step 1298/1563 Loss: 2.095 | Acc: 32.612% (13556/41568)\n",
            "Epoch 47 Step 1299/1563 Loss: 2.095 | Acc: 32.618% (13569/41600)\n",
            "Epoch 47 Step 1300/1563 Loss: 2.095 | Acc: 32.624% (13582/41632)\n",
            "Epoch 47 Step 1301/1563 Loss: 2.095 | Acc: 32.620% (13591/41664)\n",
            "Epoch 47 Step 1302/1563 Loss: 2.095 | Acc: 32.624% (13603/41696)\n",
            "Epoch 47 Step 1303/1563 Loss: 2.095 | Acc: 32.626% (13614/41728)\n",
            "Epoch 47 Step 1304/1563 Loss: 2.095 | Acc: 32.620% (13622/41760)\n",
            "Epoch 47 Step 1305/1563 Loss: 2.095 | Acc: 32.614% (13630/41792)\n",
            "Epoch 47 Step 1306/1563 Loss: 2.095 | Acc: 32.613% (13640/41824)\n",
            "Epoch 47 Step 1307/1563 Loss: 2.096 | Acc: 32.602% (13646/41856)\n",
            "Epoch 47 Step 1308/1563 Loss: 2.096 | Acc: 32.608% (13659/41888)\n",
            "Epoch 47 Step 1309/1563 Loss: 2.096 | Acc: 32.610% (13670/41920)\n",
            "Epoch 47 Step 1310/1563 Loss: 2.096 | Acc: 32.611% (13681/41952)\n",
            "Epoch 47 Step 1311/1563 Loss: 2.096 | Acc: 32.617% (13694/41984)\n",
            "Epoch 47 Step 1312/1563 Loss: 2.095 | Acc: 32.621% (13706/42016)\n",
            "Epoch 47 Step 1313/1563 Loss: 2.096 | Acc: 32.617% (13715/42048)\n",
            "Epoch 47 Step 1314/1563 Loss: 2.096 | Acc: 32.609% (13722/42080)\n",
            "Epoch 47 Step 1315/1563 Loss: 2.096 | Acc: 32.615% (13735/42112)\n",
            "Epoch 47 Step 1316/1563 Loss: 2.096 | Acc: 32.621% (13748/42144)\n",
            "Epoch 47 Step 1317/1563 Loss: 2.096 | Acc: 32.616% (13756/42176)\n",
            "Epoch 47 Step 1318/1563 Loss: 2.097 | Acc: 32.612% (13765/42208)\n",
            "Epoch 47 Step 1319/1563 Loss: 2.096 | Acc: 32.621% (13779/42240)\n",
            "Epoch 47 Step 1320/1563 Loss: 2.097 | Acc: 32.615% (13787/42272)\n",
            "Epoch 47 Step 1321/1563 Loss: 2.097 | Acc: 32.623% (13801/42304)\n",
            "Epoch 47 Step 1322/1563 Loss: 2.097 | Acc: 32.618% (13809/42336)\n",
            "Epoch 47 Step 1323/1563 Loss: 2.097 | Acc: 32.614% (13818/42368)\n",
            "Epoch 47 Step 1324/1563 Loss: 2.097 | Acc: 32.618% (13830/42400)\n",
            "Epoch 47 Step 1325/1563 Loss: 2.097 | Acc: 32.626% (13844/42432)\n",
            "Epoch 47 Step 1326/1563 Loss: 2.097 | Acc: 32.630% (13856/42464)\n",
            "Epoch 47 Step 1327/1563 Loss: 2.097 | Acc: 32.631% (13867/42496)\n",
            "Epoch 47 Step 1328/1563 Loss: 2.097 | Acc: 32.621% (13873/42528)\n",
            "Epoch 47 Step 1329/1563 Loss: 2.097 | Acc: 32.622% (13884/42560)\n",
            "Epoch 47 Step 1330/1563 Loss: 2.097 | Acc: 32.623% (13895/42592)\n",
            "Epoch 47 Step 1331/1563 Loss: 2.097 | Acc: 32.634% (13910/42624)\n",
            "Epoch 47 Step 1332/1563 Loss: 2.097 | Acc: 32.624% (13916/42656)\n",
            "Epoch 47 Step 1333/1563 Loss: 2.097 | Acc: 32.632% (13930/42688)\n",
            "Epoch 47 Step 1334/1563 Loss: 2.097 | Acc: 32.629% (13939/42720)\n",
            "Epoch 47 Step 1335/1563 Loss: 2.098 | Acc: 32.628% (13949/42752)\n",
            "Epoch 47 Step 1336/1563 Loss: 2.097 | Acc: 32.629% (13960/42784)\n",
            "Epoch 47 Step 1337/1563 Loss: 2.098 | Acc: 32.623% (13968/42816)\n",
            "Epoch 47 Step 1338/1563 Loss: 2.097 | Acc: 32.627% (13980/42848)\n",
            "Epoch 47 Step 1339/1563 Loss: 2.098 | Acc: 32.621% (13988/42880)\n",
            "Epoch 47 Step 1340/1563 Loss: 2.098 | Acc: 32.623% (13999/42912)\n",
            "Epoch 47 Step 1341/1563 Loss: 2.098 | Acc: 32.617% (14007/42944)\n",
            "Epoch 47 Step 1342/1563 Loss: 2.098 | Acc: 32.625% (14021/42976)\n",
            "Epoch 47 Step 1343/1563 Loss: 2.098 | Acc: 32.613% (14026/43008)\n",
            "Epoch 47 Step 1344/1563 Loss: 2.098 | Acc: 32.618% (14039/43040)\n",
            "Epoch 47 Step 1345/1563 Loss: 2.098 | Acc: 32.622% (14051/43072)\n",
            "Epoch 47 Step 1346/1563 Loss: 2.098 | Acc: 32.619% (14060/43104)\n",
            "Epoch 47 Step 1347/1563 Loss: 2.098 | Acc: 32.618% (14070/43136)\n",
            "Epoch 47 Step 1348/1563 Loss: 2.098 | Acc: 32.624% (14083/43168)\n",
            "Epoch 47 Step 1349/1563 Loss: 2.098 | Acc: 32.618% (14091/43200)\n",
            "Epoch 47 Step 1350/1563 Loss: 2.098 | Acc: 32.619% (14102/43232)\n",
            "Epoch 47 Step 1351/1563 Loss: 2.098 | Acc: 32.630% (14117/43264)\n",
            "Epoch 47 Step 1352/1563 Loss: 2.098 | Acc: 32.636% (14130/43296)\n",
            "Epoch 47 Step 1353/1563 Loss: 2.097 | Acc: 32.649% (14146/43328)\n",
            "Epoch 47 Step 1354/1563 Loss: 2.097 | Acc: 32.655% (14159/43360)\n",
            "Epoch 47 Step 1355/1563 Loss: 2.097 | Acc: 32.656% (14170/43392)\n",
            "Epoch 47 Step 1356/1563 Loss: 2.097 | Acc: 32.662% (14183/43424)\n",
            "Epoch 47 Step 1357/1563 Loss: 2.097 | Acc: 32.677% (14200/43456)\n",
            "Epoch 47 Step 1358/1563 Loss: 2.097 | Acc: 32.683% (14213/43488)\n",
            "Epoch 47 Step 1359/1563 Loss: 2.097 | Acc: 32.695% (14229/43520)\n",
            "Epoch 47 Step 1360/1563 Loss: 2.097 | Acc: 32.680% (14233/43552)\n",
            "Epoch 47 Step 1361/1563 Loss: 2.097 | Acc: 32.684% (14245/43584)\n",
            "Epoch 47 Step 1362/1563 Loss: 2.097 | Acc: 32.683% (14255/43616)\n",
            "Epoch 47 Step 1363/1563 Loss: 2.097 | Acc: 32.680% (14264/43648)\n",
            "Epoch 47 Step 1364/1563 Loss: 2.097 | Acc: 32.672% (14271/43680)\n",
            "Epoch 47 Step 1365/1563 Loss: 2.097 | Acc: 32.675% (14283/43712)\n",
            "Epoch 47 Step 1366/1563 Loss: 2.097 | Acc: 32.679% (14295/43744)\n",
            "Epoch 47 Step 1367/1563 Loss: 2.096 | Acc: 32.682% (14307/43776)\n",
            "Epoch 47 Step 1368/1563 Loss: 2.097 | Acc: 32.677% (14315/43808)\n",
            "Epoch 47 Step 1369/1563 Loss: 2.097 | Acc: 32.671% (14323/43840)\n",
            "Epoch 47 Step 1370/1563 Loss: 2.096 | Acc: 32.679% (14337/43872)\n",
            "Epoch 47 Step 1371/1563 Loss: 2.096 | Acc: 32.676% (14346/43904)\n",
            "Epoch 47 Step 1372/1563 Loss: 2.096 | Acc: 32.684% (14360/43936)\n",
            "Epoch 47 Step 1373/1563 Loss: 2.096 | Acc: 32.678% (14368/43968)\n",
            "Epoch 47 Step 1374/1563 Loss: 2.096 | Acc: 32.680% (14379/44000)\n",
            "Epoch 47 Step 1375/1563 Loss: 2.096 | Acc: 32.683% (14391/44032)\n",
            "Epoch 47 Step 1376/1563 Loss: 2.096 | Acc: 32.687% (14403/44064)\n",
            "Epoch 47 Step 1377/1563 Loss: 2.096 | Acc: 32.686% (14413/44096)\n",
            "Epoch 47 Step 1378/1563 Loss: 2.096 | Acc: 32.675% (14419/44128)\n",
            "Epoch 47 Step 1379/1563 Loss: 2.096 | Acc: 32.677% (14430/44160)\n",
            "Epoch 47 Step 1380/1563 Loss: 2.096 | Acc: 32.664% (14435/44192)\n",
            "Epoch 47 Step 1381/1563 Loss: 2.096 | Acc: 32.668% (14447/44224)\n",
            "Epoch 47 Step 1382/1563 Loss: 2.097 | Acc: 32.662% (14455/44256)\n",
            "Epoch 47 Step 1383/1563 Loss: 2.097 | Acc: 32.657% (14463/44288)\n",
            "Epoch 47 Step 1384/1563 Loss: 2.097 | Acc: 32.649% (14470/44320)\n",
            "Epoch 47 Step 1385/1563 Loss: 2.097 | Acc: 32.643% (14478/44352)\n",
            "Epoch 47 Step 1386/1563 Loss: 2.097 | Acc: 32.645% (14489/44384)\n",
            "Epoch 47 Step 1387/1563 Loss: 2.097 | Acc: 32.650% (14502/44416)\n",
            "Epoch 47 Step 1388/1563 Loss: 2.097 | Acc: 32.652% (14513/44448)\n",
            "Epoch 47 Step 1389/1563 Loss: 2.097 | Acc: 32.653% (14524/44480)\n",
            "Epoch 47 Step 1390/1563 Loss: 2.097 | Acc: 32.656% (14536/44512)\n",
            "Epoch 47 Step 1391/1563 Loss: 2.097 | Acc: 32.660% (14548/44544)\n",
            "Epoch 47 Step 1392/1563 Loss: 2.097 | Acc: 32.652% (14555/44576)\n",
            "Epoch 47 Step 1393/1563 Loss: 2.097 | Acc: 32.649% (14564/44608)\n",
            "Epoch 47 Step 1394/1563 Loss: 2.097 | Acc: 32.657% (14578/44640)\n",
            "Epoch 47 Step 1395/1563 Loss: 2.097 | Acc: 32.669% (14594/44672)\n",
            "Epoch 47 Step 1396/1563 Loss: 2.097 | Acc: 32.673% (14606/44704)\n",
            "Epoch 47 Step 1397/1563 Loss: 2.097 | Acc: 32.667% (14614/44736)\n",
            "Epoch 47 Step 1398/1563 Loss: 2.097 | Acc: 32.673% (14627/44768)\n",
            "Epoch 47 Step 1399/1563 Loss: 2.097 | Acc: 32.672% (14637/44800)\n",
            "Epoch 47 Step 1400/1563 Loss: 2.097 | Acc: 32.673% (14648/44832)\n",
            "Epoch 47 Step 1401/1563 Loss: 2.097 | Acc: 32.672% (14658/44864)\n",
            "Epoch 47 Step 1402/1563 Loss: 2.097 | Acc: 32.678% (14671/44896)\n",
            "Epoch 47 Step 1403/1563 Loss: 2.097 | Acc: 32.672% (14679/44928)\n",
            "Epoch 47 Step 1404/1563 Loss: 2.097 | Acc: 32.673% (14690/44960)\n",
            "Epoch 47 Step 1405/1563 Loss: 2.098 | Acc: 32.666% (14697/44992)\n",
            "Epoch 47 Step 1406/1563 Loss: 2.098 | Acc: 32.660% (14705/45024)\n",
            "Epoch 47 Step 1407/1563 Loss: 2.098 | Acc: 32.662% (14716/45056)\n",
            "Epoch 47 Step 1408/1563 Loss: 2.098 | Acc: 32.656% (14724/45088)\n",
            "Epoch 47 Step 1409/1563 Loss: 2.098 | Acc: 32.657% (14735/45120)\n",
            "Epoch 47 Step 1410/1563 Loss: 2.098 | Acc: 32.659% (14746/45152)\n",
            "Epoch 47 Step 1411/1563 Loss: 2.098 | Acc: 32.666% (14760/45184)\n",
            "Epoch 47 Step 1412/1563 Loss: 2.097 | Acc: 32.674% (14774/45216)\n",
            "Epoch 47 Step 1413/1563 Loss: 2.097 | Acc: 32.680% (14787/45248)\n",
            "Epoch 47 Step 1414/1563 Loss: 2.098 | Acc: 32.681% (14798/45280)\n",
            "Epoch 47 Step 1415/1563 Loss: 2.097 | Acc: 32.687% (14811/45312)\n",
            "Epoch 47 Step 1416/1563 Loss: 2.097 | Acc: 32.695% (14825/45344)\n",
            "Epoch 47 Step 1417/1563 Loss: 2.097 | Acc: 32.698% (14837/45376)\n",
            "Epoch 47 Step 1418/1563 Loss: 2.097 | Acc: 32.697% (14847/45408)\n",
            "Epoch 47 Step 1419/1563 Loss: 2.098 | Acc: 32.685% (14852/45440)\n",
            "Epoch 47 Step 1420/1563 Loss: 2.098 | Acc: 32.684% (14862/45472)\n",
            "Epoch 47 Step 1421/1563 Loss: 2.098 | Acc: 32.687% (14874/45504)\n",
            "Epoch 47 Step 1422/1563 Loss: 2.098 | Acc: 32.693% (14887/45536)\n",
            "Epoch 47 Step 1423/1563 Loss: 2.098 | Acc: 32.698% (14900/45568)\n",
            "Epoch 47 Step 1424/1563 Loss: 2.098 | Acc: 32.700% (14911/45600)\n",
            "Epoch 47 Step 1425/1563 Loss: 2.098 | Acc: 32.703% (14923/45632)\n",
            "Epoch 47 Step 1426/1563 Loss: 2.098 | Acc: 32.698% (14931/45664)\n",
            "Epoch 47 Step 1427/1563 Loss: 2.098 | Acc: 32.690% (14938/45696)\n",
            "Epoch 47 Step 1428/1563 Loss: 2.099 | Acc: 32.671% (14940/45728)\n",
            "Epoch 47 Step 1429/1563 Loss: 2.099 | Acc: 32.666% (14948/45760)\n",
            "Epoch 47 Step 1430/1563 Loss: 2.099 | Acc: 32.665% (14958/45792)\n",
            "Epoch 47 Step 1431/1563 Loss: 2.099 | Acc: 32.662% (14967/45824)\n",
            "Epoch 47 Step 1432/1563 Loss: 2.099 | Acc: 32.659% (14976/45856)\n",
            "Epoch 47 Step 1433/1563 Loss: 2.099 | Acc: 32.664% (14989/45888)\n",
            "Epoch 47 Step 1434/1563 Loss: 2.099 | Acc: 32.661% (14998/45920)\n",
            "Epoch 47 Step 1435/1563 Loss: 2.099 | Acc: 32.651% (15004/45952)\n",
            "Epoch 47 Step 1436/1563 Loss: 2.099 | Acc: 32.644% (15011/45984)\n",
            "Epoch 47 Step 1437/1563 Loss: 2.099 | Acc: 32.647% (15023/46016)\n",
            "Epoch 47 Step 1438/1563 Loss: 2.099 | Acc: 32.655% (15037/46048)\n",
            "Epoch 47 Step 1439/1563 Loss: 2.099 | Acc: 32.648% (15044/46080)\n",
            "Epoch 47 Step 1440/1563 Loss: 2.099 | Acc: 32.640% (15051/46112)\n",
            "Epoch 47 Step 1441/1563 Loss: 2.099 | Acc: 32.639% (15061/46144)\n",
            "Epoch 47 Step 1442/1563 Loss: 2.099 | Acc: 32.645% (15074/46176)\n",
            "Epoch 47 Step 1443/1563 Loss: 2.099 | Acc: 32.639% (15082/46208)\n",
            "Epoch 47 Step 1444/1563 Loss: 2.099 | Acc: 32.641% (15093/46240)\n",
            "Epoch 47 Step 1445/1563 Loss: 2.099 | Acc: 32.646% (15106/46272)\n",
            "Epoch 47 Step 1446/1563 Loss: 2.098 | Acc: 32.647% (15117/46304)\n",
            "Epoch 47 Step 1447/1563 Loss: 2.098 | Acc: 32.646% (15127/46336)\n",
            "Epoch 47 Step 1448/1563 Loss: 2.098 | Acc: 32.643% (15136/46368)\n",
            "Epoch 47 Step 1449/1563 Loss: 2.098 | Acc: 32.655% (15152/46400)\n",
            "Epoch 47 Step 1450/1563 Loss: 2.098 | Acc: 32.650% (15160/46432)\n",
            "Epoch 47 Step 1451/1563 Loss: 2.098 | Acc: 32.645% (15168/46464)\n",
            "Epoch 47 Step 1452/1563 Loss: 2.098 | Acc: 32.654% (15183/46496)\n",
            "Epoch 47 Step 1453/1563 Loss: 2.098 | Acc: 32.645% (15189/46528)\n",
            "Epoch 47 Step 1454/1563 Loss: 2.098 | Acc: 32.644% (15199/46560)\n",
            "Epoch 47 Step 1455/1563 Loss: 2.098 | Acc: 32.645% (15210/46592)\n",
            "Epoch 47 Step 1456/1563 Loss: 2.098 | Acc: 32.646% (15221/46624)\n",
            "Epoch 47 Step 1457/1563 Loss: 2.097 | Acc: 32.654% (15235/46656)\n",
            "Epoch 47 Step 1458/1563 Loss: 2.097 | Acc: 32.657% (15247/46688)\n",
            "Epoch 47 Step 1459/1563 Loss: 2.098 | Acc: 32.643% (15251/46720)\n",
            "Epoch 47 Step 1460/1563 Loss: 2.098 | Acc: 32.642% (15261/46752)\n",
            "Epoch 47 Step 1461/1563 Loss: 2.097 | Acc: 32.642% (15271/46784)\n",
            "Epoch 47 Step 1462/1563 Loss: 2.098 | Acc: 32.643% (15282/46816)\n",
            "Epoch 47 Step 1463/1563 Loss: 2.098 | Acc: 32.637% (15290/46848)\n",
            "Epoch 47 Step 1464/1563 Loss: 2.098 | Acc: 32.641% (15302/46880)\n",
            "Epoch 47 Step 1465/1563 Loss: 2.098 | Acc: 32.642% (15313/46912)\n",
            "Epoch 47 Step 1466/1563 Loss: 2.098 | Acc: 32.647% (15326/46944)\n",
            "Epoch 47 Step 1467/1563 Loss: 2.098 | Acc: 32.638% (15332/46976)\n",
            "Epoch 47 Step 1468/1563 Loss: 2.098 | Acc: 32.628% (15338/47008)\n",
            "Epoch 47 Step 1469/1563 Loss: 2.098 | Acc: 32.623% (15346/47040)\n",
            "Epoch 47 Step 1470/1563 Loss: 2.098 | Acc: 32.618% (15354/47072)\n",
            "Epoch 47 Step 1471/1563 Loss: 2.099 | Acc: 32.613% (15362/47104)\n",
            "Epoch 47 Step 1472/1563 Loss: 2.099 | Acc: 32.616% (15374/47136)\n",
            "Epoch 47 Step 1473/1563 Loss: 2.099 | Acc: 32.615% (15384/47168)\n",
            "Epoch 47 Step 1474/1563 Loss: 2.099 | Acc: 32.619% (15396/47200)\n",
            "Epoch 47 Step 1475/1563 Loss: 2.099 | Acc: 32.618% (15406/47232)\n",
            "Epoch 47 Step 1476/1563 Loss: 2.099 | Acc: 32.617% (15416/47264)\n",
            "Epoch 47 Step 1477/1563 Loss: 2.099 | Acc: 32.616% (15426/47296)\n",
            "Epoch 47 Step 1478/1563 Loss: 2.099 | Acc: 32.613% (15435/47328)\n",
            "Epoch 47 Step 1479/1563 Loss: 2.099 | Acc: 32.625% (15451/47360)\n",
            "Epoch 47 Step 1480/1563 Loss: 2.099 | Acc: 32.632% (15465/47392)\n",
            "Epoch 47 Step 1481/1563 Loss: 2.099 | Acc: 32.635% (15477/47424)\n",
            "Epoch 47 Step 1482/1563 Loss: 2.098 | Acc: 32.645% (15492/47456)\n",
            "Epoch 47 Step 1483/1563 Loss: 2.098 | Acc: 32.638% (15499/47488)\n",
            "Epoch 47 Step 1484/1563 Loss: 2.098 | Acc: 32.635% (15508/47520)\n",
            "Epoch 47 Step 1485/1563 Loss: 2.098 | Acc: 32.640% (15521/47552)\n",
            "Epoch 47 Step 1486/1563 Loss: 2.098 | Acc: 32.633% (15528/47584)\n",
            "Epoch 47 Step 1487/1563 Loss: 2.099 | Acc: 32.634% (15539/47616)\n",
            "Epoch 47 Step 1488/1563 Loss: 2.098 | Acc: 32.633% (15549/47648)\n",
            "Epoch 47 Step 1489/1563 Loss: 2.098 | Acc: 32.632% (15559/47680)\n",
            "Epoch 47 Step 1490/1563 Loss: 2.098 | Acc: 32.631% (15569/47712)\n",
            "Epoch 47 Step 1491/1563 Loss: 2.099 | Acc: 32.632% (15580/47744)\n",
            "Epoch 47 Step 1492/1563 Loss: 2.099 | Acc: 32.623% (15586/47776)\n",
            "Epoch 47 Step 1493/1563 Loss: 2.099 | Acc: 32.618% (15594/47808)\n",
            "Epoch 47 Step 1494/1563 Loss: 2.099 | Acc: 32.617% (15604/47840)\n",
            "Epoch 47 Step 1495/1563 Loss: 2.098 | Acc: 32.624% (15618/47872)\n",
            "Epoch 47 Step 1496/1563 Loss: 2.098 | Acc: 32.634% (15633/47904)\n",
            "Epoch 47 Step 1497/1563 Loss: 2.098 | Acc: 32.629% (15641/47936)\n",
            "Epoch 47 Step 1498/1563 Loss: 2.098 | Acc: 32.632% (15653/47968)\n",
            "Epoch 47 Step 1499/1563 Loss: 2.098 | Acc: 32.635% (15665/48000)\n",
            "Epoch 47 Step 1500/1563 Loss: 2.098 | Acc: 32.626% (15671/48032)\n",
            "Epoch 47 Step 1501/1563 Loss: 2.098 | Acc: 32.621% (15679/48064)\n",
            "Epoch 47 Step 1502/1563 Loss: 2.098 | Acc: 32.628% (15693/48096)\n",
            "Epoch 47 Step 1503/1563 Loss: 2.098 | Acc: 32.621% (15700/48128)\n",
            "Epoch 47 Step 1504/1563 Loss: 2.098 | Acc: 32.625% (15712/48160)\n",
            "Epoch 47 Step 1505/1563 Loss: 2.098 | Acc: 32.624% (15722/48192)\n",
            "Epoch 47 Step 1506/1563 Loss: 2.098 | Acc: 32.631% (15736/48224)\n",
            "Epoch 47 Step 1507/1563 Loss: 2.098 | Acc: 32.632% (15747/48256)\n",
            "Epoch 47 Step 1508/1563 Loss: 2.098 | Acc: 32.625% (15754/48288)\n",
            "Epoch 47 Step 1509/1563 Loss: 2.098 | Acc: 32.618% (15761/48320)\n",
            "Epoch 47 Step 1510/1563 Loss: 2.098 | Acc: 32.615% (15770/48352)\n",
            "Epoch 47 Step 1511/1563 Loss: 2.098 | Acc: 32.622% (15784/48384)\n",
            "Epoch 47 Step 1512/1563 Loss: 2.098 | Acc: 32.619% (15793/48416)\n",
            "Epoch 47 Step 1513/1563 Loss: 2.098 | Acc: 32.614% (15801/48448)\n",
            "Epoch 47 Step 1514/1563 Loss: 2.097 | Acc: 32.611% (15810/48480)\n",
            "Epoch 47 Step 1515/1563 Loss: 2.097 | Acc: 32.617% (15823/48512)\n",
            "Epoch 47 Step 1516/1563 Loss: 2.098 | Acc: 32.612% (15831/48544)\n",
            "Epoch 47 Step 1517/1563 Loss: 2.098 | Acc: 32.607% (15839/48576)\n",
            "Epoch 47 Step 1518/1563 Loss: 2.098 | Acc: 32.600% (15846/48608)\n",
            "Epoch 47 Step 1519/1563 Loss: 2.098 | Acc: 32.601% (15857/48640)\n",
            "Epoch 47 Step 1520/1563 Loss: 2.098 | Acc: 32.606% (15870/48672)\n",
            "Epoch 47 Step 1521/1563 Loss: 2.098 | Acc: 32.615% (15885/48704)\n",
            "Epoch 47 Step 1522/1563 Loss: 2.098 | Acc: 32.614% (15895/48736)\n",
            "Epoch 47 Step 1523/1563 Loss: 2.098 | Acc: 32.609% (15903/48768)\n",
            "Epoch 47 Step 1524/1563 Loss: 2.098 | Acc: 32.617% (15917/48800)\n",
            "Epoch 47 Step 1525/1563 Loss: 2.098 | Acc: 32.620% (15929/48832)\n",
            "Epoch 47 Step 1526/1563 Loss: 2.098 | Acc: 32.619% (15939/48864)\n",
            "Epoch 47 Step 1527/1563 Loss: 2.098 | Acc: 32.616% (15948/48896)\n",
            "Epoch 47 Step 1528/1563 Loss: 2.098 | Acc: 32.609% (15955/48928)\n",
            "Epoch 47 Step 1529/1563 Loss: 2.097 | Acc: 32.621% (15971/48960)\n",
            "Epoch 47 Step 1530/1563 Loss: 2.097 | Acc: 32.624% (15983/48992)\n",
            "Epoch 47 Step 1531/1563 Loss: 2.097 | Acc: 32.631% (15997/49024)\n",
            "Epoch 47 Step 1532/1563 Loss: 2.097 | Acc: 32.638% (16011/49056)\n",
            "Epoch 47 Step 1533/1563 Loss: 2.097 | Acc: 32.643% (16024/49088)\n",
            "Epoch 47 Step 1534/1563 Loss: 2.097 | Acc: 32.651% (16038/49120)\n",
            "Epoch 47 Step 1535/1563 Loss: 2.097 | Acc: 32.644% (16045/49152)\n",
            "Epoch 47 Step 1536/1563 Loss: 2.096 | Acc: 32.649% (16058/49184)\n",
            "Epoch 47 Step 1537/1563 Loss: 2.096 | Acc: 32.646% (16067/49216)\n",
            "Epoch 47 Step 1538/1563 Loss: 2.097 | Acc: 32.641% (16075/49248)\n",
            "Epoch 47 Step 1539/1563 Loss: 2.096 | Acc: 32.646% (16088/49280)\n",
            "Epoch 47 Step 1540/1563 Loss: 2.097 | Acc: 32.641% (16096/49312)\n",
            "Epoch 47 Step 1541/1563 Loss: 2.096 | Acc: 32.642% (16107/49344)\n",
            "Epoch 47 Step 1542/1563 Loss: 2.096 | Acc: 32.645% (16119/49376)\n",
            "Epoch 47 Step 1543/1563 Loss: 2.096 | Acc: 32.647% (16130/49408)\n",
            "Epoch 47 Step 1544/1563 Loss: 2.096 | Acc: 32.652% (16143/49440)\n",
            "Epoch 47 Step 1545/1563 Loss: 2.096 | Acc: 32.657% (16156/49472)\n",
            "Epoch 47 Step 1546/1563 Loss: 2.096 | Acc: 32.652% (16164/49504)\n",
            "Epoch 47 Step 1547/1563 Loss: 2.097 | Acc: 32.649% (16173/49536)\n",
            "Epoch 47 Step 1548/1563 Loss: 2.097 | Acc: 32.652% (16185/49568)\n",
            "Epoch 47 Step 1549/1563 Loss: 2.097 | Acc: 32.655% (16197/49600)\n",
            "Epoch 47 Step 1550/1563 Loss: 2.097 | Acc: 32.658% (16209/49632)\n",
            "Epoch 47 Step 1551/1563 Loss: 2.097 | Acc: 32.659% (16220/49664)\n",
            "Epoch 47 Step 1552/1563 Loss: 2.096 | Acc: 32.657% (16229/49696)\n",
            "Epoch 47 Step 1553/1563 Loss: 2.096 | Acc: 32.656% (16239/49728)\n",
            "Epoch 47 Step 1554/1563 Loss: 2.097 | Acc: 32.653% (16248/49760)\n",
            "Epoch 47 Step 1555/1563 Loss: 2.097 | Acc: 32.656% (16260/49792)\n",
            "Epoch 47 Step 1556/1563 Loss: 2.097 | Acc: 32.653% (16269/49824)\n",
            "Epoch 47 Step 1557/1563 Loss: 2.096 | Acc: 32.656% (16281/49856)\n",
            "Epoch 47 Step 1558/1563 Loss: 2.096 | Acc: 32.657% (16292/49888)\n",
            "Epoch 47 Step 1559/1563 Loss: 2.096 | Acc: 32.660% (16304/49920)\n",
            "Epoch 47 Step 1560/1563 Loss: 2.096 | Acc: 32.661% (16315/49952)\n",
            "Epoch 47 Step 1561/1563 Loss: 2.096 | Acc: 32.658% (16324/49984)\n",
            "Epoch 47 Step 1562/1563 Loss: 2.096 | Acc: 32.658% (16329/50000)\n",
            "Epoch 47 Step 0/313 Test Loss: 1.562 | Test Acc: 46.875% (15/32)\n",
            "Epoch 47 Step 1/313 Test Loss: 1.968 | Test Acc: 35.938% (23/64)\n",
            "Epoch 47 Step 2/313 Test Loss: 2.009 | Test Acc: 34.375% (33/96)\n",
            "Epoch 47 Step 3/313 Test Loss: 2.038 | Test Acc: 32.812% (42/128)\n",
            "Epoch 47 Step 4/313 Test Loss: 2.049 | Test Acc: 33.125% (53/160)\n",
            "Epoch 47 Step 5/313 Test Loss: 2.023 | Test Acc: 33.854% (65/192)\n",
            "Epoch 47 Step 6/313 Test Loss: 2.071 | Test Acc: 32.143% (72/224)\n",
            "Epoch 47 Step 7/313 Test Loss: 2.069 | Test Acc: 31.250% (80/256)\n",
            "Epoch 47 Step 8/313 Test Loss: 2.066 | Test Acc: 31.944% (92/288)\n",
            "Epoch 47 Step 9/313 Test Loss: 2.026 | Test Acc: 33.125% (106/320)\n",
            "Epoch 47 Step 10/313 Test Loss: 2.025 | Test Acc: 32.670% (115/352)\n",
            "Epoch 47 Step 11/313 Test Loss: 2.074 | Test Acc: 32.031% (123/384)\n",
            "Epoch 47 Step 12/313 Test Loss: 2.057 | Test Acc: 32.212% (134/416)\n",
            "Epoch 47 Step 13/313 Test Loss: 2.075 | Test Acc: 32.366% (145/448)\n",
            "Epoch 47 Step 14/313 Test Loss: 2.087 | Test Acc: 32.292% (155/480)\n",
            "Epoch 47 Step 15/313 Test Loss: 2.092 | Test Acc: 32.422% (166/512)\n",
            "Epoch 47 Step 16/313 Test Loss: 2.071 | Test Acc: 32.353% (176/544)\n",
            "Epoch 47 Step 17/313 Test Loss: 2.057 | Test Acc: 32.639% (188/576)\n",
            "Epoch 47 Step 18/313 Test Loss: 2.045 | Test Acc: 32.566% (198/608)\n",
            "Epoch 47 Step 19/313 Test Loss: 2.023 | Test Acc: 33.125% (212/640)\n",
            "Epoch 47 Step 20/313 Test Loss: 2.008 | Test Acc: 33.482% (225/672)\n",
            "Epoch 47 Step 21/313 Test Loss: 2.031 | Test Acc: 32.670% (230/704)\n",
            "Epoch 47 Step 22/313 Test Loss: 2.019 | Test Acc: 33.152% (244/736)\n",
            "Epoch 47 Step 23/313 Test Loss: 2.040 | Test Acc: 32.943% (253/768)\n",
            "Epoch 47 Step 24/313 Test Loss: 2.040 | Test Acc: 32.750% (262/800)\n",
            "Epoch 47 Step 25/313 Test Loss: 2.035 | Test Acc: 32.812% (273/832)\n",
            "Epoch 47 Step 26/313 Test Loss: 2.027 | Test Acc: 32.986% (285/864)\n",
            "Epoch 47 Step 27/313 Test Loss: 2.021 | Test Acc: 33.371% (299/896)\n",
            "Epoch 47 Step 28/313 Test Loss: 2.019 | Test Acc: 33.621% (312/928)\n",
            "Epoch 47 Step 29/313 Test Loss: 2.012 | Test Acc: 33.854% (325/960)\n",
            "Epoch 47 Step 30/313 Test Loss: 2.016 | Test Acc: 33.569% (333/992)\n",
            "Epoch 47 Step 31/313 Test Loss: 2.010 | Test Acc: 33.887% (347/1024)\n",
            "Epoch 47 Step 32/313 Test Loss: 2.014 | Test Acc: 33.902% (358/1056)\n",
            "Epoch 47 Step 33/313 Test Loss: 2.007 | Test Acc: 34.375% (374/1088)\n",
            "Epoch 47 Step 34/313 Test Loss: 1.999 | Test Acc: 34.464% (386/1120)\n",
            "Epoch 47 Step 35/313 Test Loss: 2.008 | Test Acc: 34.288% (395/1152)\n",
            "Epoch 47 Step 36/313 Test Loss: 2.002 | Test Acc: 34.375% (407/1184)\n",
            "Epoch 47 Step 37/313 Test Loss: 2.017 | Test Acc: 34.457% (419/1216)\n",
            "Epoch 47 Step 38/313 Test Loss: 2.026 | Test Acc: 34.295% (428/1248)\n",
            "Epoch 47 Step 39/313 Test Loss: 2.024 | Test Acc: 34.688% (444/1280)\n",
            "Epoch 47 Step 40/313 Test Loss: 2.024 | Test Acc: 34.756% (456/1312)\n",
            "Epoch 47 Step 41/313 Test Loss: 2.025 | Test Acc: 34.598% (465/1344)\n",
            "Epoch 47 Step 42/313 Test Loss: 2.020 | Test Acc: 34.593% (476/1376)\n",
            "Epoch 47 Step 43/313 Test Loss: 2.027 | Test Acc: 34.659% (488/1408)\n",
            "Epoch 47 Step 44/313 Test Loss: 2.018 | Test Acc: 34.931% (503/1440)\n",
            "Epoch 47 Step 45/313 Test Loss: 2.008 | Test Acc: 35.054% (516/1472)\n",
            "Epoch 47 Step 46/313 Test Loss: 2.010 | Test Acc: 34.907% (525/1504)\n",
            "Epoch 47 Step 47/313 Test Loss: 2.006 | Test Acc: 34.831% (535/1536)\n",
            "Epoch 47 Step 48/313 Test Loss: 2.002 | Test Acc: 34.694% (544/1568)\n",
            "Epoch 47 Step 49/313 Test Loss: 2.013 | Test Acc: 34.500% (552/1600)\n",
            "Epoch 47 Step 50/313 Test Loss: 2.017 | Test Acc: 34.314% (560/1632)\n",
            "Epoch 47 Step 51/313 Test Loss: 2.012 | Test Acc: 34.495% (574/1664)\n",
            "Epoch 47 Step 52/313 Test Loss: 2.012 | Test Acc: 34.611% (587/1696)\n",
            "Epoch 47 Step 53/313 Test Loss: 2.020 | Test Acc: 34.549% (597/1728)\n",
            "Epoch 47 Step 54/313 Test Loss: 2.029 | Test Acc: 34.489% (607/1760)\n",
            "Epoch 47 Step 55/313 Test Loss: 2.022 | Test Acc: 34.710% (622/1792)\n",
            "Epoch 47 Step 56/313 Test Loss: 2.026 | Test Acc: 34.485% (629/1824)\n",
            "Epoch 47 Step 57/313 Test Loss: 2.021 | Test Acc: 34.698% (644/1856)\n",
            "Epoch 47 Step 58/313 Test Loss: 2.018 | Test Acc: 34.746% (656/1888)\n",
            "Epoch 47 Step 59/313 Test Loss: 2.018 | Test Acc: 34.635% (665/1920)\n",
            "Epoch 47 Step 60/313 Test Loss: 2.020 | Test Acc: 34.529% (674/1952)\n",
            "Epoch 47 Step 61/313 Test Loss: 2.017 | Test Acc: 34.677% (688/1984)\n",
            "Epoch 47 Step 62/313 Test Loss: 2.020 | Test Acc: 34.673% (699/2016)\n",
            "Epoch 47 Step 63/313 Test Loss: 2.014 | Test Acc: 34.766% (712/2048)\n",
            "Epoch 47 Step 64/313 Test Loss: 2.011 | Test Acc: 34.808% (724/2080)\n",
            "Epoch 47 Step 65/313 Test Loss: 2.013 | Test Acc: 34.801% (735/2112)\n",
            "Epoch 47 Step 66/313 Test Loss: 2.012 | Test Acc: 34.748% (745/2144)\n",
            "Epoch 47 Step 67/313 Test Loss: 2.015 | Test Acc: 34.605% (753/2176)\n",
            "Epoch 47 Step 68/313 Test Loss: 2.018 | Test Acc: 34.601% (764/2208)\n",
            "Epoch 47 Step 69/313 Test Loss: 2.017 | Test Acc: 34.509% (773/2240)\n",
            "Epoch 47 Step 70/313 Test Loss: 2.018 | Test Acc: 34.595% (786/2272)\n",
            "Epoch 47 Step 71/313 Test Loss: 2.019 | Test Acc: 34.462% (794/2304)\n",
            "Epoch 47 Step 72/313 Test Loss: 2.022 | Test Acc: 34.503% (806/2336)\n",
            "Epoch 47 Step 73/313 Test Loss: 2.021 | Test Acc: 34.417% (815/2368)\n",
            "Epoch 47 Step 74/313 Test Loss: 2.020 | Test Acc: 34.458% (827/2400)\n",
            "Epoch 47 Step 75/313 Test Loss: 2.021 | Test Acc: 34.334% (835/2432)\n",
            "Epoch 47 Step 76/313 Test Loss: 2.023 | Test Acc: 34.253% (844/2464)\n",
            "Epoch 47 Step 77/313 Test Loss: 2.020 | Test Acc: 34.455% (860/2496)\n",
            "Epoch 47 Step 78/313 Test Loss: 2.022 | Test Acc: 34.335% (868/2528)\n",
            "Epoch 47 Step 79/313 Test Loss: 2.024 | Test Acc: 34.336% (879/2560)\n",
            "Epoch 47 Step 80/313 Test Loss: 2.023 | Test Acc: 34.221% (887/2592)\n",
            "Epoch 47 Step 81/313 Test Loss: 2.022 | Test Acc: 34.299% (900/2624)\n",
            "Epoch 47 Step 82/313 Test Loss: 2.025 | Test Acc: 34.224% (909/2656)\n",
            "Epoch 47 Step 83/313 Test Loss: 2.021 | Test Acc: 34.301% (922/2688)\n",
            "Epoch 47 Step 84/313 Test Loss: 2.019 | Test Acc: 34.301% (933/2720)\n",
            "Epoch 47 Step 85/313 Test Loss: 2.020 | Test Acc: 34.266% (943/2752)\n",
            "Epoch 47 Step 86/313 Test Loss: 2.021 | Test Acc: 34.303% (955/2784)\n",
            "Epoch 47 Step 87/313 Test Loss: 2.022 | Test Acc: 34.304% (966/2816)\n",
            "Epoch 47 Step 88/313 Test Loss: 2.020 | Test Acc: 34.375% (979/2848)\n",
            "Epoch 47 Step 89/313 Test Loss: 2.019 | Test Acc: 34.375% (990/2880)\n",
            "Epoch 47 Step 90/313 Test Loss: 2.023 | Test Acc: 34.306% (999/2912)\n",
            "Epoch 47 Step 91/313 Test Loss: 2.021 | Test Acc: 34.443% (1014/2944)\n",
            "Epoch 47 Step 92/313 Test Loss: 2.021 | Test Acc: 34.476% (1026/2976)\n",
            "Epoch 47 Step 93/313 Test Loss: 2.021 | Test Acc: 34.408% (1035/3008)\n",
            "Epoch 47 Step 94/313 Test Loss: 2.019 | Test Acc: 34.474% (1048/3040)\n",
            "Epoch 47 Step 95/313 Test Loss: 2.015 | Test Acc: 34.570% (1062/3072)\n",
            "Epoch 47 Step 96/313 Test Loss: 2.011 | Test Acc: 34.697% (1077/3104)\n",
            "Epoch 47 Step 97/313 Test Loss: 2.016 | Test Acc: 34.471% (1081/3136)\n",
            "Epoch 47 Step 98/313 Test Loss: 2.013 | Test Acc: 34.564% (1095/3168)\n",
            "Epoch 47 Step 99/313 Test Loss: 2.011 | Test Acc: 34.656% (1109/3200)\n",
            "Epoch 47 Step 100/313 Test Loss: 2.016 | Test Acc: 34.468% (1114/3232)\n",
            "Epoch 47 Step 101/313 Test Loss: 2.013 | Test Acc: 34.559% (1128/3264)\n",
            "Epoch 47 Step 102/313 Test Loss: 2.007 | Test Acc: 34.709% (1144/3296)\n",
            "Epoch 47 Step 103/313 Test Loss: 2.010 | Test Acc: 34.706% (1155/3328)\n",
            "Epoch 47 Step 104/313 Test Loss: 2.009 | Test Acc: 34.643% (1164/3360)\n",
            "Epoch 47 Step 105/313 Test Loss: 2.006 | Test Acc: 34.699% (1177/3392)\n",
            "Epoch 47 Step 106/313 Test Loss: 2.007 | Test Acc: 34.755% (1190/3424)\n",
            "Epoch 47 Step 107/313 Test Loss: 2.007 | Test Acc: 34.722% (1200/3456)\n",
            "Epoch 47 Step 108/313 Test Loss: 2.006 | Test Acc: 34.776% (1213/3488)\n",
            "Epoch 47 Step 109/313 Test Loss: 2.006 | Test Acc: 34.688% (1221/3520)\n",
            "Epoch 47 Step 110/313 Test Loss: 2.003 | Test Acc: 34.741% (1234/3552)\n",
            "Epoch 47 Step 111/313 Test Loss: 1.999 | Test Acc: 34.905% (1251/3584)\n",
            "Epoch 47 Step 112/313 Test Loss: 2.000 | Test Acc: 34.873% (1261/3616)\n",
            "Epoch 47 Step 113/313 Test Loss: 2.000 | Test Acc: 34.868% (1272/3648)\n",
            "Epoch 47 Step 114/313 Test Loss: 1.997 | Test Acc: 34.864% (1283/3680)\n",
            "Epoch 47 Step 115/313 Test Loss: 1.996 | Test Acc: 34.887% (1295/3712)\n",
            "Epoch 47 Step 116/313 Test Loss: 1.994 | Test Acc: 34.963% (1309/3744)\n",
            "Epoch 47 Step 117/313 Test Loss: 1.993 | Test Acc: 35.037% (1323/3776)\n",
            "Epoch 47 Step 118/313 Test Loss: 1.992 | Test Acc: 35.058% (1335/3808)\n",
            "Epoch 47 Step 119/313 Test Loss: 1.988 | Test Acc: 35.260% (1354/3840)\n",
            "Epoch 47 Step 120/313 Test Loss: 1.986 | Test Acc: 35.279% (1366/3872)\n",
            "Epoch 47 Step 121/313 Test Loss: 1.987 | Test Acc: 35.272% (1377/3904)\n",
            "Epoch 47 Step 122/313 Test Loss: 1.988 | Test Acc: 35.188% (1385/3936)\n",
            "Epoch 47 Step 123/313 Test Loss: 1.985 | Test Acc: 35.207% (1397/3968)\n",
            "Epoch 47 Step 124/313 Test Loss: 1.988 | Test Acc: 35.175% (1407/4000)\n",
            "Epoch 47 Step 125/313 Test Loss: 1.986 | Test Acc: 35.218% (1420/4032)\n",
            "Epoch 47 Step 126/313 Test Loss: 1.989 | Test Acc: 35.187% (1430/4064)\n",
            "Epoch 47 Step 127/313 Test Loss: 1.988 | Test Acc: 35.254% (1444/4096)\n",
            "Epoch 47 Step 128/313 Test Loss: 1.990 | Test Acc: 35.247% (1455/4128)\n",
            "Epoch 47 Step 129/313 Test Loss: 1.991 | Test Acc: 35.192% (1464/4160)\n",
            "Epoch 47 Step 130/313 Test Loss: 1.990 | Test Acc: 35.281% (1479/4192)\n",
            "Epoch 47 Step 131/313 Test Loss: 1.992 | Test Acc: 35.346% (1493/4224)\n",
            "Epoch 47 Step 132/313 Test Loss: 1.993 | Test Acc: 35.197% (1498/4256)\n",
            "Epoch 47 Step 133/313 Test Loss: 1.991 | Test Acc: 35.285% (1513/4288)\n",
            "Epoch 47 Step 134/313 Test Loss: 1.993 | Test Acc: 35.231% (1522/4320)\n",
            "Epoch 47 Step 135/313 Test Loss: 1.993 | Test Acc: 35.202% (1532/4352)\n",
            "Epoch 47 Step 136/313 Test Loss: 1.993 | Test Acc: 35.173% (1542/4384)\n",
            "Epoch 47 Step 137/313 Test Loss: 1.990 | Test Acc: 35.190% (1554/4416)\n",
            "Epoch 47 Step 138/313 Test Loss: 1.991 | Test Acc: 35.162% (1564/4448)\n",
            "Epoch 47 Step 139/313 Test Loss: 1.990 | Test Acc: 35.223% (1578/4480)\n",
            "Epoch 47 Step 140/313 Test Loss: 1.987 | Test Acc: 35.306% (1593/4512)\n",
            "Epoch 47 Step 141/313 Test Loss: 1.985 | Test Acc: 35.321% (1605/4544)\n",
            "Epoch 47 Step 142/313 Test Loss: 1.986 | Test Acc: 35.249% (1613/4576)\n",
            "Epoch 47 Step 143/313 Test Loss: 1.985 | Test Acc: 35.330% (1628/4608)\n",
            "Epoch 47 Step 144/313 Test Loss: 1.985 | Test Acc: 35.259% (1636/4640)\n",
            "Epoch 47 Step 145/313 Test Loss: 1.983 | Test Acc: 35.338% (1651/4672)\n",
            "Epoch 47 Step 146/313 Test Loss: 1.983 | Test Acc: 35.332% (1662/4704)\n",
            "Epoch 47 Step 147/313 Test Loss: 1.984 | Test Acc: 35.283% (1671/4736)\n",
            "Epoch 47 Step 148/313 Test Loss: 1.985 | Test Acc: 35.319% (1684/4768)\n",
            "Epoch 47 Step 149/313 Test Loss: 1.986 | Test Acc: 35.271% (1693/4800)\n",
            "Epoch 47 Step 150/313 Test Loss: 1.983 | Test Acc: 35.348% (1708/4832)\n",
            "Epoch 47 Step 151/313 Test Loss: 1.983 | Test Acc: 35.424% (1723/4864)\n",
            "Epoch 47 Step 152/313 Test Loss: 1.981 | Test Acc: 35.519% (1739/4896)\n",
            "Epoch 47 Step 153/313 Test Loss: 1.977 | Test Acc: 35.552% (1752/4928)\n",
            "Epoch 47 Step 154/313 Test Loss: 1.977 | Test Acc: 35.544% (1763/4960)\n",
            "Epoch 47 Step 155/313 Test Loss: 1.978 | Test Acc: 35.537% (1774/4992)\n",
            "Epoch 47 Step 156/313 Test Loss: 1.977 | Test Acc: 35.549% (1786/5024)\n",
            "Epoch 47 Step 157/313 Test Loss: 1.981 | Test Acc: 35.483% (1794/5056)\n",
            "Epoch 47 Step 158/313 Test Loss: 1.981 | Test Acc: 35.476% (1805/5088)\n",
            "Epoch 47 Step 159/313 Test Loss: 1.982 | Test Acc: 35.469% (1816/5120)\n",
            "Epoch 47 Step 160/313 Test Loss: 1.979 | Test Acc: 35.559% (1832/5152)\n",
            "Epoch 47 Step 161/313 Test Loss: 1.978 | Test Acc: 35.629% (1847/5184)\n",
            "Epoch 47 Step 162/313 Test Loss: 1.978 | Test Acc: 35.660% (1860/5216)\n",
            "Epoch 47 Step 163/313 Test Loss: 1.982 | Test Acc: 35.556% (1866/5248)\n",
            "Epoch 47 Step 164/313 Test Loss: 1.983 | Test Acc: 35.511% (1875/5280)\n",
            "Epoch 47 Step 165/313 Test Loss: 1.984 | Test Acc: 35.542% (1888/5312)\n",
            "Epoch 47 Step 166/313 Test Loss: 1.987 | Test Acc: 35.442% (1894/5344)\n",
            "Epoch 47 Step 167/313 Test Loss: 1.990 | Test Acc: 35.435% (1905/5376)\n",
            "Epoch 47 Step 168/313 Test Loss: 1.992 | Test Acc: 35.374% (1913/5408)\n",
            "Epoch 47 Step 169/313 Test Loss: 1.992 | Test Acc: 35.349% (1923/5440)\n",
            "Epoch 47 Step 170/313 Test Loss: 1.992 | Test Acc: 35.344% (1934/5472)\n",
            "Epoch 47 Step 171/313 Test Loss: 1.994 | Test Acc: 35.320% (1944/5504)\n",
            "Epoch 47 Step 172/313 Test Loss: 1.995 | Test Acc: 35.278% (1953/5536)\n",
            "Epoch 47 Step 173/313 Test Loss: 1.996 | Test Acc: 35.201% (1960/5568)\n",
            "Epoch 47 Step 174/313 Test Loss: 1.998 | Test Acc: 35.161% (1969/5600)\n",
            "Epoch 47 Step 175/313 Test Loss: 1.998 | Test Acc: 35.192% (1982/5632)\n",
            "Epoch 47 Step 176/313 Test Loss: 1.997 | Test Acc: 35.169% (1992/5664)\n",
            "Epoch 47 Step 177/313 Test Loss: 1.995 | Test Acc: 35.218% (2006/5696)\n",
            "Epoch 47 Step 178/313 Test Loss: 1.992 | Test Acc: 35.283% (2021/5728)\n",
            "Epoch 47 Step 179/313 Test Loss: 1.992 | Test Acc: 35.226% (2029/5760)\n",
            "Epoch 47 Step 180/313 Test Loss: 1.990 | Test Acc: 35.307% (2045/5792)\n",
            "Epoch 47 Step 181/313 Test Loss: 1.991 | Test Acc: 35.302% (2056/5824)\n",
            "Epoch 47 Step 182/313 Test Loss: 1.993 | Test Acc: 35.297% (2067/5856)\n",
            "Epoch 47 Step 183/313 Test Loss: 1.993 | Test Acc: 35.309% (2079/5888)\n",
            "Epoch 47 Step 184/313 Test Loss: 1.992 | Test Acc: 35.321% (2091/5920)\n",
            "Epoch 47 Step 185/313 Test Loss: 1.992 | Test Acc: 35.316% (2102/5952)\n",
            "Epoch 47 Step 186/313 Test Loss: 1.991 | Test Acc: 35.394% (2118/5984)\n",
            "Epoch 47 Step 187/313 Test Loss: 1.990 | Test Acc: 35.406% (2130/6016)\n",
            "Epoch 47 Step 188/313 Test Loss: 1.989 | Test Acc: 35.400% (2141/6048)\n",
            "Epoch 47 Step 189/313 Test Loss: 1.990 | Test Acc: 35.428% (2154/6080)\n",
            "Epoch 47 Step 190/313 Test Loss: 1.989 | Test Acc: 35.455% (2167/6112)\n",
            "Epoch 47 Step 191/313 Test Loss: 1.988 | Test Acc: 35.449% (2178/6144)\n",
            "Epoch 47 Step 192/313 Test Loss: 1.988 | Test Acc: 35.427% (2188/6176)\n",
            "Epoch 47 Step 193/313 Test Loss: 1.987 | Test Acc: 35.470% (2202/6208)\n",
            "Epoch 47 Step 194/313 Test Loss: 1.988 | Test Acc: 35.449% (2212/6240)\n",
            "Epoch 47 Step 195/313 Test Loss: 1.988 | Test Acc: 35.459% (2224/6272)\n",
            "Epoch 47 Step 196/313 Test Loss: 1.986 | Test Acc: 35.501% (2238/6304)\n",
            "Epoch 47 Step 197/313 Test Loss: 1.986 | Test Acc: 35.574% (2254/6336)\n",
            "Epoch 47 Step 198/313 Test Loss: 1.984 | Test Acc: 35.663% (2271/6368)\n",
            "Epoch 47 Step 199/313 Test Loss: 1.983 | Test Acc: 35.656% (2282/6400)\n",
            "Epoch 47 Step 200/313 Test Loss: 1.983 | Test Acc: 35.619% (2291/6432)\n",
            "Epoch 47 Step 201/313 Test Loss: 1.984 | Test Acc: 35.597% (2301/6464)\n",
            "Epoch 47 Step 202/313 Test Loss: 1.984 | Test Acc: 35.653% (2316/6496)\n",
            "Epoch 47 Step 203/313 Test Loss: 1.983 | Test Acc: 35.692% (2330/6528)\n",
            "Epoch 47 Step 204/313 Test Loss: 1.985 | Test Acc: 35.671% (2340/6560)\n",
            "Epoch 47 Step 205/313 Test Loss: 1.986 | Test Acc: 35.634% (2349/6592)\n",
            "Epoch 47 Step 206/313 Test Loss: 1.987 | Test Acc: 35.583% (2357/6624)\n",
            "Epoch 47 Step 207/313 Test Loss: 1.985 | Test Acc: 35.637% (2372/6656)\n",
            "Epoch 47 Step 208/313 Test Loss: 1.985 | Test Acc: 35.646% (2384/6688)\n",
            "Epoch 47 Step 209/313 Test Loss: 1.986 | Test Acc: 35.565% (2390/6720)\n",
            "Epoch 47 Step 210/313 Test Loss: 1.985 | Test Acc: 35.604% (2404/6752)\n",
            "Epoch 47 Step 211/313 Test Loss: 1.985 | Test Acc: 35.584% (2414/6784)\n",
            "Epoch 47 Step 212/313 Test Loss: 1.983 | Test Acc: 35.651% (2430/6816)\n",
            "Epoch 47 Step 213/313 Test Loss: 1.982 | Test Acc: 35.675% (2443/6848)\n",
            "Epoch 47 Step 214/313 Test Loss: 1.985 | Test Acc: 35.610% (2450/6880)\n",
            "Epoch 47 Step 215/313 Test Loss: 1.983 | Test Acc: 35.677% (2466/6912)\n",
            "Epoch 47 Step 216/313 Test Loss: 1.984 | Test Acc: 35.671% (2477/6944)\n",
            "Epoch 47 Step 217/313 Test Loss: 1.984 | Test Acc: 35.622% (2485/6976)\n",
            "Epoch 47 Step 218/313 Test Loss: 1.986 | Test Acc: 35.616% (2496/7008)\n",
            "Epoch 47 Step 219/313 Test Loss: 1.988 | Test Acc: 35.582% (2505/7040)\n",
            "Epoch 47 Step 220/313 Test Loss: 1.991 | Test Acc: 35.535% (2513/7072)\n",
            "Epoch 47 Step 221/313 Test Loss: 1.991 | Test Acc: 35.557% (2526/7104)\n",
            "Epoch 47 Step 222/313 Test Loss: 1.993 | Test Acc: 35.510% (2534/7136)\n",
            "Epoch 47 Step 223/313 Test Loss: 1.991 | Test Acc: 35.533% (2547/7168)\n",
            "Epoch 47 Step 224/313 Test Loss: 1.992 | Test Acc: 35.500% (2556/7200)\n",
            "Epoch 47 Step 225/313 Test Loss: 1.993 | Test Acc: 35.454% (2564/7232)\n",
            "Epoch 47 Step 226/313 Test Loss: 1.992 | Test Acc: 35.449% (2575/7264)\n",
            "Epoch 47 Step 227/313 Test Loss: 1.989 | Test Acc: 35.540% (2593/7296)\n",
            "Epoch 47 Step 228/313 Test Loss: 1.989 | Test Acc: 35.521% (2603/7328)\n",
            "Epoch 47 Step 229/313 Test Loss: 1.989 | Test Acc: 35.516% (2614/7360)\n",
            "Epoch 47 Step 230/313 Test Loss: 1.988 | Test Acc: 35.525% (2626/7392)\n",
            "Epoch 47 Step 231/313 Test Loss: 1.987 | Test Acc: 35.587% (2642/7424)\n",
            "Epoch 47 Step 232/313 Test Loss: 1.986 | Test Acc: 35.582% (2653/7456)\n",
            "Epoch 47 Step 233/313 Test Loss: 1.987 | Test Acc: 35.590% (2665/7488)\n",
            "Epoch 47 Step 234/313 Test Loss: 1.986 | Test Acc: 35.612% (2678/7520)\n",
            "Epoch 47 Step 235/313 Test Loss: 1.984 | Test Acc: 35.633% (2691/7552)\n",
            "Epoch 47 Step 236/313 Test Loss: 1.986 | Test Acc: 35.601% (2700/7584)\n",
            "Epoch 47 Step 237/313 Test Loss: 1.986 | Test Acc: 35.583% (2710/7616)\n",
            "Epoch 47 Step 238/313 Test Loss: 1.987 | Test Acc: 35.539% (2718/7648)\n",
            "Epoch 47 Step 239/313 Test Loss: 1.985 | Test Acc: 35.547% (2730/7680)\n",
            "Epoch 47 Step 240/313 Test Loss: 1.984 | Test Acc: 35.607% (2746/7712)\n",
            "Epoch 47 Step 241/313 Test Loss: 1.983 | Test Acc: 35.666% (2762/7744)\n",
            "Epoch 47 Step 242/313 Test Loss: 1.982 | Test Acc: 35.687% (2775/7776)\n",
            "Epoch 47 Step 243/313 Test Loss: 1.980 | Test Acc: 35.733% (2790/7808)\n",
            "Epoch 47 Step 244/313 Test Loss: 1.982 | Test Acc: 35.689% (2798/7840)\n",
            "Epoch 47 Step 245/313 Test Loss: 1.983 | Test Acc: 35.671% (2808/7872)\n",
            "Epoch 47 Step 246/313 Test Loss: 1.983 | Test Acc: 35.665% (2819/7904)\n",
            "Epoch 47 Step 247/313 Test Loss: 1.983 | Test Acc: 35.635% (2828/7936)\n",
            "Epoch 47 Step 248/313 Test Loss: 1.984 | Test Acc: 35.580% (2835/7968)\n",
            "Epoch 47 Step 249/313 Test Loss: 1.983 | Test Acc: 35.575% (2846/8000)\n",
            "Epoch 47 Step 250/313 Test Loss: 1.982 | Test Acc: 35.608% (2860/8032)\n",
            "Epoch 47 Step 251/313 Test Loss: 1.981 | Test Acc: 35.627% (2873/8064)\n",
            "Epoch 47 Step 252/313 Test Loss: 1.981 | Test Acc: 35.647% (2886/8096)\n",
            "Epoch 47 Step 253/313 Test Loss: 1.981 | Test Acc: 35.630% (2896/8128)\n",
            "Epoch 47 Step 254/313 Test Loss: 1.982 | Test Acc: 35.600% (2905/8160)\n",
            "Epoch 47 Step 255/313 Test Loss: 1.981 | Test Acc: 35.608% (2917/8192)\n",
            "Epoch 47 Step 256/313 Test Loss: 1.981 | Test Acc: 35.554% (2924/8224)\n",
            "Epoch 47 Step 257/313 Test Loss: 1.981 | Test Acc: 35.562% (2936/8256)\n",
            "Epoch 47 Step 258/313 Test Loss: 1.981 | Test Acc: 35.533% (2945/8288)\n",
            "Epoch 47 Step 259/313 Test Loss: 1.982 | Test Acc: 35.481% (2952/8320)\n",
            "Epoch 47 Step 260/313 Test Loss: 1.983 | Test Acc: 35.453% (2961/8352)\n",
            "Epoch 47 Step 261/313 Test Loss: 1.982 | Test Acc: 35.460% (2973/8384)\n",
            "Epoch 47 Step 262/313 Test Loss: 1.981 | Test Acc: 35.433% (2982/8416)\n",
            "Epoch 47 Step 263/313 Test Loss: 1.981 | Test Acc: 35.440% (2994/8448)\n",
            "Epoch 47 Step 264/313 Test Loss: 1.982 | Test Acc: 35.413% (3003/8480)\n",
            "Epoch 47 Step 265/313 Test Loss: 1.983 | Test Acc: 35.385% (3012/8512)\n",
            "Epoch 47 Step 266/313 Test Loss: 1.983 | Test Acc: 35.393% (3024/8544)\n",
            "Epoch 47 Step 267/313 Test Loss: 1.984 | Test Acc: 35.378% (3034/8576)\n",
            "Epoch 47 Step 268/313 Test Loss: 1.983 | Test Acc: 35.409% (3048/8608)\n",
            "Epoch 47 Step 269/313 Test Loss: 1.983 | Test Acc: 35.417% (3060/8640)\n",
            "Epoch 47 Step 270/313 Test Loss: 1.984 | Test Acc: 35.378% (3068/8672)\n",
            "Epoch 47 Step 271/313 Test Loss: 1.985 | Test Acc: 35.317% (3074/8704)\n",
            "Epoch 47 Step 272/313 Test Loss: 1.985 | Test Acc: 35.291% (3083/8736)\n",
            "Epoch 47 Step 273/313 Test Loss: 1.984 | Test Acc: 35.322% (3097/8768)\n",
            "Epoch 47 Step 274/313 Test Loss: 1.983 | Test Acc: 35.295% (3106/8800)\n",
            "Epoch 47 Step 275/313 Test Loss: 1.984 | Test Acc: 35.281% (3116/8832)\n",
            "Epoch 47 Step 276/313 Test Loss: 1.986 | Test Acc: 35.244% (3124/8864)\n",
            "Epoch 47 Step 277/313 Test Loss: 1.985 | Test Acc: 35.252% (3136/8896)\n",
            "Epoch 47 Step 278/313 Test Loss: 1.983 | Test Acc: 35.316% (3153/8928)\n",
            "Epoch 47 Step 279/313 Test Loss: 1.984 | Test Acc: 35.301% (3163/8960)\n",
            "Epoch 47 Step 280/313 Test Loss: 1.984 | Test Acc: 35.254% (3170/8992)\n",
            "Epoch 47 Step 281/313 Test Loss: 1.986 | Test Acc: 35.273% (3183/9024)\n",
            "Epoch 47 Step 282/313 Test Loss: 1.985 | Test Acc: 35.292% (3196/9056)\n",
            "Epoch 47 Step 283/313 Test Loss: 1.985 | Test Acc: 35.332% (3211/9088)\n",
            "Epoch 47 Step 284/313 Test Loss: 1.985 | Test Acc: 35.340% (3223/9120)\n",
            "Epoch 47 Step 285/313 Test Loss: 1.986 | Test Acc: 35.358% (3236/9152)\n",
            "Epoch 47 Step 286/313 Test Loss: 1.986 | Test Acc: 35.377% (3249/9184)\n",
            "Epoch 47 Step 287/313 Test Loss: 1.984 | Test Acc: 35.438% (3266/9216)\n",
            "Epoch 47 Step 288/313 Test Loss: 1.983 | Test Acc: 35.467% (3280/9248)\n",
            "Epoch 47 Step 289/313 Test Loss: 1.982 | Test Acc: 35.463% (3291/9280)\n",
            "Epoch 47 Step 290/313 Test Loss: 1.982 | Test Acc: 35.427% (3299/9312)\n",
            "Epoch 47 Step 291/313 Test Loss: 1.983 | Test Acc: 35.392% (3307/9344)\n",
            "Epoch 47 Step 292/313 Test Loss: 1.982 | Test Acc: 35.388% (3318/9376)\n",
            "Epoch 47 Step 293/313 Test Loss: 1.982 | Test Acc: 35.385% (3329/9408)\n",
            "Epoch 47 Step 294/313 Test Loss: 1.982 | Test Acc: 35.381% (3340/9440)\n",
            "Epoch 47 Step 295/313 Test Loss: 1.982 | Test Acc: 35.378% (3351/9472)\n",
            "Epoch 47 Step 296/313 Test Loss: 1.982 | Test Acc: 35.396% (3364/9504)\n",
            "Epoch 47 Step 297/313 Test Loss: 1.982 | Test Acc: 35.392% (3375/9536)\n",
            "Epoch 47 Step 298/313 Test Loss: 1.981 | Test Acc: 35.420% (3389/9568)\n",
            "Epoch 47 Step 299/313 Test Loss: 1.980 | Test Acc: 35.448% (3403/9600)\n",
            "Epoch 47 Step 300/313 Test Loss: 1.980 | Test Acc: 35.465% (3416/9632)\n",
            "Epoch 47 Step 301/313 Test Loss: 1.979 | Test Acc: 35.472% (3428/9664)\n",
            "Epoch 47 Step 302/313 Test Loss: 1.981 | Test Acc: 35.448% (3437/9696)\n",
            "Epoch 47 Step 303/313 Test Loss: 1.980 | Test Acc: 35.506% (3454/9728)\n",
            "Epoch 47 Step 304/313 Test Loss: 1.981 | Test Acc: 35.492% (3464/9760)\n",
            "Epoch 47 Step 305/313 Test Loss: 1.982 | Test Acc: 35.468% (3473/9792)\n",
            "Epoch 47 Step 306/313 Test Loss: 1.983 | Test Acc: 35.515% (3489/9824)\n",
            "Epoch 47 Step 307/313 Test Loss: 1.983 | Test Acc: 35.491% (3498/9856)\n",
            "Epoch 47 Step 308/313 Test Loss: 1.984 | Test Acc: 35.457% (3506/9888)\n",
            "Epoch 47 Step 309/313 Test Loss: 1.984 | Test Acc: 35.433% (3515/9920)\n",
            "Epoch 47 Step 310/313 Test Loss: 1.985 | Test Acc: 35.460% (3529/9952)\n",
            "Epoch 47 Step 311/313 Test Loss: 1.984 | Test Acc: 35.497% (3544/9984)\n",
            "Epoch 47 Step 312/313 Test Loss: 1.983 | Test Acc: 35.500% (3550/10000)\n",
            "\n",
            "Epoch: 48\n",
            "Epoch 48 Step 0/1563 Loss: 1.729 | Acc: 34.375% (11/32)\n",
            "Epoch 48 Step 1/1563 Loss: 1.595 | Acc: 35.938% (23/64)\n",
            "Epoch 48 Step 2/1563 Loss: 1.724 | Acc: 36.458% (35/96)\n",
            "Epoch 48 Step 3/1563 Loss: 1.747 | Acc: 36.719% (47/128)\n",
            "Epoch 48 Step 4/1563 Loss: 1.726 | Acc: 36.875% (59/160)\n",
            "Epoch 48 Step 5/1563 Loss: 1.807 | Acc: 34.896% (67/192)\n",
            "Epoch 48 Step 6/1563 Loss: 1.899 | Acc: 32.589% (73/224)\n",
            "Epoch 48 Step 7/1563 Loss: 1.876 | Acc: 34.375% (88/256)\n",
            "Epoch 48 Step 8/1563 Loss: 1.905 | Acc: 32.292% (93/288)\n",
            "Epoch 48 Step 9/1563 Loss: 1.964 | Acc: 30.625% (98/320)\n",
            "Epoch 48 Step 10/1563 Loss: 1.970 | Acc: 30.398% (107/352)\n",
            "Epoch 48 Step 11/1563 Loss: 2.010 | Acc: 30.469% (117/384)\n",
            "Epoch 48 Step 12/1563 Loss: 1.987 | Acc: 31.250% (130/416)\n",
            "Epoch 48 Step 13/1563 Loss: 1.998 | Acc: 30.804% (138/448)\n",
            "Epoch 48 Step 14/1563 Loss: 1.986 | Acc: 31.250% (150/480)\n",
            "Epoch 48 Step 15/1563 Loss: 2.020 | Acc: 30.664% (157/512)\n",
            "Epoch 48 Step 16/1563 Loss: 1.995 | Acc: 31.434% (171/544)\n",
            "Epoch 48 Step 17/1563 Loss: 1.998 | Acc: 31.771% (183/576)\n",
            "Epoch 48 Step 18/1563 Loss: 1.987 | Acc: 31.908% (194/608)\n",
            "Epoch 48 Step 19/1563 Loss: 2.008 | Acc: 31.406% (201/640)\n",
            "Epoch 48 Step 20/1563 Loss: 1.991 | Acc: 31.845% (214/672)\n",
            "Epoch 48 Step 21/1563 Loss: 1.990 | Acc: 31.960% (225/704)\n",
            "Epoch 48 Step 22/1563 Loss: 2.025 | Acc: 31.658% (233/736)\n",
            "Epoch 48 Step 23/1563 Loss: 2.031 | Acc: 31.510% (242/768)\n",
            "Epoch 48 Step 24/1563 Loss: 2.039 | Acc: 31.125% (249/800)\n",
            "Epoch 48 Step 25/1563 Loss: 2.078 | Acc: 30.649% (255/832)\n",
            "Epoch 48 Step 26/1563 Loss: 2.085 | Acc: 30.324% (262/864)\n",
            "Epoch 48 Step 27/1563 Loss: 2.090 | Acc: 29.911% (268/896)\n",
            "Epoch 48 Step 28/1563 Loss: 2.078 | Acc: 29.849% (277/928)\n",
            "Epoch 48 Step 29/1563 Loss: 2.067 | Acc: 30.208% (290/960)\n",
            "Epoch 48 Step 30/1563 Loss: 2.066 | Acc: 30.040% (298/992)\n",
            "Epoch 48 Step 31/1563 Loss: 2.068 | Acc: 29.883% (306/1024)\n",
            "Epoch 48 Step 32/1563 Loss: 2.076 | Acc: 29.924% (316/1056)\n",
            "Epoch 48 Step 33/1563 Loss: 2.068 | Acc: 29.963% (326/1088)\n",
            "Epoch 48 Step 34/1563 Loss: 2.060 | Acc: 30.357% (340/1120)\n",
            "Epoch 48 Step 35/1563 Loss: 2.054 | Acc: 30.642% (353/1152)\n",
            "Epoch 48 Step 36/1563 Loss: 2.061 | Acc: 30.743% (364/1184)\n",
            "Epoch 48 Step 37/1563 Loss: 2.065 | Acc: 30.757% (374/1216)\n",
            "Epoch 48 Step 38/1563 Loss: 2.068 | Acc: 31.010% (387/1248)\n",
            "Epoch 48 Step 39/1563 Loss: 2.070 | Acc: 30.938% (396/1280)\n",
            "Epoch 48 Step 40/1563 Loss: 2.069 | Acc: 30.869% (405/1312)\n",
            "Epoch 48 Step 41/1563 Loss: 2.071 | Acc: 30.804% (414/1344)\n",
            "Epoch 48 Step 42/1563 Loss: 2.067 | Acc: 30.814% (424/1376)\n",
            "Epoch 48 Step 43/1563 Loss: 2.065 | Acc: 30.611% (431/1408)\n",
            "Epoch 48 Step 44/1563 Loss: 2.076 | Acc: 30.556% (440/1440)\n",
            "Epoch 48 Step 45/1563 Loss: 2.077 | Acc: 30.503% (449/1472)\n",
            "Epoch 48 Step 46/1563 Loss: 2.077 | Acc: 30.253% (455/1504)\n",
            "Epoch 48 Step 47/1563 Loss: 2.084 | Acc: 30.339% (466/1536)\n",
            "Epoch 48 Step 48/1563 Loss: 2.089 | Acc: 30.230% (474/1568)\n",
            "Epoch 48 Step 49/1563 Loss: 2.092 | Acc: 30.312% (485/1600)\n",
            "Epoch 48 Step 50/1563 Loss: 2.100 | Acc: 30.147% (492/1632)\n",
            "Epoch 48 Step 51/1563 Loss: 2.089 | Acc: 30.469% (507/1664)\n",
            "Epoch 48 Step 52/1563 Loss: 2.083 | Acc: 30.660% (520/1696)\n",
            "Epoch 48 Step 53/1563 Loss: 2.079 | Acc: 30.671% (530/1728)\n",
            "Epoch 48 Step 54/1563 Loss: 2.074 | Acc: 30.568% (538/1760)\n",
            "Epoch 48 Step 55/1563 Loss: 2.066 | Acc: 30.859% (553/1792)\n",
            "Epoch 48 Step 56/1563 Loss: 2.069 | Acc: 30.866% (563/1824)\n",
            "Epoch 48 Step 57/1563 Loss: 2.070 | Acc: 30.927% (574/1856)\n",
            "Epoch 48 Step 58/1563 Loss: 2.071 | Acc: 30.720% (580/1888)\n",
            "Epoch 48 Step 59/1563 Loss: 2.078 | Acc: 30.469% (585/1920)\n",
            "Epoch 48 Step 60/1563 Loss: 2.081 | Acc: 30.482% (595/1952)\n",
            "Epoch 48 Step 61/1563 Loss: 2.088 | Acc: 30.343% (602/1984)\n",
            "Epoch 48 Step 62/1563 Loss: 2.087 | Acc: 30.456% (614/2016)\n",
            "Epoch 48 Step 63/1563 Loss: 2.085 | Acc: 30.615% (627/2048)\n",
            "Epoch 48 Step 64/1563 Loss: 2.082 | Acc: 30.817% (641/2080)\n",
            "Epoch 48 Step 65/1563 Loss: 2.081 | Acc: 30.777% (650/2112)\n",
            "Epoch 48 Step 66/1563 Loss: 2.083 | Acc: 30.597% (656/2144)\n",
            "Epoch 48 Step 67/1563 Loss: 2.080 | Acc: 30.790% (670/2176)\n",
            "Epoch 48 Step 68/1563 Loss: 2.083 | Acc: 30.752% (679/2208)\n",
            "Epoch 48 Step 69/1563 Loss: 2.081 | Acc: 30.804% (690/2240)\n",
            "Epoch 48 Step 70/1563 Loss: 2.083 | Acc: 30.854% (701/2272)\n",
            "Epoch 48 Step 71/1563 Loss: 2.086 | Acc: 30.686% (707/2304)\n",
            "Epoch 48 Step 72/1563 Loss: 2.083 | Acc: 30.693% (717/2336)\n",
            "Epoch 48 Step 73/1563 Loss: 2.077 | Acc: 30.954% (733/2368)\n",
            "Epoch 48 Step 74/1563 Loss: 2.077 | Acc: 31.042% (745/2400)\n",
            "Epoch 48 Step 75/1563 Loss: 2.075 | Acc: 31.044% (755/2432)\n",
            "Epoch 48 Step 76/1563 Loss: 2.074 | Acc: 31.006% (764/2464)\n",
            "Epoch 48 Step 77/1563 Loss: 2.070 | Acc: 31.250% (780/2496)\n",
            "Epoch 48 Step 78/1563 Loss: 2.073 | Acc: 31.171% (788/2528)\n",
            "Epoch 48 Step 79/1563 Loss: 2.069 | Acc: 31.328% (802/2560)\n",
            "Epoch 48 Step 80/1563 Loss: 2.067 | Acc: 31.404% (814/2592)\n",
            "Epoch 48 Step 81/1563 Loss: 2.072 | Acc: 31.174% (818/2624)\n",
            "Epoch 48 Step 82/1563 Loss: 2.075 | Acc: 31.062% (825/2656)\n",
            "Epoch 48 Step 83/1563 Loss: 2.081 | Acc: 31.064% (835/2688)\n",
            "Epoch 48 Step 84/1563 Loss: 2.081 | Acc: 31.029% (844/2720)\n",
            "Epoch 48 Step 85/1563 Loss: 2.080 | Acc: 31.214% (859/2752)\n",
            "Epoch 48 Step 86/1563 Loss: 2.078 | Acc: 31.178% (868/2784)\n",
            "Epoch 48 Step 87/1563 Loss: 2.076 | Acc: 31.250% (880/2816)\n",
            "Epoch 48 Step 88/1563 Loss: 2.071 | Acc: 31.390% (894/2848)\n",
            "Epoch 48 Step 89/1563 Loss: 2.072 | Acc: 31.458% (906/2880)\n",
            "Epoch 48 Step 90/1563 Loss: 2.066 | Acc: 31.731% (924/2912)\n",
            "Epoch 48 Step 91/1563 Loss: 2.069 | Acc: 31.692% (933/2944)\n",
            "Epoch 48 Step 92/1563 Loss: 2.071 | Acc: 31.687% (943/2976)\n",
            "Epoch 48 Step 93/1563 Loss: 2.073 | Acc: 31.715% (954/3008)\n",
            "Epoch 48 Step 94/1563 Loss: 2.070 | Acc: 31.842% (968/3040)\n",
            "Epoch 48 Step 95/1563 Loss: 2.067 | Acc: 31.934% (981/3072)\n",
            "Epoch 48 Step 96/1563 Loss: 2.067 | Acc: 31.894% (990/3104)\n",
            "Epoch 48 Step 97/1563 Loss: 2.064 | Acc: 31.983% (1003/3136)\n",
            "Epoch 48 Step 98/1563 Loss: 2.068 | Acc: 31.944% (1012/3168)\n",
            "Epoch 48 Step 99/1563 Loss: 2.068 | Acc: 31.906% (1021/3200)\n",
            "Epoch 48 Step 100/1563 Loss: 2.072 | Acc: 31.900% (1031/3232)\n",
            "Epoch 48 Step 101/1563 Loss: 2.072 | Acc: 31.924% (1042/3264)\n",
            "Epoch 48 Step 102/1563 Loss: 2.073 | Acc: 31.978% (1054/3296)\n",
            "Epoch 48 Step 103/1563 Loss: 2.072 | Acc: 31.941% (1063/3328)\n",
            "Epoch 48 Step 104/1563 Loss: 2.075 | Acc: 31.935% (1073/3360)\n",
            "Epoch 48 Step 105/1563 Loss: 2.076 | Acc: 31.928% (1083/3392)\n",
            "Epoch 48 Step 106/1563 Loss: 2.076 | Acc: 31.980% (1095/3424)\n",
            "Epoch 48 Step 107/1563 Loss: 2.075 | Acc: 32.002% (1106/3456)\n",
            "Epoch 48 Step 108/1563 Loss: 2.077 | Acc: 31.967% (1115/3488)\n",
            "Epoch 48 Step 109/1563 Loss: 2.076 | Acc: 32.102% (1130/3520)\n",
            "Epoch 48 Step 110/1563 Loss: 2.073 | Acc: 32.151% (1142/3552)\n",
            "Epoch 48 Step 111/1563 Loss: 2.072 | Acc: 32.143% (1152/3584)\n",
            "Epoch 48 Step 112/1563 Loss: 2.072 | Acc: 32.163% (1163/3616)\n",
            "Epoch 48 Step 113/1563 Loss: 2.068 | Acc: 32.292% (1178/3648)\n",
            "Epoch 48 Step 114/1563 Loss: 2.065 | Acc: 32.310% (1189/3680)\n",
            "Epoch 48 Step 115/1563 Loss: 2.062 | Acc: 32.274% (1198/3712)\n",
            "Epoch 48 Step 116/1563 Loss: 2.065 | Acc: 32.212% (1206/3744)\n",
            "Epoch 48 Step 117/1563 Loss: 2.062 | Acc: 32.309% (1220/3776)\n",
            "Epoch 48 Step 118/1563 Loss: 2.059 | Acc: 32.353% (1232/3808)\n",
            "Epoch 48 Step 119/1563 Loss: 2.057 | Acc: 32.422% (1245/3840)\n",
            "Epoch 48 Step 120/1563 Loss: 2.058 | Acc: 32.412% (1255/3872)\n",
            "Epoch 48 Step 121/1563 Loss: 2.058 | Acc: 32.351% (1263/3904)\n",
            "Epoch 48 Step 122/1563 Loss: 2.059 | Acc: 32.393% (1275/3936)\n",
            "Epoch 48 Step 123/1563 Loss: 2.059 | Acc: 32.384% (1285/3968)\n",
            "Epoch 48 Step 124/1563 Loss: 2.058 | Acc: 32.500% (1300/4000)\n",
            "Epoch 48 Step 125/1563 Loss: 2.056 | Acc: 32.465% (1309/4032)\n",
            "Epoch 48 Step 126/1563 Loss: 2.059 | Acc: 32.456% (1319/4064)\n",
            "Epoch 48 Step 127/1563 Loss: 2.059 | Acc: 32.373% (1326/4096)\n",
            "Epoch 48 Step 128/1563 Loss: 2.061 | Acc: 32.389% (1337/4128)\n",
            "Epoch 48 Step 129/1563 Loss: 2.059 | Acc: 32.404% (1348/4160)\n",
            "Epoch 48 Step 130/1563 Loss: 2.061 | Acc: 32.395% (1358/4192)\n",
            "Epoch 48 Step 131/1563 Loss: 2.062 | Acc: 32.434% (1370/4224)\n",
            "Epoch 48 Step 132/1563 Loss: 2.060 | Acc: 32.542% (1385/4256)\n",
            "Epoch 48 Step 133/1563 Loss: 2.062 | Acc: 32.533% (1395/4288)\n",
            "Epoch 48 Step 134/1563 Loss: 2.064 | Acc: 32.500% (1404/4320)\n",
            "Epoch 48 Step 135/1563 Loss: 2.065 | Acc: 32.376% (1409/4352)\n",
            "Epoch 48 Step 136/1563 Loss: 2.064 | Acc: 32.436% (1422/4384)\n",
            "Epoch 48 Step 137/1563 Loss: 2.063 | Acc: 32.405% (1431/4416)\n",
            "Epoch 48 Step 138/1563 Loss: 2.066 | Acc: 32.284% (1436/4448)\n",
            "Epoch 48 Step 139/1563 Loss: 2.066 | Acc: 32.344% (1449/4480)\n",
            "Epoch 48 Step 140/1563 Loss: 2.066 | Acc: 32.336% (1459/4512)\n",
            "Epoch 48 Step 141/1563 Loss: 2.067 | Acc: 32.306% (1468/4544)\n",
            "Epoch 48 Step 142/1563 Loss: 2.065 | Acc: 32.408% (1483/4576)\n",
            "Epoch 48 Step 143/1563 Loss: 2.066 | Acc: 32.422% (1494/4608)\n",
            "Epoch 48 Step 144/1563 Loss: 2.064 | Acc: 32.478% (1507/4640)\n",
            "Epoch 48 Step 145/1563 Loss: 2.066 | Acc: 32.427% (1515/4672)\n",
            "Epoch 48 Step 146/1563 Loss: 2.064 | Acc: 32.526% (1530/4704)\n",
            "Epoch 48 Step 147/1563 Loss: 2.064 | Acc: 32.517% (1540/4736)\n",
            "Epoch 48 Step 148/1563 Loss: 2.066 | Acc: 32.571% (1553/4768)\n",
            "Epoch 48 Step 149/1563 Loss: 2.064 | Acc: 32.583% (1564/4800)\n",
            "Epoch 48 Step 150/1563 Loss: 2.068 | Acc: 32.512% (1571/4832)\n",
            "Epoch 48 Step 151/1563 Loss: 2.065 | Acc: 32.504% (1581/4864)\n",
            "Epoch 48 Step 152/1563 Loss: 2.065 | Acc: 32.516% (1592/4896)\n",
            "Epoch 48 Step 153/1563 Loss: 2.063 | Acc: 32.569% (1605/4928)\n",
            "Epoch 48 Step 154/1563 Loss: 2.063 | Acc: 32.560% (1615/4960)\n",
            "Epoch 48 Step 155/1563 Loss: 2.062 | Acc: 32.592% (1627/4992)\n",
            "Epoch 48 Step 156/1563 Loss: 2.063 | Acc: 32.584% (1637/5024)\n",
            "Epoch 48 Step 157/1563 Loss: 2.062 | Acc: 32.634% (1650/5056)\n",
            "Epoch 48 Step 158/1563 Loss: 2.060 | Acc: 32.665% (1662/5088)\n",
            "Epoch 48 Step 159/1563 Loss: 2.061 | Acc: 32.617% (1670/5120)\n",
            "Epoch 48 Step 160/1563 Loss: 2.058 | Acc: 32.686% (1684/5152)\n",
            "Epoch 48 Step 161/1563 Loss: 2.059 | Acc: 32.697% (1695/5184)\n",
            "Epoch 48 Step 162/1563 Loss: 2.058 | Acc: 32.688% (1705/5216)\n",
            "Epoch 48 Step 163/1563 Loss: 2.057 | Acc: 32.736% (1718/5248)\n",
            "Epoch 48 Step 164/1563 Loss: 2.055 | Acc: 32.746% (1729/5280)\n",
            "Epoch 48 Step 165/1563 Loss: 2.056 | Acc: 32.700% (1737/5312)\n",
            "Epoch 48 Step 166/1563 Loss: 2.059 | Acc: 32.579% (1741/5344)\n",
            "Epoch 48 Step 167/1563 Loss: 2.061 | Acc: 32.552% (1750/5376)\n",
            "Epoch 48 Step 168/1563 Loss: 2.061 | Acc: 32.600% (1763/5408)\n",
            "Epoch 48 Step 169/1563 Loss: 2.065 | Acc: 32.518% (1769/5440)\n",
            "Epoch 48 Step 170/1563 Loss: 2.065 | Acc: 32.456% (1776/5472)\n",
            "Epoch 48 Step 171/1563 Loss: 2.064 | Acc: 32.485% (1788/5504)\n",
            "Epoch 48 Step 172/1563 Loss: 2.061 | Acc: 32.496% (1799/5536)\n",
            "Epoch 48 Step 173/1563 Loss: 2.058 | Acc: 32.579% (1814/5568)\n",
            "Epoch 48 Step 174/1563 Loss: 2.059 | Acc: 32.536% (1822/5600)\n",
            "Epoch 48 Step 175/1563 Loss: 2.059 | Acc: 32.475% (1829/5632)\n",
            "Epoch 48 Step 176/1563 Loss: 2.060 | Acc: 32.521% (1842/5664)\n",
            "Epoch 48 Step 177/1563 Loss: 2.059 | Acc: 32.532% (1853/5696)\n",
            "Epoch 48 Step 178/1563 Loss: 2.057 | Acc: 32.559% (1865/5728)\n",
            "Epoch 48 Step 179/1563 Loss: 2.057 | Acc: 32.483% (1871/5760)\n",
            "Epoch 48 Step 180/1563 Loss: 2.058 | Acc: 32.476% (1881/5792)\n",
            "Epoch 48 Step 181/1563 Loss: 2.059 | Acc: 32.435% (1889/5824)\n",
            "Epoch 48 Step 182/1563 Loss: 2.059 | Acc: 32.445% (1900/5856)\n",
            "Epoch 48 Step 183/1563 Loss: 2.061 | Acc: 32.422% (1909/5888)\n",
            "Epoch 48 Step 184/1563 Loss: 2.059 | Acc: 32.483% (1923/5920)\n",
            "Epoch 48 Step 185/1563 Loss: 2.060 | Acc: 32.443% (1931/5952)\n",
            "Epoch 48 Step 186/1563 Loss: 2.063 | Acc: 32.403% (1939/5984)\n",
            "Epoch 48 Step 187/1563 Loss: 2.064 | Acc: 32.397% (1949/6016)\n",
            "Epoch 48 Step 188/1563 Loss: 2.063 | Acc: 32.457% (1963/6048)\n",
            "Epoch 48 Step 189/1563 Loss: 2.063 | Acc: 32.451% (1973/6080)\n",
            "Epoch 48 Step 190/1563 Loss: 2.062 | Acc: 32.493% (1986/6112)\n",
            "Epoch 48 Step 191/1563 Loss: 2.061 | Acc: 32.568% (2001/6144)\n",
            "Epoch 48 Step 192/1563 Loss: 2.061 | Acc: 32.562% (2011/6176)\n",
            "Epoch 48 Step 193/1563 Loss: 2.062 | Acc: 32.603% (2024/6208)\n",
            "Epoch 48 Step 194/1563 Loss: 2.062 | Acc: 32.628% (2036/6240)\n",
            "Epoch 48 Step 195/1563 Loss: 2.061 | Acc: 32.717% (2052/6272)\n",
            "Epoch 48 Step 196/1563 Loss: 2.062 | Acc: 32.662% (2059/6304)\n",
            "Epoch 48 Step 197/1563 Loss: 2.063 | Acc: 32.670% (2070/6336)\n",
            "Epoch 48 Step 198/1563 Loss: 2.064 | Acc: 32.648% (2079/6368)\n",
            "Epoch 48 Step 199/1563 Loss: 2.064 | Acc: 32.641% (2089/6400)\n",
            "Epoch 48 Step 200/1563 Loss: 2.063 | Acc: 32.649% (2100/6432)\n",
            "Epoch 48 Step 201/1563 Loss: 2.063 | Acc: 32.596% (2107/6464)\n",
            "Epoch 48 Step 202/1563 Loss: 2.064 | Acc: 32.620% (2119/6496)\n",
            "Epoch 48 Step 203/1563 Loss: 2.065 | Acc: 32.598% (2128/6528)\n",
            "Epoch 48 Step 204/1563 Loss: 2.065 | Acc: 32.576% (2137/6560)\n",
            "Epoch 48 Step 205/1563 Loss: 2.065 | Acc: 32.600% (2149/6592)\n",
            "Epoch 48 Step 206/1563 Loss: 2.065 | Acc: 32.594% (2159/6624)\n",
            "Epoch 48 Step 207/1563 Loss: 2.064 | Acc: 32.587% (2169/6656)\n",
            "Epoch 48 Step 208/1563 Loss: 2.067 | Acc: 32.551% (2177/6688)\n",
            "Epoch 48 Step 209/1563 Loss: 2.066 | Acc: 32.574% (2189/6720)\n",
            "Epoch 48 Step 210/1563 Loss: 2.067 | Acc: 32.568% (2199/6752)\n",
            "Epoch 48 Step 211/1563 Loss: 2.065 | Acc: 32.591% (2211/6784)\n",
            "Epoch 48 Step 212/1563 Loss: 2.064 | Acc: 32.658% (2226/6816)\n",
            "Epoch 48 Step 213/1563 Loss: 2.065 | Acc: 32.637% (2235/6848)\n",
            "Epoch 48 Step 214/1563 Loss: 2.064 | Acc: 32.631% (2245/6880)\n",
            "Epoch 48 Step 215/1563 Loss: 2.066 | Acc: 32.595% (2253/6912)\n",
            "Epoch 48 Step 216/1563 Loss: 2.065 | Acc: 32.632% (2266/6944)\n",
            "Epoch 48 Step 217/1563 Loss: 2.064 | Acc: 32.612% (2275/6976)\n",
            "Epoch 48 Step 218/1563 Loss: 2.065 | Acc: 32.620% (2286/7008)\n",
            "Epoch 48 Step 219/1563 Loss: 2.065 | Acc: 32.628% (2297/7040)\n",
            "Epoch 48 Step 220/1563 Loss: 2.066 | Acc: 32.579% (2304/7072)\n",
            "Epoch 48 Step 221/1563 Loss: 2.064 | Acc: 32.615% (2317/7104)\n",
            "Epoch 48 Step 222/1563 Loss: 2.064 | Acc: 32.623% (2328/7136)\n",
            "Epoch 48 Step 223/1563 Loss: 2.063 | Acc: 32.645% (2340/7168)\n",
            "Epoch 48 Step 224/1563 Loss: 2.063 | Acc: 32.708% (2355/7200)\n",
            "Epoch 48 Step 225/1563 Loss: 2.062 | Acc: 32.716% (2366/7232)\n",
            "Epoch 48 Step 226/1563 Loss: 2.063 | Acc: 32.695% (2375/7264)\n",
            "Epoch 48 Step 227/1563 Loss: 2.061 | Acc: 32.717% (2387/7296)\n",
            "Epoch 48 Step 228/1563 Loss: 2.063 | Acc: 32.724% (2398/7328)\n",
            "Epoch 48 Step 229/1563 Loss: 2.064 | Acc: 32.717% (2408/7360)\n",
            "Epoch 48 Step 230/1563 Loss: 2.065 | Acc: 32.711% (2418/7392)\n",
            "Epoch 48 Step 231/1563 Loss: 2.066 | Acc: 32.745% (2431/7424)\n",
            "Epoch 48 Step 232/1563 Loss: 2.067 | Acc: 32.739% (2441/7456)\n",
            "Epoch 48 Step 233/1563 Loss: 2.066 | Acc: 32.772% (2454/7488)\n",
            "Epoch 48 Step 234/1563 Loss: 2.065 | Acc: 32.806% (2467/7520)\n",
            "Epoch 48 Step 235/1563 Loss: 2.066 | Acc: 32.826% (2479/7552)\n",
            "Epoch 48 Step 236/1563 Loss: 2.064 | Acc: 32.925% (2497/7584)\n",
            "Epoch 48 Step 237/1563 Loss: 2.064 | Acc: 32.957% (2510/7616)\n",
            "Epoch 48 Step 238/1563 Loss: 2.065 | Acc: 32.924% (2518/7648)\n",
            "Epoch 48 Step 239/1563 Loss: 2.066 | Acc: 32.891% (2526/7680)\n",
            "Epoch 48 Step 240/1563 Loss: 2.067 | Acc: 32.858% (2534/7712)\n",
            "Epoch 48 Step 241/1563 Loss: 2.066 | Acc: 32.903% (2548/7744)\n",
            "Epoch 48 Step 242/1563 Loss: 2.065 | Acc: 32.935% (2561/7776)\n",
            "Epoch 48 Step 243/1563 Loss: 2.065 | Acc: 32.928% (2571/7808)\n",
            "Epoch 48 Step 244/1563 Loss: 2.066 | Acc: 32.908% (2580/7840)\n",
            "Epoch 48 Step 245/1563 Loss: 2.066 | Acc: 32.889% (2589/7872)\n",
            "Epoch 48 Step 246/1563 Loss: 2.066 | Acc: 32.907% (2601/7904)\n",
            "Epoch 48 Step 247/1563 Loss: 2.065 | Acc: 32.964% (2616/7936)\n",
            "Epoch 48 Step 248/1563 Loss: 2.065 | Acc: 32.932% (2624/7968)\n",
            "Epoch 48 Step 249/1563 Loss: 2.064 | Acc: 32.987% (2639/8000)\n",
            "Epoch 48 Step 250/1563 Loss: 2.064 | Acc: 32.956% (2647/8032)\n",
            "Epoch 48 Step 251/1563 Loss: 2.065 | Acc: 32.974% (2659/8064)\n",
            "Epoch 48 Step 252/1563 Loss: 2.067 | Acc: 32.942% (2667/8096)\n",
            "Epoch 48 Step 253/1563 Loss: 2.068 | Acc: 32.960% (2679/8128)\n",
            "Epoch 48 Step 254/1563 Loss: 2.068 | Acc: 32.929% (2687/8160)\n",
            "Epoch 48 Step 255/1563 Loss: 2.066 | Acc: 33.008% (2704/8192)\n",
            "Epoch 48 Step 256/1563 Loss: 2.065 | Acc: 33.062% (2719/8224)\n",
            "Epoch 48 Step 257/1563 Loss: 2.064 | Acc: 33.103% (2733/8256)\n",
            "Epoch 48 Step 258/1563 Loss: 2.064 | Acc: 33.084% (2742/8288)\n",
            "Epoch 48 Step 259/1563 Loss: 2.065 | Acc: 33.065% (2751/8320)\n",
            "Epoch 48 Step 260/1563 Loss: 2.064 | Acc: 33.094% (2764/8352)\n",
            "Epoch 48 Step 261/1563 Loss: 2.064 | Acc: 33.075% (2773/8384)\n",
            "Epoch 48 Step 262/1563 Loss: 2.064 | Acc: 33.080% (2784/8416)\n",
            "Epoch 48 Step 263/1563 Loss: 2.064 | Acc: 33.073% (2794/8448)\n",
            "Epoch 48 Step 264/1563 Loss: 2.065 | Acc: 33.066% (2804/8480)\n",
            "Epoch 48 Step 265/1563 Loss: 2.065 | Acc: 33.071% (2815/8512)\n",
            "Epoch 48 Step 266/1563 Loss: 2.065 | Acc: 33.088% (2827/8544)\n",
            "Epoch 48 Step 267/1563 Loss: 2.065 | Acc: 33.081% (2837/8576)\n",
            "Epoch 48 Step 268/1563 Loss: 2.065 | Acc: 33.086% (2848/8608)\n",
            "Epoch 48 Step 269/1563 Loss: 2.066 | Acc: 33.044% (2855/8640)\n",
            "Epoch 48 Step 270/1563 Loss: 2.067 | Acc: 33.026% (2864/8672)\n",
            "Epoch 48 Step 271/1563 Loss: 2.069 | Acc: 33.008% (2873/8704)\n",
            "Epoch 48 Step 272/1563 Loss: 2.068 | Acc: 33.024% (2885/8736)\n",
            "Epoch 48 Step 273/1563 Loss: 2.068 | Acc: 33.018% (2895/8768)\n",
            "Epoch 48 Step 274/1563 Loss: 2.069 | Acc: 33.011% (2905/8800)\n",
            "Epoch 48 Step 275/1563 Loss: 2.069 | Acc: 32.994% (2914/8832)\n",
            "Epoch 48 Step 276/1563 Loss: 2.069 | Acc: 33.066% (2931/8864)\n",
            "Epoch 48 Step 277/1563 Loss: 2.068 | Acc: 33.116% (2946/8896)\n",
            "Epoch 48 Step 278/1563 Loss: 2.068 | Acc: 33.154% (2960/8928)\n",
            "Epoch 48 Step 279/1563 Loss: 2.067 | Acc: 33.181% (2973/8960)\n",
            "Epoch 48 Step 280/1563 Loss: 2.067 | Acc: 33.163% (2982/8992)\n",
            "Epoch 48 Step 281/1563 Loss: 2.067 | Acc: 33.200% (2996/9024)\n",
            "Epoch 48 Step 282/1563 Loss: 2.065 | Acc: 33.282% (3014/9056)\n",
            "Epoch 48 Step 283/1563 Loss: 2.066 | Acc: 33.242% (3021/9088)\n",
            "Epoch 48 Step 284/1563 Loss: 2.067 | Acc: 33.180% (3026/9120)\n",
            "Epoch 48 Step 285/1563 Loss: 2.067 | Acc: 33.151% (3034/9152)\n",
            "Epoch 48 Step 286/1563 Loss: 2.067 | Acc: 33.155% (3045/9184)\n",
            "Epoch 48 Step 287/1563 Loss: 2.068 | Acc: 33.149% (3055/9216)\n",
            "Epoch 48 Step 288/1563 Loss: 2.068 | Acc: 33.131% (3064/9248)\n",
            "Epoch 48 Step 289/1563 Loss: 2.068 | Acc: 33.157% (3077/9280)\n",
            "Epoch 48 Step 290/1563 Loss: 2.068 | Acc: 33.151% (3087/9312)\n",
            "Epoch 48 Step 291/1563 Loss: 2.069 | Acc: 33.155% (3098/9344)\n",
            "Epoch 48 Step 292/1563 Loss: 2.069 | Acc: 33.170% (3110/9376)\n",
            "Epoch 48 Step 293/1563 Loss: 2.068 | Acc: 33.195% (3123/9408)\n",
            "Epoch 48 Step 294/1563 Loss: 2.067 | Acc: 33.242% (3138/9440)\n",
            "Epoch 48 Step 295/1563 Loss: 2.068 | Acc: 33.203% (3145/9472)\n",
            "Epoch 48 Step 296/1563 Loss: 2.066 | Acc: 33.260% (3161/9504)\n",
            "Epoch 48 Step 297/1563 Loss: 2.067 | Acc: 33.253% (3171/9536)\n",
            "Epoch 48 Step 298/1563 Loss: 2.066 | Acc: 33.278% (3184/9568)\n",
            "Epoch 48 Step 299/1563 Loss: 2.068 | Acc: 33.240% (3191/9600)\n",
            "Epoch 48 Step 300/1563 Loss: 2.067 | Acc: 33.243% (3202/9632)\n",
            "Epoch 48 Step 301/1563 Loss: 2.066 | Acc: 33.257% (3214/9664)\n",
            "Epoch 48 Step 302/1563 Loss: 2.064 | Acc: 33.282% (3227/9696)\n",
            "Epoch 48 Step 303/1563 Loss: 2.064 | Acc: 33.285% (3238/9728)\n",
            "Epoch 48 Step 304/1563 Loss: 2.064 | Acc: 33.279% (3248/9760)\n",
            "Epoch 48 Step 305/1563 Loss: 2.064 | Acc: 33.252% (3256/9792)\n",
            "Epoch 48 Step 306/1563 Loss: 2.065 | Acc: 33.194% (3261/9824)\n",
            "Epoch 48 Step 307/1563 Loss: 2.065 | Acc: 33.168% (3269/9856)\n",
            "Epoch 48 Step 308/1563 Loss: 2.065 | Acc: 33.172% (3280/9888)\n",
            "Epoch 48 Step 309/1563 Loss: 2.064 | Acc: 33.216% (3295/9920)\n",
            "Epoch 48 Step 310/1563 Loss: 2.064 | Acc: 33.179% (3302/9952)\n",
            "Epoch 48 Step 311/1563 Loss: 2.064 | Acc: 33.173% (3312/9984)\n",
            "Epoch 48 Step 312/1563 Loss: 2.064 | Acc: 33.197% (3325/10016)\n",
            "Epoch 48 Step 313/1563 Loss: 2.064 | Acc: 33.221% (3338/10048)\n",
            "Epoch 48 Step 314/1563 Loss: 2.065 | Acc: 33.204% (3347/10080)\n",
            "Epoch 48 Step 315/1563 Loss: 2.064 | Acc: 33.238% (3361/10112)\n",
            "Epoch 48 Step 316/1563 Loss: 2.064 | Acc: 33.222% (3370/10144)\n",
            "Epoch 48 Step 317/1563 Loss: 2.063 | Acc: 33.245% (3383/10176)\n",
            "Epoch 48 Step 318/1563 Loss: 2.061 | Acc: 33.229% (3392/10208)\n",
            "Epoch 48 Step 319/1563 Loss: 2.061 | Acc: 33.242% (3404/10240)\n",
            "Epoch 48 Step 320/1563 Loss: 2.060 | Acc: 33.207% (3411/10272)\n",
            "Epoch 48 Step 321/1563 Loss: 2.061 | Acc: 33.201% (3421/10304)\n",
            "Epoch 48 Step 322/1563 Loss: 2.063 | Acc: 33.175% (3429/10336)\n",
            "Epoch 48 Step 323/1563 Loss: 2.062 | Acc: 33.169% (3439/10368)\n",
            "Epoch 48 Step 324/1563 Loss: 2.061 | Acc: 33.202% (3453/10400)\n",
            "Epoch 48 Step 325/1563 Loss: 2.061 | Acc: 33.215% (3465/10432)\n",
            "Epoch 48 Step 326/1563 Loss: 2.063 | Acc: 33.209% (3475/10464)\n",
            "Epoch 48 Step 327/1563 Loss: 2.062 | Acc: 33.213% (3486/10496)\n",
            "Epoch 48 Step 328/1563 Loss: 2.061 | Acc: 33.197% (3495/10528)\n",
            "Epoch 48 Step 329/1563 Loss: 2.060 | Acc: 33.210% (3507/10560)\n",
            "Epoch 48 Step 330/1563 Loss: 2.060 | Acc: 33.185% (3515/10592)\n",
            "Epoch 48 Step 331/1563 Loss: 2.059 | Acc: 33.198% (3527/10624)\n",
            "Epoch 48 Step 332/1563 Loss: 2.059 | Acc: 33.193% (3537/10656)\n",
            "Epoch 48 Step 333/1563 Loss: 2.058 | Acc: 33.205% (3549/10688)\n",
            "Epoch 48 Step 334/1563 Loss: 2.059 | Acc: 33.209% (3560/10720)\n",
            "Epoch 48 Step 335/1563 Loss: 2.060 | Acc: 33.203% (3570/10752)\n",
            "Epoch 48 Step 336/1563 Loss: 2.060 | Acc: 33.225% (3583/10784)\n",
            "Epoch 48 Step 337/1563 Loss: 2.060 | Acc: 33.192% (3590/10816)\n",
            "Epoch 48 Step 338/1563 Loss: 2.061 | Acc: 33.177% (3599/10848)\n",
            "Epoch 48 Step 339/1563 Loss: 2.061 | Acc: 33.162% (3608/10880)\n",
            "Epoch 48 Step 340/1563 Loss: 2.062 | Acc: 33.120% (3614/10912)\n",
            "Epoch 48 Step 341/1563 Loss: 2.061 | Acc: 33.132% (3626/10944)\n",
            "Epoch 48 Step 342/1563 Loss: 2.061 | Acc: 33.127% (3636/10976)\n",
            "Epoch 48 Step 343/1563 Loss: 2.060 | Acc: 33.130% (3647/11008)\n",
            "Epoch 48 Step 344/1563 Loss: 2.061 | Acc: 33.116% (3656/11040)\n",
            "Epoch 48 Step 345/1563 Loss: 2.060 | Acc: 33.138% (3669/11072)\n",
            "Epoch 48 Step 346/1563 Loss: 2.059 | Acc: 33.132% (3679/11104)\n",
            "Epoch 48 Step 347/1563 Loss: 2.059 | Acc: 33.136% (3690/11136)\n",
            "Epoch 48 Step 348/1563 Loss: 2.059 | Acc: 33.139% (3701/11168)\n",
            "Epoch 48 Step 349/1563 Loss: 2.060 | Acc: 33.134% (3711/11200)\n",
            "Epoch 48 Step 350/1563 Loss: 2.060 | Acc: 33.129% (3721/11232)\n",
            "Epoch 48 Step 351/1563 Loss: 2.059 | Acc: 33.141% (3733/11264)\n",
            "Epoch 48 Step 352/1563 Loss: 2.059 | Acc: 33.109% (3740/11296)\n",
            "Epoch 48 Step 353/1563 Loss: 2.058 | Acc: 33.148% (3755/11328)\n",
            "Epoch 48 Step 354/1563 Loss: 2.059 | Acc: 33.125% (3763/11360)\n",
            "Epoch 48 Step 355/1563 Loss: 2.059 | Acc: 33.067% (3767/11392)\n",
            "Epoch 48 Step 356/1563 Loss: 2.059 | Acc: 33.097% (3781/11424)\n",
            "Epoch 48 Step 357/1563 Loss: 2.059 | Acc: 33.092% (3791/11456)\n",
            "Epoch 48 Step 358/1563 Loss: 2.059 | Acc: 33.113% (3804/11488)\n",
            "Epoch 48 Step 359/1563 Loss: 2.060 | Acc: 33.108% (3814/11520)\n",
            "Epoch 48 Step 360/1563 Loss: 2.061 | Acc: 33.102% (3824/11552)\n",
            "Epoch 48 Step 361/1563 Loss: 2.062 | Acc: 33.123% (3837/11584)\n",
            "Epoch 48 Step 362/1563 Loss: 2.062 | Acc: 33.144% (3850/11616)\n",
            "Epoch 48 Step 363/1563 Loss: 2.062 | Acc: 33.130% (3859/11648)\n",
            "Epoch 48 Step 364/1563 Loss: 2.063 | Acc: 33.116% (3868/11680)\n",
            "Epoch 48 Step 365/1563 Loss: 2.063 | Acc: 33.128% (3880/11712)\n",
            "Epoch 48 Step 366/1563 Loss: 2.064 | Acc: 33.123% (3890/11744)\n",
            "Epoch 48 Step 367/1563 Loss: 2.064 | Acc: 33.101% (3898/11776)\n",
            "Epoch 48 Step 368/1563 Loss: 2.064 | Acc: 33.122% (3911/11808)\n",
            "Epoch 48 Step 369/1563 Loss: 2.064 | Acc: 33.100% (3919/11840)\n",
            "Epoch 48 Step 370/1563 Loss: 2.064 | Acc: 33.086% (3928/11872)\n",
            "Epoch 48 Step 371/1563 Loss: 2.065 | Acc: 33.081% (3938/11904)\n",
            "Epoch 48 Step 372/1563 Loss: 2.064 | Acc: 33.110% (3952/11936)\n",
            "Epoch 48 Step 373/1563 Loss: 2.065 | Acc: 33.063% (3957/11968)\n",
            "Epoch 48 Step 374/1563 Loss: 2.066 | Acc: 33.058% (3967/12000)\n",
            "Epoch 48 Step 375/1563 Loss: 2.066 | Acc: 33.045% (3976/12032)\n",
            "Epoch 48 Step 376/1563 Loss: 2.065 | Acc: 33.040% (3986/12064)\n",
            "Epoch 48 Step 377/1563 Loss: 2.065 | Acc: 33.052% (3998/12096)\n",
            "Epoch 48 Step 378/1563 Loss: 2.066 | Acc: 33.031% (4006/12128)\n",
            "Epoch 48 Step 379/1563 Loss: 2.065 | Acc: 33.043% (4018/12160)\n",
            "Epoch 48 Step 380/1563 Loss: 2.065 | Acc: 33.046% (4029/12192)\n",
            "Epoch 48 Step 381/1563 Loss: 2.064 | Acc: 33.066% (4042/12224)\n",
            "Epoch 48 Step 382/1563 Loss: 2.064 | Acc: 33.045% (4050/12256)\n",
            "Epoch 48 Step 383/1563 Loss: 2.065 | Acc: 33.032% (4059/12288)\n",
            "Epoch 48 Step 384/1563 Loss: 2.066 | Acc: 32.995% (4065/12320)\n",
            "Epoch 48 Step 385/1563 Loss: 2.066 | Acc: 32.966% (4072/12352)\n",
            "Epoch 48 Step 386/1563 Loss: 2.066 | Acc: 32.962% (4082/12384)\n",
            "Epoch 48 Step 387/1563 Loss: 2.066 | Acc: 32.941% (4090/12416)\n",
            "Epoch 48 Step 388/1563 Loss: 2.067 | Acc: 32.929% (4099/12448)\n",
            "Epoch 48 Step 389/1563 Loss: 2.068 | Acc: 32.901% (4106/12480)\n",
            "Epoch 48 Step 390/1563 Loss: 2.067 | Acc: 32.920% (4119/12512)\n",
            "Epoch 48 Step 391/1563 Loss: 2.067 | Acc: 32.908% (4128/12544)\n",
            "Epoch 48 Step 392/1563 Loss: 2.067 | Acc: 32.920% (4140/12576)\n",
            "Epoch 48 Step 393/1563 Loss: 2.066 | Acc: 32.955% (4155/12608)\n",
            "Epoch 48 Step 394/1563 Loss: 2.065 | Acc: 32.943% (4164/12640)\n",
            "Epoch 48 Step 395/1563 Loss: 2.066 | Acc: 32.939% (4174/12672)\n",
            "Epoch 48 Step 396/1563 Loss: 2.066 | Acc: 32.950% (4186/12704)\n",
            "Epoch 48 Step 397/1563 Loss: 2.065 | Acc: 32.962% (4198/12736)\n",
            "Epoch 48 Step 398/1563 Loss: 2.065 | Acc: 32.957% (4208/12768)\n",
            "Epoch 48 Step 399/1563 Loss: 2.064 | Acc: 32.977% (4221/12800)\n",
            "Epoch 48 Step 400/1563 Loss: 2.065 | Acc: 32.996% (4234/12832)\n",
            "Epoch 48 Step 401/1563 Loss: 2.065 | Acc: 32.984% (4243/12864)\n",
            "Epoch 48 Step 402/1563 Loss: 2.065 | Acc: 33.002% (4256/12896)\n",
            "Epoch 48 Step 403/1563 Loss: 2.064 | Acc: 33.006% (4267/12928)\n",
            "Epoch 48 Step 404/1563 Loss: 2.065 | Acc: 32.986% (4275/12960)\n",
            "Epoch 48 Step 405/1563 Loss: 2.066 | Acc: 32.943% (4280/12992)\n",
            "Epoch 48 Step 406/1563 Loss: 2.065 | Acc: 32.947% (4291/13024)\n",
            "Epoch 48 Step 407/1563 Loss: 2.064 | Acc: 32.950% (4302/13056)\n",
            "Epoch 48 Step 408/1563 Loss: 2.065 | Acc: 32.954% (4313/13088)\n",
            "Epoch 48 Step 409/1563 Loss: 2.064 | Acc: 32.965% (4325/13120)\n",
            "Epoch 48 Step 410/1563 Loss: 2.063 | Acc: 32.991% (4339/13152)\n",
            "Epoch 48 Step 411/1563 Loss: 2.064 | Acc: 32.987% (4349/13184)\n",
            "Epoch 48 Step 412/1563 Loss: 2.063 | Acc: 33.013% (4363/13216)\n",
            "Epoch 48 Step 413/1563 Loss: 2.064 | Acc: 32.986% (4370/13248)\n",
            "Epoch 48 Step 414/1563 Loss: 2.064 | Acc: 32.974% (4379/13280)\n",
            "Epoch 48 Step 415/1563 Loss: 2.064 | Acc: 33.000% (4393/13312)\n",
            "Epoch 48 Step 416/1563 Loss: 2.063 | Acc: 33.004% (4404/13344)\n",
            "Epoch 48 Step 417/1563 Loss: 2.062 | Acc: 33.022% (4417/13376)\n",
            "Epoch 48 Step 418/1563 Loss: 2.062 | Acc: 33.033% (4429/13408)\n",
            "Epoch 48 Step 419/1563 Loss: 2.063 | Acc: 32.991% (4434/13440)\n",
            "Epoch 48 Step 420/1563 Loss: 2.064 | Acc: 32.987% (4444/13472)\n",
            "Epoch 48 Step 421/1563 Loss: 2.063 | Acc: 33.012% (4458/13504)\n",
            "Epoch 48 Step 422/1563 Loss: 2.065 | Acc: 33.008% (4468/13536)\n",
            "Epoch 48 Step 423/1563 Loss: 2.064 | Acc: 33.026% (4481/13568)\n",
            "Epoch 48 Step 424/1563 Loss: 2.065 | Acc: 33.037% (4493/13600)\n",
            "Epoch 48 Step 425/1563 Loss: 2.065 | Acc: 33.018% (4501/13632)\n",
            "Epoch 48 Step 426/1563 Loss: 2.066 | Acc: 32.992% (4508/13664)\n",
            "Epoch 48 Step 427/1563 Loss: 2.066 | Acc: 32.988% (4518/13696)\n",
            "Epoch 48 Step 428/1563 Loss: 2.067 | Acc: 32.984% (4528/13728)\n",
            "Epoch 48 Step 429/1563 Loss: 2.067 | Acc: 32.972% (4537/13760)\n",
            "Epoch 48 Step 430/1563 Loss: 2.068 | Acc: 32.976% (4548/13792)\n",
            "Epoch 48 Step 431/1563 Loss: 2.068 | Acc: 32.993% (4561/13824)\n",
            "Epoch 48 Step 432/1563 Loss: 2.069 | Acc: 32.960% (4567/13856)\n",
            "Epoch 48 Step 433/1563 Loss: 2.068 | Acc: 32.993% (4582/13888)\n",
            "Epoch 48 Step 434/1563 Loss: 2.068 | Acc: 33.003% (4594/13920)\n",
            "Epoch 48 Step 435/1563 Loss: 2.068 | Acc: 32.977% (4601/13952)\n",
            "Epoch 48 Step 436/1563 Loss: 2.067 | Acc: 32.995% (4614/13984)\n",
            "Epoch 48 Step 437/1563 Loss: 2.068 | Acc: 32.991% (4624/14016)\n",
            "Epoch 48 Step 438/1563 Loss: 2.068 | Acc: 33.008% (4637/14048)\n",
            "Epoch 48 Step 439/1563 Loss: 2.067 | Acc: 33.018% (4649/14080)\n",
            "Epoch 48 Step 440/1563 Loss: 2.067 | Acc: 33.036% (4662/14112)\n",
            "Epoch 48 Step 441/1563 Loss: 2.067 | Acc: 33.060% (4676/14144)\n",
            "Epoch 48 Step 442/1563 Loss: 2.066 | Acc: 33.091% (4691/14176)\n",
            "Epoch 48 Step 443/1563 Loss: 2.066 | Acc: 33.108% (4704/14208)\n",
            "Epoch 48 Step 444/1563 Loss: 2.065 | Acc: 33.104% (4714/14240)\n",
            "Epoch 48 Step 445/1563 Loss: 2.067 | Acc: 33.086% (4722/14272)\n",
            "Epoch 48 Step 446/1563 Loss: 2.066 | Acc: 33.103% (4735/14304)\n",
            "Epoch 48 Step 447/1563 Loss: 2.067 | Acc: 33.098% (4745/14336)\n",
            "Epoch 48 Step 448/1563 Loss: 2.067 | Acc: 33.080% (4753/14368)\n",
            "Epoch 48 Step 449/1563 Loss: 2.067 | Acc: 33.062% (4761/14400)\n",
            "Epoch 48 Step 450/1563 Loss: 2.068 | Acc: 33.058% (4771/14432)\n",
            "Epoch 48 Step 451/1563 Loss: 2.068 | Acc: 33.061% (4782/14464)\n",
            "Epoch 48 Step 452/1563 Loss: 2.068 | Acc: 33.071% (4794/14496)\n",
            "Epoch 48 Step 453/1563 Loss: 2.067 | Acc: 33.088% (4807/14528)\n",
            "Epoch 48 Step 454/1563 Loss: 2.067 | Acc: 33.111% (4821/14560)\n",
            "Epoch 48 Step 455/1563 Loss: 2.066 | Acc: 33.121% (4833/14592)\n",
            "Epoch 48 Step 456/1563 Loss: 2.065 | Acc: 33.124% (4844/14624)\n",
            "Epoch 48 Step 457/1563 Loss: 2.065 | Acc: 33.113% (4853/14656)\n",
            "Epoch 48 Step 458/1563 Loss: 2.065 | Acc: 33.129% (4866/14688)\n",
            "Epoch 48 Step 459/1563 Loss: 2.065 | Acc: 33.132% (4877/14720)\n",
            "Epoch 48 Step 460/1563 Loss: 2.064 | Acc: 33.162% (4892/14752)\n",
            "Epoch 48 Step 461/1563 Loss: 2.064 | Acc: 33.151% (4901/14784)\n",
            "Epoch 48 Step 462/1563 Loss: 2.065 | Acc: 33.140% (4910/14816)\n",
            "Epoch 48 Step 463/1563 Loss: 2.065 | Acc: 33.136% (4920/14848)\n",
            "Epoch 48 Step 464/1563 Loss: 2.065 | Acc: 33.152% (4933/14880)\n",
            "Epoch 48 Step 465/1563 Loss: 2.065 | Acc: 33.168% (4946/14912)\n",
            "Epoch 48 Step 466/1563 Loss: 2.064 | Acc: 33.171% (4957/14944)\n",
            "Epoch 48 Step 467/1563 Loss: 2.064 | Acc: 33.186% (4970/14976)\n",
            "Epoch 48 Step 468/1563 Loss: 2.064 | Acc: 33.169% (4978/15008)\n",
            "Epoch 48 Step 469/1563 Loss: 2.064 | Acc: 33.165% (4988/15040)\n",
            "Epoch 48 Step 470/1563 Loss: 2.064 | Acc: 33.154% (4997/15072)\n",
            "Epoch 48 Step 471/1563 Loss: 2.064 | Acc: 33.144% (5006/15104)\n",
            "Epoch 48 Step 472/1563 Loss: 2.064 | Acc: 33.146% (5017/15136)\n",
            "Epoch 48 Step 473/1563 Loss: 2.065 | Acc: 33.122% (5024/15168)\n",
            "Epoch 48 Step 474/1563 Loss: 2.066 | Acc: 33.079% (5028/15200)\n",
            "Epoch 48 Step 475/1563 Loss: 2.066 | Acc: 33.062% (5036/15232)\n",
            "Epoch 48 Step 476/1563 Loss: 2.065 | Acc: 33.091% (5051/15264)\n",
            "Epoch 48 Step 477/1563 Loss: 2.065 | Acc: 33.074% (5059/15296)\n",
            "Epoch 48 Step 478/1563 Loss: 2.065 | Acc: 33.090% (5072/15328)\n",
            "Epoch 48 Step 479/1563 Loss: 2.065 | Acc: 33.086% (5082/15360)\n",
            "Epoch 48 Step 480/1563 Loss: 2.065 | Acc: 33.089% (5093/15392)\n",
            "Epoch 48 Step 481/1563 Loss: 2.065 | Acc: 33.111% (5107/15424)\n",
            "Epoch 48 Step 482/1563 Loss: 2.065 | Acc: 33.120% (5119/15456)\n",
            "Epoch 48 Step 483/1563 Loss: 2.066 | Acc: 33.103% (5127/15488)\n",
            "Epoch 48 Step 484/1563 Loss: 2.066 | Acc: 33.099% (5137/15520)\n",
            "Epoch 48 Step 485/1563 Loss: 2.065 | Acc: 33.108% (5149/15552)\n",
            "Epoch 48 Step 486/1563 Loss: 2.065 | Acc: 33.111% (5160/15584)\n",
            "Epoch 48 Step 487/1563 Loss: 2.065 | Acc: 33.113% (5171/15616)\n",
            "Epoch 48 Step 488/1563 Loss: 2.064 | Acc: 33.142% (5186/15648)\n",
            "Epoch 48 Step 489/1563 Loss: 2.064 | Acc: 33.170% (5201/15680)\n",
            "Epoch 48 Step 490/1563 Loss: 2.064 | Acc: 33.153% (5209/15712)\n",
            "Epoch 48 Step 491/1563 Loss: 2.065 | Acc: 33.136% (5217/15744)\n",
            "Epoch 48 Step 492/1563 Loss: 2.065 | Acc: 33.133% (5227/15776)\n",
            "Epoch 48 Step 493/1563 Loss: 2.066 | Acc: 33.122% (5236/15808)\n",
            "Epoch 48 Step 494/1563 Loss: 2.065 | Acc: 33.106% (5244/15840)\n",
            "Epoch 48 Step 495/1563 Loss: 2.066 | Acc: 33.065% (5248/15872)\n",
            "Epoch 48 Step 496/1563 Loss: 2.065 | Acc: 33.086% (5262/15904)\n",
            "Epoch 48 Step 497/1563 Loss: 2.066 | Acc: 33.076% (5271/15936)\n",
            "Epoch 48 Step 498/1563 Loss: 2.067 | Acc: 33.060% (5279/15968)\n",
            "Epoch 48 Step 499/1563 Loss: 2.067 | Acc: 33.044% (5287/16000)\n",
            "Epoch 48 Step 500/1563 Loss: 2.068 | Acc: 33.028% (5295/16032)\n",
            "Epoch 48 Step 501/1563 Loss: 2.067 | Acc: 33.037% (5307/16064)\n",
            "Epoch 48 Step 502/1563 Loss: 2.068 | Acc: 33.014% (5314/16096)\n",
            "Epoch 48 Step 503/1563 Loss: 2.069 | Acc: 33.005% (5323/16128)\n",
            "Epoch 48 Step 504/1563 Loss: 2.069 | Acc: 32.995% (5332/16160)\n",
            "Epoch 48 Step 505/1563 Loss: 2.068 | Acc: 32.992% (5342/16192)\n",
            "Epoch 48 Step 506/1563 Loss: 2.068 | Acc: 32.994% (5353/16224)\n",
            "Epoch 48 Step 507/1563 Loss: 2.068 | Acc: 33.009% (5366/16256)\n",
            "Epoch 48 Step 508/1563 Loss: 2.068 | Acc: 33.018% (5378/16288)\n",
            "Epoch 48 Step 509/1563 Loss: 2.068 | Acc: 33.009% (5387/16320)\n",
            "Epoch 48 Step 510/1563 Loss: 2.069 | Acc: 33.005% (5397/16352)\n",
            "Epoch 48 Step 511/1563 Loss: 2.068 | Acc: 33.026% (5411/16384)\n",
            "Epoch 48 Step 512/1563 Loss: 2.069 | Acc: 33.004% (5418/16416)\n",
            "Epoch 48 Step 513/1563 Loss: 2.070 | Acc: 32.995% (5427/16448)\n",
            "Epoch 48 Step 514/1563 Loss: 2.071 | Acc: 32.961% (5432/16480)\n",
            "Epoch 48 Step 515/1563 Loss: 2.070 | Acc: 32.976% (5445/16512)\n",
            "Epoch 48 Step 516/1563 Loss: 2.070 | Acc: 32.979% (5456/16544)\n",
            "Epoch 48 Step 517/1563 Loss: 2.070 | Acc: 32.981% (5467/16576)\n",
            "Epoch 48 Step 518/1563 Loss: 2.069 | Acc: 32.990% (5479/16608)\n",
            "Epoch 48 Step 519/1563 Loss: 2.069 | Acc: 32.987% (5489/16640)\n",
            "Epoch 48 Step 520/1563 Loss: 2.069 | Acc: 32.989% (5500/16672)\n",
            "Epoch 48 Step 521/1563 Loss: 2.069 | Acc: 33.004% (5513/16704)\n",
            "Epoch 48 Step 522/1563 Loss: 2.068 | Acc: 33.007% (5524/16736)\n",
            "Epoch 48 Step 523/1563 Loss: 2.069 | Acc: 32.997% (5533/16768)\n",
            "Epoch 48 Step 524/1563 Loss: 2.068 | Acc: 32.982% (5541/16800)\n",
            "Epoch 48 Step 525/1563 Loss: 2.068 | Acc: 32.997% (5554/16832)\n",
            "Epoch 48 Step 526/1563 Loss: 2.069 | Acc: 32.981% (5562/16864)\n",
            "Epoch 48 Step 527/1563 Loss: 2.068 | Acc: 32.990% (5574/16896)\n",
            "Epoch 48 Step 528/1563 Loss: 2.069 | Acc: 32.963% (5580/16928)\n",
            "Epoch 48 Step 529/1563 Loss: 2.069 | Acc: 32.966% (5591/16960)\n",
            "Epoch 48 Step 530/1563 Loss: 2.069 | Acc: 32.951% (5599/16992)\n",
            "Epoch 48 Step 531/1563 Loss: 2.069 | Acc: 32.948% (5609/17024)\n",
            "Epoch 48 Step 532/1563 Loss: 2.069 | Acc: 32.933% (5617/17056)\n",
            "Epoch 48 Step 533/1563 Loss: 2.069 | Acc: 32.959% (5632/17088)\n",
            "Epoch 48 Step 534/1563 Loss: 2.068 | Acc: 32.956% (5642/17120)\n",
            "Epoch 48 Step 535/1563 Loss: 2.068 | Acc: 32.952% (5652/17152)\n",
            "Epoch 48 Step 536/1563 Loss: 2.068 | Acc: 32.938% (5660/17184)\n",
            "Epoch 48 Step 537/1563 Loss: 2.068 | Acc: 32.946% (5672/17216)\n",
            "Epoch 48 Step 538/1563 Loss: 2.068 | Acc: 32.960% (5685/17248)\n",
            "Epoch 48 Step 539/1563 Loss: 2.068 | Acc: 32.986% (5700/17280)\n",
            "Epoch 48 Step 540/1563 Loss: 2.067 | Acc: 32.989% (5711/17312)\n",
            "Epoch 48 Step 541/1563 Loss: 2.067 | Acc: 32.980% (5720/17344)\n",
            "Epoch 48 Step 542/1563 Loss: 2.067 | Acc: 32.988% (5732/17376)\n",
            "Epoch 48 Step 543/1563 Loss: 2.067 | Acc: 32.996% (5744/17408)\n",
            "Epoch 48 Step 544/1563 Loss: 2.066 | Acc: 33.022% (5759/17440)\n",
            "Epoch 48 Step 545/1563 Loss: 2.065 | Acc: 33.036% (5772/17472)\n",
            "Epoch 48 Step 546/1563 Loss: 2.066 | Acc: 32.998% (5776/17504)\n",
            "Epoch 48 Step 547/1563 Loss: 2.066 | Acc: 33.006% (5788/17536)\n",
            "Epoch 48 Step 548/1563 Loss: 2.066 | Acc: 33.003% (5798/17568)\n",
            "Epoch 48 Step 549/1563 Loss: 2.066 | Acc: 33.011% (5810/17600)\n",
            "Epoch 48 Step 550/1563 Loss: 2.066 | Acc: 33.025% (5823/17632)\n",
            "Epoch 48 Step 551/1563 Loss: 2.065 | Acc: 33.028% (5834/17664)\n",
            "Epoch 48 Step 552/1563 Loss: 2.065 | Acc: 33.036% (5846/17696)\n",
            "Epoch 48 Step 553/1563 Loss: 2.065 | Acc: 33.032% (5856/17728)\n",
            "Epoch 48 Step 554/1563 Loss: 2.066 | Acc: 33.012% (5863/17760)\n",
            "Epoch 48 Step 555/1563 Loss: 2.066 | Acc: 33.037% (5878/17792)\n",
            "Epoch 48 Step 556/1563 Loss: 2.066 | Acc: 33.051% (5891/17824)\n",
            "Epoch 48 Step 557/1563 Loss: 2.065 | Acc: 33.076% (5906/17856)\n",
            "Epoch 48 Step 558/1563 Loss: 2.064 | Acc: 33.112% (5923/17888)\n",
            "Epoch 48 Step 559/1563 Loss: 2.063 | Acc: 33.136% (5938/17920)\n",
            "Epoch 48 Step 560/1563 Loss: 2.063 | Acc: 33.122% (5946/17952)\n",
            "Epoch 48 Step 561/1563 Loss: 2.062 | Acc: 33.146% (5961/17984)\n",
            "Epoch 48 Step 562/1563 Loss: 2.062 | Acc: 33.154% (5973/18016)\n",
            "Epoch 48 Step 563/1563 Loss: 2.062 | Acc: 33.150% (5983/18048)\n",
            "Epoch 48 Step 564/1563 Loss: 2.062 | Acc: 33.125% (5989/18080)\n",
            "Epoch 48 Step 565/1563 Loss: 2.063 | Acc: 33.122% (5999/18112)\n",
            "Epoch 48 Step 566/1563 Loss: 2.063 | Acc: 33.107% (6007/18144)\n",
            "Epoch 48 Step 567/1563 Loss: 2.063 | Acc: 33.099% (6016/18176)\n",
            "Epoch 48 Step 568/1563 Loss: 2.063 | Acc: 33.095% (6026/18208)\n",
            "Epoch 48 Step 569/1563 Loss: 2.062 | Acc: 33.098% (6037/18240)\n",
            "Epoch 48 Step 570/1563 Loss: 2.062 | Acc: 33.078% (6044/18272)\n",
            "Epoch 48 Step 571/1563 Loss: 2.062 | Acc: 33.080% (6055/18304)\n",
            "Epoch 48 Step 572/1563 Loss: 2.064 | Acc: 33.039% (6058/18336)\n",
            "Epoch 48 Step 573/1563 Loss: 2.065 | Acc: 33.030% (6067/18368)\n",
            "Epoch 48 Step 574/1563 Loss: 2.065 | Acc: 33.038% (6079/18400)\n",
            "Epoch 48 Step 575/1563 Loss: 2.065 | Acc: 33.046% (6091/18432)\n",
            "Epoch 48 Step 576/1563 Loss: 2.065 | Acc: 33.059% (6104/18464)\n",
            "Epoch 48 Step 577/1563 Loss: 2.065 | Acc: 33.050% (6113/18496)\n",
            "Epoch 48 Step 578/1563 Loss: 2.066 | Acc: 33.020% (6118/18528)\n",
            "Epoch 48 Step 579/1563 Loss: 2.067 | Acc: 33.006% (6126/18560)\n",
            "Epoch 48 Step 580/1563 Loss: 2.067 | Acc: 32.998% (6135/18592)\n",
            "Epoch 48 Step 581/1563 Loss: 2.067 | Acc: 32.995% (6145/18624)\n",
            "Epoch 48 Step 582/1563 Loss: 2.067 | Acc: 33.014% (6159/18656)\n",
            "Epoch 48 Step 583/1563 Loss: 2.066 | Acc: 33.027% (6172/18688)\n",
            "Epoch 48 Step 584/1563 Loss: 2.066 | Acc: 33.045% (6186/18720)\n",
            "Epoch 48 Step 585/1563 Loss: 2.066 | Acc: 33.047% (6197/18752)\n",
            "Epoch 48 Step 586/1563 Loss: 2.066 | Acc: 33.049% (6208/18784)\n",
            "Epoch 48 Step 587/1563 Loss: 2.065 | Acc: 33.041% (6217/18816)\n",
            "Epoch 48 Step 588/1563 Loss: 2.065 | Acc: 33.059% (6231/18848)\n",
            "Epoch 48 Step 589/1563 Loss: 2.065 | Acc: 33.051% (6240/18880)\n",
            "Epoch 48 Step 590/1563 Loss: 2.066 | Acc: 33.048% (6250/18912)\n",
            "Epoch 48 Step 591/1563 Loss: 2.067 | Acc: 33.050% (6261/18944)\n",
            "Epoch 48 Step 592/1563 Loss: 2.067 | Acc: 33.052% (6272/18976)\n",
            "Epoch 48 Step 593/1563 Loss: 2.067 | Acc: 33.070% (6286/19008)\n",
            "Epoch 48 Step 594/1563 Loss: 2.066 | Acc: 33.114% (6305/19040)\n",
            "Epoch 48 Step 595/1563 Loss: 2.067 | Acc: 33.106% (6314/19072)\n",
            "Epoch 48 Step 596/1563 Loss: 2.067 | Acc: 33.108% (6325/19104)\n",
            "Epoch 48 Step 597/1563 Loss: 2.067 | Acc: 33.084% (6331/19136)\n",
            "Epoch 48 Step 598/1563 Loss: 2.067 | Acc: 33.102% (6345/19168)\n",
            "Epoch 48 Step 599/1563 Loss: 2.067 | Acc: 33.099% (6355/19200)\n",
            "Epoch 48 Step 600/1563 Loss: 2.067 | Acc: 33.070% (6360/19232)\n",
            "Epoch 48 Step 601/1563 Loss: 2.068 | Acc: 33.056% (6368/19264)\n",
            "Epoch 48 Step 602/1563 Loss: 2.068 | Acc: 33.059% (6379/19296)\n",
            "Epoch 48 Step 603/1563 Loss: 2.067 | Acc: 33.056% (6389/19328)\n",
            "Epoch 48 Step 604/1563 Loss: 2.068 | Acc: 33.048% (6398/19360)\n",
            "Epoch 48 Step 605/1563 Loss: 2.068 | Acc: 33.045% (6408/19392)\n",
            "Epoch 48 Step 606/1563 Loss: 2.068 | Acc: 33.067% (6423/19424)\n",
            "Epoch 48 Step 607/1563 Loss: 2.068 | Acc: 33.064% (6433/19456)\n",
            "Epoch 48 Step 608/1563 Loss: 2.068 | Acc: 33.067% (6444/19488)\n",
            "Epoch 48 Step 609/1563 Loss: 2.068 | Acc: 33.064% (6454/19520)\n",
            "Epoch 48 Step 610/1563 Loss: 2.068 | Acc: 33.071% (6466/19552)\n",
            "Epoch 48 Step 611/1563 Loss: 2.068 | Acc: 33.068% (6476/19584)\n",
            "Epoch 48 Step 612/1563 Loss: 2.067 | Acc: 33.060% (6485/19616)\n",
            "Epoch 48 Step 613/1563 Loss: 2.067 | Acc: 33.062% (6496/19648)\n",
            "Epoch 48 Step 614/1563 Loss: 2.067 | Acc: 33.064% (6507/19680)\n",
            "Epoch 48 Step 615/1563 Loss: 2.066 | Acc: 33.081% (6521/19712)\n",
            "Epoch 48 Step 616/1563 Loss: 2.066 | Acc: 33.089% (6533/19744)\n",
            "Epoch 48 Step 617/1563 Loss: 2.066 | Acc: 33.081% (6542/19776)\n",
            "Epoch 48 Step 618/1563 Loss: 2.067 | Acc: 33.067% (6550/19808)\n",
            "Epoch 48 Step 619/1563 Loss: 2.067 | Acc: 33.080% (6563/19840)\n",
            "Epoch 48 Step 620/1563 Loss: 2.068 | Acc: 33.072% (6572/19872)\n",
            "Epoch 48 Step 621/1563 Loss: 2.068 | Acc: 33.064% (6581/19904)\n",
            "Epoch 48 Step 622/1563 Loss: 2.067 | Acc: 33.076% (6594/19936)\n",
            "Epoch 48 Step 623/1563 Loss: 2.067 | Acc: 33.088% (6607/19968)\n",
            "Epoch 48 Step 624/1563 Loss: 2.067 | Acc: 33.080% (6616/20000)\n",
            "Epoch 48 Step 625/1563 Loss: 2.067 | Acc: 33.082% (6627/20032)\n",
            "Epoch 48 Step 626/1563 Loss: 2.067 | Acc: 33.084% (6638/20064)\n",
            "Epoch 48 Step 627/1563 Loss: 2.068 | Acc: 33.061% (6644/20096)\n",
            "Epoch 48 Step 628/1563 Loss: 2.068 | Acc: 33.093% (6661/20128)\n",
            "Epoch 48 Step 629/1563 Loss: 2.067 | Acc: 33.085% (6670/20160)\n",
            "Epoch 48 Step 630/1563 Loss: 2.068 | Acc: 33.077% (6679/20192)\n",
            "Epoch 48 Step 631/1563 Loss: 2.068 | Acc: 33.065% (6687/20224)\n",
            "Epoch 48 Step 632/1563 Loss: 2.069 | Acc: 33.057% (6696/20256)\n",
            "Epoch 48 Step 633/1563 Loss: 2.069 | Acc: 33.059% (6707/20288)\n",
            "Epoch 48 Step 634/1563 Loss: 2.068 | Acc: 33.061% (6718/20320)\n",
            "Epoch 48 Step 635/1563 Loss: 2.068 | Acc: 33.068% (6730/20352)\n",
            "Epoch 48 Step 636/1563 Loss: 2.068 | Acc: 33.075% (6742/20384)\n",
            "Epoch 48 Step 637/1563 Loss: 2.068 | Acc: 33.092% (6756/20416)\n",
            "Epoch 48 Step 638/1563 Loss: 2.068 | Acc: 33.094% (6767/20448)\n",
            "Epoch 48 Step 639/1563 Loss: 2.068 | Acc: 33.101% (6779/20480)\n",
            "Epoch 48 Step 640/1563 Loss: 2.068 | Acc: 33.103% (6790/20512)\n",
            "Epoch 48 Step 641/1563 Loss: 2.068 | Acc: 33.100% (6800/20544)\n",
            "Epoch 48 Step 642/1563 Loss: 2.067 | Acc: 33.116% (6814/20576)\n",
            "Epoch 48 Step 643/1563 Loss: 2.067 | Acc: 33.128% (6827/20608)\n",
            "Epoch 48 Step 644/1563 Loss: 2.067 | Acc: 33.120% (6836/20640)\n",
            "Epoch 48 Step 645/1563 Loss: 2.067 | Acc: 33.117% (6846/20672)\n",
            "Epoch 48 Step 646/1563 Loss: 2.068 | Acc: 33.095% (6852/20704)\n",
            "Epoch 48 Step 647/1563 Loss: 2.068 | Acc: 33.068% (6857/20736)\n",
            "Epoch 48 Step 648/1563 Loss: 2.068 | Acc: 33.070% (6868/20768)\n",
            "Epoch 48 Step 649/1563 Loss: 2.068 | Acc: 33.087% (6882/20800)\n",
            "Epoch 48 Step 650/1563 Loss: 2.067 | Acc: 33.098% (6895/20832)\n",
            "Epoch 48 Step 651/1563 Loss: 2.067 | Acc: 33.100% (6906/20864)\n",
            "Epoch 48 Step 652/1563 Loss: 2.067 | Acc: 33.097% (6916/20896)\n",
            "Epoch 48 Step 653/1563 Loss: 2.068 | Acc: 33.066% (6920/20928)\n",
            "Epoch 48 Step 654/1563 Loss: 2.068 | Acc: 33.039% (6925/20960)\n",
            "Epoch 48 Step 655/1563 Loss: 2.068 | Acc: 33.032% (6934/20992)\n",
            "Epoch 48 Step 656/1563 Loss: 2.068 | Acc: 33.034% (6945/21024)\n",
            "Epoch 48 Step 657/1563 Loss: 2.068 | Acc: 33.012% (6951/21056)\n",
            "Epoch 48 Step 658/1563 Loss: 2.068 | Acc: 32.990% (6957/21088)\n",
            "Epoch 48 Step 659/1563 Loss: 2.068 | Acc: 32.992% (6968/21120)\n",
            "Epoch 48 Step 660/1563 Loss: 2.068 | Acc: 32.990% (6978/21152)\n",
            "Epoch 48 Step 661/1563 Loss: 2.068 | Acc: 32.982% (6987/21184)\n",
            "Epoch 48 Step 662/1563 Loss: 2.068 | Acc: 32.994% (7000/21216)\n",
            "Epoch 48 Step 663/1563 Loss: 2.068 | Acc: 32.987% (7009/21248)\n",
            "Epoch 48 Step 664/1563 Loss: 2.068 | Acc: 33.017% (7026/21280)\n",
            "Epoch 48 Step 665/1563 Loss: 2.068 | Acc: 33.024% (7038/21312)\n",
            "Epoch 48 Step 666/1563 Loss: 2.068 | Acc: 33.026% (7049/21344)\n",
            "Epoch 48 Step 667/1563 Loss: 2.068 | Acc: 33.018% (7058/21376)\n",
            "Epoch 48 Step 668/1563 Loss: 2.069 | Acc: 33.011% (7067/21408)\n",
            "Epoch 48 Step 669/1563 Loss: 2.068 | Acc: 33.013% (7078/21440)\n",
            "Epoch 48 Step 670/1563 Loss: 2.068 | Acc: 33.006% (7087/21472)\n",
            "Epoch 48 Step 671/1563 Loss: 2.068 | Acc: 32.989% (7094/21504)\n",
            "Epoch 48 Step 672/1563 Loss: 2.069 | Acc: 32.977% (7102/21536)\n",
            "Epoch 48 Step 673/1563 Loss: 2.069 | Acc: 32.970% (7111/21568)\n",
            "Epoch 48 Step 674/1563 Loss: 2.069 | Acc: 32.972% (7122/21600)\n",
            "Epoch 48 Step 675/1563 Loss: 2.070 | Acc: 32.970% (7132/21632)\n",
            "Epoch 48 Step 676/1563 Loss: 2.071 | Acc: 32.949% (7138/21664)\n",
            "Epoch 48 Step 677/1563 Loss: 2.071 | Acc: 32.951% (7149/21696)\n",
            "Epoch 48 Step 678/1563 Loss: 2.071 | Acc: 32.962% (7162/21728)\n",
            "Epoch 48 Step 679/1563 Loss: 2.071 | Acc: 32.946% (7169/21760)\n",
            "Epoch 48 Step 680/1563 Loss: 2.070 | Acc: 32.948% (7180/21792)\n",
            "Epoch 48 Step 681/1563 Loss: 2.070 | Acc: 32.932% (7187/21824)\n",
            "Epoch 48 Step 682/1563 Loss: 2.070 | Acc: 32.929% (7197/21856)\n",
            "Epoch 48 Step 683/1563 Loss: 2.070 | Acc: 32.945% (7211/21888)\n",
            "Epoch 48 Step 684/1563 Loss: 2.070 | Acc: 32.952% (7223/21920)\n",
            "Epoch 48 Step 685/1563 Loss: 2.070 | Acc: 32.945% (7232/21952)\n",
            "Epoch 48 Step 686/1563 Loss: 2.069 | Acc: 32.960% (7246/21984)\n",
            "Epoch 48 Step 687/1563 Loss: 2.070 | Acc: 32.949% (7254/22016)\n",
            "Epoch 48 Step 688/1563 Loss: 2.069 | Acc: 32.951% (7265/22048)\n",
            "Epoch 48 Step 689/1563 Loss: 2.069 | Acc: 32.953% (7276/22080)\n",
            "Epoch 48 Step 690/1563 Loss: 2.070 | Acc: 32.937% (7283/22112)\n",
            "Epoch 48 Step 691/1563 Loss: 2.070 | Acc: 32.930% (7292/22144)\n",
            "Epoch 48 Step 692/1563 Loss: 2.070 | Acc: 32.937% (7304/22176)\n",
            "Epoch 48 Step 693/1563 Loss: 2.069 | Acc: 32.943% (7316/22208)\n",
            "Epoch 48 Step 694/1563 Loss: 2.070 | Acc: 32.918% (7321/22240)\n",
            "Epoch 48 Step 695/1563 Loss: 2.070 | Acc: 32.934% (7335/22272)\n",
            "Epoch 48 Step 696/1563 Loss: 2.070 | Acc: 32.949% (7349/22304)\n",
            "Epoch 48 Step 697/1563 Loss: 2.070 | Acc: 32.942% (7358/22336)\n",
            "Epoch 48 Step 698/1563 Loss: 2.070 | Acc: 32.958% (7372/22368)\n",
            "Epoch 48 Step 699/1563 Loss: 2.070 | Acc: 32.946% (7380/22400)\n",
            "Epoch 48 Step 700/1563 Loss: 2.070 | Acc: 32.971% (7396/22432)\n",
            "Epoch 48 Step 701/1563 Loss: 2.071 | Acc: 32.937% (7399/22464)\n",
            "Epoch 48 Step 702/1563 Loss: 2.071 | Acc: 32.935% (7409/22496)\n",
            "Epoch 48 Step 703/1563 Loss: 2.071 | Acc: 32.946% (7422/22528)\n",
            "Epoch 48 Step 704/1563 Loss: 2.071 | Acc: 32.948% (7433/22560)\n",
            "Epoch 48 Step 705/1563 Loss: 2.071 | Acc: 32.941% (7442/22592)\n",
            "Epoch 48 Step 706/1563 Loss: 2.071 | Acc: 32.930% (7450/22624)\n",
            "Epoch 48 Step 707/1563 Loss: 2.071 | Acc: 32.914% (7457/22656)\n",
            "Epoch 48 Step 708/1563 Loss: 2.071 | Acc: 32.925% (7470/22688)\n",
            "Epoch 48 Step 709/1563 Loss: 2.072 | Acc: 32.931% (7482/22720)\n",
            "Epoch 48 Step 710/1563 Loss: 2.072 | Acc: 32.938% (7494/22752)\n",
            "Epoch 48 Step 711/1563 Loss: 2.072 | Acc: 32.940% (7505/22784)\n",
            "Epoch 48 Step 712/1563 Loss: 2.072 | Acc: 32.942% (7516/22816)\n",
            "Epoch 48 Step 713/1563 Loss: 2.072 | Acc: 32.944% (7527/22848)\n",
            "Epoch 48 Step 714/1563 Loss: 2.072 | Acc: 32.946% (7538/22880)\n",
            "Epoch 48 Step 715/1563 Loss: 2.072 | Acc: 32.935% (7546/22912)\n",
            "Epoch 48 Step 716/1563 Loss: 2.072 | Acc: 32.919% (7553/22944)\n",
            "Epoch 48 Step 717/1563 Loss: 2.073 | Acc: 32.908% (7561/22976)\n",
            "Epoch 48 Step 718/1563 Loss: 2.073 | Acc: 32.910% (7572/23008)\n",
            "Epoch 48 Step 719/1563 Loss: 2.074 | Acc: 32.878% (7575/23040)\n",
            "Epoch 48 Step 720/1563 Loss: 2.074 | Acc: 32.880% (7586/23072)\n",
            "Epoch 48 Step 721/1563 Loss: 2.073 | Acc: 32.895% (7600/23104)\n",
            "Epoch 48 Step 722/1563 Loss: 2.073 | Acc: 32.875% (7606/23136)\n",
            "Epoch 48 Step 723/1563 Loss: 2.074 | Acc: 32.873% (7616/23168)\n",
            "Epoch 48 Step 724/1563 Loss: 2.073 | Acc: 32.884% (7629/23200)\n",
            "Epoch 48 Step 725/1563 Loss: 2.073 | Acc: 32.873% (7637/23232)\n",
            "Epoch 48 Step 726/1563 Loss: 2.074 | Acc: 32.866% (7646/23264)\n",
            "Epoch 48 Step 727/1563 Loss: 2.074 | Acc: 32.873% (7658/23296)\n",
            "Epoch 48 Step 728/1563 Loss: 2.073 | Acc: 32.883% (7671/23328)\n",
            "Epoch 48 Step 729/1563 Loss: 2.074 | Acc: 32.864% (7677/23360)\n",
            "Epoch 48 Step 730/1563 Loss: 2.073 | Acc: 32.879% (7691/23392)\n",
            "Epoch 48 Step 731/1563 Loss: 2.073 | Acc: 32.872% (7700/23424)\n",
            "Epoch 48 Step 732/1563 Loss: 2.073 | Acc: 32.883% (7713/23456)\n",
            "Epoch 48 Step 733/1563 Loss: 2.073 | Acc: 32.876% (7722/23488)\n",
            "Epoch 48 Step 734/1563 Loss: 2.073 | Acc: 32.878% (7733/23520)\n",
            "Epoch 48 Step 735/1563 Loss: 2.072 | Acc: 32.893% (7747/23552)\n",
            "Epoch 48 Step 736/1563 Loss: 2.073 | Acc: 32.891% (7757/23584)\n",
            "Epoch 48 Step 737/1563 Loss: 2.073 | Acc: 32.884% (7766/23616)\n",
            "Epoch 48 Step 738/1563 Loss: 2.073 | Acc: 32.887% (7777/23648)\n",
            "Epoch 48 Step 739/1563 Loss: 2.072 | Acc: 32.893% (7789/23680)\n",
            "Epoch 48 Step 740/1563 Loss: 2.072 | Acc: 32.907% (7803/23712)\n",
            "Epoch 48 Step 741/1563 Loss: 2.073 | Acc: 32.901% (7812/23744)\n",
            "Epoch 48 Step 742/1563 Loss: 2.073 | Acc: 32.895% (7821/23776)\n",
            "Epoch 48 Step 743/1563 Loss: 2.072 | Acc: 32.901% (7833/23808)\n",
            "Epoch 48 Step 744/1563 Loss: 2.072 | Acc: 32.911% (7846/23840)\n",
            "Epoch 48 Step 745/1563 Loss: 2.072 | Acc: 32.892% (7852/23872)\n",
            "Epoch 48 Step 746/1563 Loss: 2.072 | Acc: 32.898% (7864/23904)\n",
            "Epoch 48 Step 747/1563 Loss: 2.072 | Acc: 32.884% (7871/23936)\n",
            "Epoch 48 Step 748/1563 Loss: 2.072 | Acc: 32.886% (7882/23968)\n",
            "Epoch 48 Step 749/1563 Loss: 2.072 | Acc: 32.867% (7888/24000)\n",
            "Epoch 48 Step 750/1563 Loss: 2.072 | Acc: 32.869% (7899/24032)\n",
            "Epoch 48 Step 751/1563 Loss: 2.072 | Acc: 32.875% (7911/24064)\n",
            "Epoch 48 Step 752/1563 Loss: 2.073 | Acc: 32.864% (7919/24096)\n",
            "Epoch 48 Step 753/1563 Loss: 2.072 | Acc: 32.879% (7933/24128)\n",
            "Epoch 48 Step 754/1563 Loss: 2.073 | Acc: 32.864% (7940/24160)\n",
            "Epoch 48 Step 755/1563 Loss: 2.073 | Acc: 32.883% (7955/24192)\n",
            "Epoch 48 Step 756/1563 Loss: 2.072 | Acc: 32.905% (7971/24224)\n",
            "Epoch 48 Step 757/1563 Loss: 2.072 | Acc: 32.911% (7983/24256)\n",
            "Epoch 48 Step 758/1563 Loss: 2.072 | Acc: 32.913% (7994/24288)\n",
            "Epoch 48 Step 759/1563 Loss: 2.072 | Acc: 32.903% (8002/24320)\n",
            "Epoch 48 Step 760/1563 Loss: 2.073 | Acc: 32.888% (8009/24352)\n",
            "Epoch 48 Step 761/1563 Loss: 2.072 | Acc: 32.903% (8023/24384)\n",
            "Epoch 48 Step 762/1563 Loss: 2.072 | Acc: 32.917% (8037/24416)\n",
            "Epoch 48 Step 763/1563 Loss: 2.072 | Acc: 32.907% (8045/24448)\n",
            "Epoch 48 Step 764/1563 Loss: 2.073 | Acc: 32.908% (8056/24480)\n",
            "Epoch 48 Step 765/1563 Loss: 2.073 | Acc: 32.898% (8064/24512)\n",
            "Epoch 48 Step 766/1563 Loss: 2.073 | Acc: 32.876% (8069/24544)\n",
            "Epoch 48 Step 767/1563 Loss: 2.074 | Acc: 32.861% (8076/24576)\n",
            "Epoch 48 Step 768/1563 Loss: 2.074 | Acc: 32.863% (8087/24608)\n",
            "Epoch 48 Step 769/1563 Loss: 2.073 | Acc: 32.861% (8097/24640)\n",
            "Epoch 48 Step 770/1563 Loss: 2.073 | Acc: 32.867% (8109/24672)\n",
            "Epoch 48 Step 771/1563 Loss: 2.073 | Acc: 32.869% (8120/24704)\n",
            "Epoch 48 Step 772/1563 Loss: 2.073 | Acc: 32.871% (8131/24736)\n",
            "Epoch 48 Step 773/1563 Loss: 2.073 | Acc: 32.873% (8142/24768)\n",
            "Epoch 48 Step 774/1563 Loss: 2.074 | Acc: 32.867% (8151/24800)\n",
            "Epoch 48 Step 775/1563 Loss: 2.074 | Acc: 32.865% (8161/24832)\n",
            "Epoch 48 Step 776/1563 Loss: 2.073 | Acc: 32.855% (8169/24864)\n",
            "Epoch 48 Step 777/1563 Loss: 2.074 | Acc: 32.849% (8178/24896)\n",
            "Epoch 48 Step 778/1563 Loss: 2.074 | Acc: 32.859% (8191/24928)\n",
            "Epoch 48 Step 779/1563 Loss: 2.073 | Acc: 32.877% (8206/24960)\n",
            "Epoch 48 Step 780/1563 Loss: 2.074 | Acc: 32.875% (8216/24992)\n",
            "Epoch 48 Step 781/1563 Loss: 2.074 | Acc: 32.876% (8227/25024)\n",
            "Epoch 48 Step 782/1563 Loss: 2.073 | Acc: 32.890% (8241/25056)\n",
            "Epoch 48 Step 783/1563 Loss: 2.073 | Acc: 32.892% (8252/25088)\n",
            "Epoch 48 Step 784/1563 Loss: 2.073 | Acc: 32.898% (8264/25120)\n",
            "Epoch 48 Step 785/1563 Loss: 2.073 | Acc: 32.900% (8275/25152)\n",
            "Epoch 48 Step 786/1563 Loss: 2.073 | Acc: 32.886% (8282/25184)\n",
            "Epoch 48 Step 787/1563 Loss: 2.074 | Acc: 32.884% (8292/25216)\n",
            "Epoch 48 Step 788/1563 Loss: 2.073 | Acc: 32.898% (8306/25248)\n",
            "Epoch 48 Step 789/1563 Loss: 2.073 | Acc: 32.915% (8321/25280)\n",
            "Epoch 48 Step 790/1563 Loss: 2.073 | Acc: 32.890% (8325/25312)\n",
            "Epoch 48 Step 791/1563 Loss: 2.073 | Acc: 32.903% (8339/25344)\n",
            "Epoch 48 Step 792/1563 Loss: 2.073 | Acc: 32.889% (8346/25376)\n",
            "Epoch 48 Step 793/1563 Loss: 2.073 | Acc: 32.895% (8358/25408)\n",
            "Epoch 48 Step 794/1563 Loss: 2.073 | Acc: 32.897% (8369/25440)\n",
            "Epoch 48 Step 795/1563 Loss: 2.072 | Acc: 32.911% (8383/25472)\n",
            "Epoch 48 Step 796/1563 Loss: 2.072 | Acc: 32.905% (8392/25504)\n",
            "Epoch 48 Step 797/1563 Loss: 2.072 | Acc: 32.899% (8401/25536)\n",
            "Epoch 48 Step 798/1563 Loss: 2.073 | Acc: 32.904% (8413/25568)\n",
            "Epoch 48 Step 799/1563 Loss: 2.072 | Acc: 32.918% (8427/25600)\n",
            "Epoch 48 Step 800/1563 Loss: 2.073 | Acc: 32.912% (8436/25632)\n",
            "Epoch 48 Step 801/1563 Loss: 2.073 | Acc: 32.898% (8443/25664)\n",
            "Epoch 48 Step 802/1563 Loss: 2.073 | Acc: 32.900% (8454/25696)\n",
            "Epoch 48 Step 803/1563 Loss: 2.072 | Acc: 32.902% (8465/25728)\n",
            "Epoch 48 Step 804/1563 Loss: 2.072 | Acc: 32.904% (8476/25760)\n",
            "Epoch 48 Step 805/1563 Loss: 2.072 | Acc: 32.898% (8485/25792)\n",
            "Epoch 48 Step 806/1563 Loss: 2.072 | Acc: 32.896% (8495/25824)\n",
            "Epoch 48 Step 807/1563 Loss: 2.072 | Acc: 32.898% (8506/25856)\n",
            "Epoch 48 Step 808/1563 Loss: 2.072 | Acc: 32.892% (8515/25888)\n",
            "Epoch 48 Step 809/1563 Loss: 2.072 | Acc: 32.894% (8526/25920)\n",
            "Epoch 48 Step 810/1563 Loss: 2.072 | Acc: 32.895% (8537/25952)\n",
            "Epoch 48 Step 811/1563 Loss: 2.072 | Acc: 32.893% (8547/25984)\n",
            "Epoch 48 Step 812/1563 Loss: 2.072 | Acc: 32.911% (8562/26016)\n",
            "Epoch 48 Step 813/1563 Loss: 2.072 | Acc: 32.905% (8571/26048)\n",
            "Epoch 48 Step 814/1563 Loss: 2.072 | Acc: 32.895% (8579/26080)\n",
            "Epoch 48 Step 815/1563 Loss: 2.072 | Acc: 32.889% (8588/26112)\n",
            "Epoch 48 Step 816/1563 Loss: 2.072 | Acc: 32.883% (8597/26144)\n",
            "Epoch 48 Step 817/1563 Loss: 2.072 | Acc: 32.877% (8606/26176)\n",
            "Epoch 48 Step 818/1563 Loss: 2.072 | Acc: 32.875% (8616/26208)\n",
            "Epoch 48 Step 819/1563 Loss: 2.072 | Acc: 32.858% (8622/26240)\n",
            "Epoch 48 Step 820/1563 Loss: 2.072 | Acc: 32.864% (8634/26272)\n",
            "Epoch 48 Step 821/1563 Loss: 2.073 | Acc: 32.858% (8643/26304)\n",
            "Epoch 48 Step 822/1563 Loss: 2.073 | Acc: 32.852% (8652/26336)\n",
            "Epoch 48 Step 823/1563 Loss: 2.073 | Acc: 32.854% (8663/26368)\n",
            "Epoch 48 Step 824/1563 Loss: 2.073 | Acc: 32.841% (8670/26400)\n",
            "Epoch 48 Step 825/1563 Loss: 2.073 | Acc: 32.843% (8681/26432)\n",
            "Epoch 48 Step 826/1563 Loss: 2.072 | Acc: 32.845% (8692/26464)\n",
            "Epoch 48 Step 827/1563 Loss: 2.072 | Acc: 32.846% (8703/26496)\n",
            "Epoch 48 Step 828/1563 Loss: 2.072 | Acc: 32.845% (8713/26528)\n",
            "Epoch 48 Step 829/1563 Loss: 2.072 | Acc: 32.854% (8726/26560)\n",
            "Epoch 48 Step 830/1563 Loss: 2.072 | Acc: 32.848% (8735/26592)\n",
            "Epoch 48 Step 831/1563 Loss: 2.072 | Acc: 32.850% (8746/26624)\n",
            "Epoch 48 Step 832/1563 Loss: 2.073 | Acc: 32.844% (8755/26656)\n",
            "Epoch 48 Step 833/1563 Loss: 2.073 | Acc: 32.842% (8765/26688)\n",
            "Epoch 48 Step 834/1563 Loss: 2.073 | Acc: 32.848% (8777/26720)\n",
            "Epoch 48 Step 835/1563 Loss: 2.073 | Acc: 32.835% (8784/26752)\n",
            "Epoch 48 Step 836/1563 Loss: 2.073 | Acc: 32.826% (8792/26784)\n",
            "Epoch 48 Step 837/1563 Loss: 2.073 | Acc: 32.820% (8801/26816)\n",
            "Epoch 48 Step 838/1563 Loss: 2.073 | Acc: 32.811% (8809/26848)\n",
            "Epoch 48 Step 839/1563 Loss: 2.073 | Acc: 32.820% (8822/26880)\n",
            "Epoch 48 Step 840/1563 Loss: 2.073 | Acc: 32.826% (8834/26912)\n",
            "Epoch 48 Step 841/1563 Loss: 2.073 | Acc: 32.816% (8842/26944)\n",
            "Epoch 48 Step 842/1563 Loss: 2.073 | Acc: 32.814% (8852/26976)\n",
            "Epoch 48 Step 843/1563 Loss: 2.072 | Acc: 32.831% (8867/27008)\n",
            "Epoch 48 Step 844/1563 Loss: 2.072 | Acc: 32.814% (8873/27040)\n",
            "Epoch 48 Step 845/1563 Loss: 2.073 | Acc: 32.812% (8883/27072)\n",
            "Epoch 48 Step 846/1563 Loss: 2.073 | Acc: 32.800% (8890/27104)\n",
            "Epoch 48 Step 847/1563 Loss: 2.073 | Acc: 32.794% (8899/27136)\n",
            "Epoch 48 Step 848/1563 Loss: 2.073 | Acc: 32.811% (8914/27168)\n",
            "Epoch 48 Step 849/1563 Loss: 2.072 | Acc: 32.805% (8923/27200)\n",
            "Epoch 48 Step 850/1563 Loss: 2.072 | Acc: 32.811% (8935/27232)\n",
            "Epoch 48 Step 851/1563 Loss: 2.072 | Acc: 32.805% (8944/27264)\n",
            "Epoch 48 Step 852/1563 Loss: 2.072 | Acc: 32.803% (8954/27296)\n",
            "Epoch 48 Step 853/1563 Loss: 2.072 | Acc: 32.802% (8964/27328)\n",
            "Epoch 48 Step 854/1563 Loss: 2.073 | Acc: 32.792% (8972/27360)\n",
            "Epoch 48 Step 855/1563 Loss: 2.073 | Acc: 32.802% (8985/27392)\n",
            "Epoch 48 Step 856/1563 Loss: 2.073 | Acc: 32.796% (8994/27424)\n",
            "Epoch 48 Step 857/1563 Loss: 2.073 | Acc: 32.794% (9004/27456)\n",
            "Epoch 48 Step 858/1563 Loss: 2.074 | Acc: 32.782% (9011/27488)\n",
            "Epoch 48 Step 859/1563 Loss: 2.074 | Acc: 32.783% (9022/27520)\n",
            "Epoch 48 Step 860/1563 Loss: 2.075 | Acc: 32.771% (9029/27552)\n",
            "Epoch 48 Step 861/1563 Loss: 2.075 | Acc: 32.773% (9040/27584)\n",
            "Epoch 48 Step 862/1563 Loss: 2.075 | Acc: 32.771% (9050/27616)\n",
            "Epoch 48 Step 863/1563 Loss: 2.075 | Acc: 32.776% (9062/27648)\n",
            "Epoch 48 Step 864/1563 Loss: 2.076 | Acc: 32.782% (9074/27680)\n",
            "Epoch 48 Step 865/1563 Loss: 2.076 | Acc: 32.776% (9083/27712)\n",
            "Epoch 48 Step 866/1563 Loss: 2.076 | Acc: 32.767% (9091/27744)\n",
            "Epoch 48 Step 867/1563 Loss: 2.076 | Acc: 32.773% (9103/27776)\n",
            "Epoch 48 Step 868/1563 Loss: 2.077 | Acc: 32.764% (9111/27808)\n",
            "Epoch 48 Step 869/1563 Loss: 2.076 | Acc: 32.769% (9123/27840)\n",
            "Epoch 48 Step 870/1563 Loss: 2.077 | Acc: 32.775% (9135/27872)\n",
            "Epoch 48 Step 871/1563 Loss: 2.076 | Acc: 32.787% (9149/27904)\n",
            "Epoch 48 Step 872/1563 Loss: 2.077 | Acc: 32.778% (9157/27936)\n",
            "Epoch 48 Step 873/1563 Loss: 2.077 | Acc: 32.766% (9164/27968)\n",
            "Epoch 48 Step 874/1563 Loss: 2.077 | Acc: 32.782% (9179/28000)\n",
            "Epoch 48 Step 875/1563 Loss: 2.077 | Acc: 32.784% (9190/28032)\n",
            "Epoch 48 Step 876/1563 Loss: 2.078 | Acc: 32.786% (9201/28064)\n",
            "Epoch 48 Step 877/1563 Loss: 2.077 | Acc: 32.784% (9211/28096)\n",
            "Epoch 48 Step 878/1563 Loss: 2.078 | Acc: 32.765% (9216/28128)\n",
            "Epoch 48 Step 879/1563 Loss: 2.078 | Acc: 32.766% (9227/28160)\n",
            "Epoch 48 Step 880/1563 Loss: 2.078 | Acc: 32.754% (9234/28192)\n",
            "Epoch 48 Step 881/1563 Loss: 2.078 | Acc: 32.756% (9245/28224)\n",
            "Epoch 48 Step 882/1563 Loss: 2.079 | Acc: 32.747% (9253/28256)\n",
            "Epoch 48 Step 883/1563 Loss: 2.079 | Acc: 32.745% (9263/28288)\n",
            "Epoch 48 Step 884/1563 Loss: 2.079 | Acc: 32.744% (9273/28320)\n",
            "Epoch 48 Step 885/1563 Loss: 2.079 | Acc: 32.753% (9286/28352)\n",
            "Epoch 48 Step 886/1563 Loss: 2.080 | Acc: 32.747% (9295/28384)\n",
            "Epoch 48 Step 887/1563 Loss: 2.080 | Acc: 32.756% (9308/28416)\n",
            "Epoch 48 Step 888/1563 Loss: 2.080 | Acc: 32.747% (9316/28448)\n",
            "Epoch 48 Step 889/1563 Loss: 2.079 | Acc: 32.753% (9328/28480)\n",
            "Epoch 48 Step 890/1563 Loss: 2.078 | Acc: 32.779% (9346/28512)\n",
            "Epoch 48 Step 891/1563 Loss: 2.078 | Acc: 32.802% (9363/28544)\n",
            "Epoch 48 Step 892/1563 Loss: 2.078 | Acc: 32.797% (9372/28576)\n",
            "Epoch 48 Step 893/1563 Loss: 2.078 | Acc: 32.809% (9386/28608)\n",
            "Epoch 48 Step 894/1563 Loss: 2.078 | Acc: 32.793% (9392/28640)\n",
            "Epoch 48 Step 895/1563 Loss: 2.078 | Acc: 32.778% (9398/28672)\n",
            "Epoch 48 Step 896/1563 Loss: 2.078 | Acc: 32.776% (9408/28704)\n",
            "Epoch 48 Step 897/1563 Loss: 2.078 | Acc: 32.799% (9425/28736)\n",
            "Epoch 48 Step 898/1563 Loss: 2.078 | Acc: 32.790% (9433/28768)\n",
            "Epoch 48 Step 899/1563 Loss: 2.078 | Acc: 32.795% (9445/28800)\n",
            "Epoch 48 Step 900/1563 Loss: 2.078 | Acc: 32.790% (9454/28832)\n",
            "Epoch 48 Step 901/1563 Loss: 2.079 | Acc: 32.788% (9464/28864)\n",
            "Epoch 48 Step 902/1563 Loss: 2.079 | Acc: 32.793% (9476/28896)\n",
            "Epoch 48 Step 903/1563 Loss: 2.079 | Acc: 32.799% (9488/28928)\n",
            "Epoch 48 Step 904/1563 Loss: 2.079 | Acc: 32.807% (9501/28960)\n",
            "Epoch 48 Step 905/1563 Loss: 2.079 | Acc: 32.802% (9510/28992)\n",
            "Epoch 48 Step 906/1563 Loss: 2.079 | Acc: 32.814% (9524/29024)\n",
            "Epoch 48 Step 907/1563 Loss: 2.079 | Acc: 32.819% (9536/29056)\n",
            "Epoch 48 Step 908/1563 Loss: 2.079 | Acc: 32.821% (9547/29088)\n",
            "Epoch 48 Step 909/1563 Loss: 2.079 | Acc: 32.816% (9556/29120)\n",
            "Epoch 48 Step 910/1563 Loss: 2.079 | Acc: 32.821% (9568/29152)\n",
            "Epoch 48 Step 911/1563 Loss: 2.079 | Acc: 32.826% (9580/29184)\n",
            "Epoch 48 Step 912/1563 Loss: 2.079 | Acc: 32.824% (9590/29216)\n",
            "Epoch 48 Step 913/1563 Loss: 2.079 | Acc: 32.826% (9601/29248)\n",
            "Epoch 48 Step 914/1563 Loss: 2.079 | Acc: 32.814% (9608/29280)\n",
            "Epoch 48 Step 915/1563 Loss: 2.079 | Acc: 32.826% (9622/29312)\n",
            "Epoch 48 Step 916/1563 Loss: 2.079 | Acc: 32.828% (9633/29344)\n",
            "Epoch 48 Step 917/1563 Loss: 2.079 | Acc: 32.836% (9646/29376)\n",
            "Epoch 48 Step 918/1563 Loss: 2.079 | Acc: 32.835% (9656/29408)\n",
            "Epoch 48 Step 919/1563 Loss: 2.078 | Acc: 32.846% (9670/29440)\n",
            "Epoch 48 Step 920/1563 Loss: 2.078 | Acc: 32.855% (9683/29472)\n",
            "Epoch 48 Step 921/1563 Loss: 2.078 | Acc: 32.874% (9699/29504)\n",
            "Epoch 48 Step 922/1563 Loss: 2.077 | Acc: 32.885% (9713/29536)\n",
            "Epoch 48 Step 923/1563 Loss: 2.077 | Acc: 32.877% (9721/29568)\n",
            "Epoch 48 Step 924/1563 Loss: 2.077 | Acc: 32.878% (9732/29600)\n",
            "Epoch 48 Step 925/1563 Loss: 2.078 | Acc: 32.863% (9738/29632)\n",
            "Epoch 48 Step 926/1563 Loss: 2.078 | Acc: 32.882% (9754/29664)\n",
            "Epoch 48 Step 927/1563 Loss: 2.078 | Acc: 32.893% (9768/29696)\n",
            "Epoch 48 Step 928/1563 Loss: 2.078 | Acc: 32.895% (9779/29728)\n",
            "Epoch 48 Step 929/1563 Loss: 2.078 | Acc: 32.893% (9789/29760)\n",
            "Epoch 48 Step 930/1563 Loss: 2.078 | Acc: 32.895% (9800/29792)\n",
            "Epoch 48 Step 931/1563 Loss: 2.077 | Acc: 32.913% (9816/29824)\n",
            "Epoch 48 Step 932/1563 Loss: 2.077 | Acc: 32.911% (9826/29856)\n",
            "Epoch 48 Step 933/1563 Loss: 2.077 | Acc: 32.920% (9839/29888)\n",
            "Epoch 48 Step 934/1563 Loss: 2.077 | Acc: 32.924% (9851/29920)\n",
            "Epoch 48 Step 935/1563 Loss: 2.077 | Acc: 32.929% (9863/29952)\n",
            "Epoch 48 Step 936/1563 Loss: 2.077 | Acc: 32.934% (9875/29984)\n",
            "Epoch 48 Step 937/1563 Loss: 2.077 | Acc: 32.936% (9886/30016)\n",
            "Epoch 48 Step 938/1563 Loss: 2.077 | Acc: 32.937% (9897/30048)\n",
            "Epoch 48 Step 939/1563 Loss: 2.077 | Acc: 32.942% (9909/30080)\n",
            "Epoch 48 Step 940/1563 Loss: 2.077 | Acc: 32.947% (9921/30112)\n",
            "Epoch 48 Step 941/1563 Loss: 2.077 | Acc: 32.942% (9930/30144)\n",
            "Epoch 48 Step 942/1563 Loss: 2.077 | Acc: 32.933% (9938/30176)\n",
            "Epoch 48 Step 943/1563 Loss: 2.077 | Acc: 32.915% (9943/30208)\n",
            "Epoch 48 Step 944/1563 Loss: 2.077 | Acc: 32.920% (9955/30240)\n",
            "Epoch 48 Step 945/1563 Loss: 2.077 | Acc: 32.922% (9966/30272)\n",
            "Epoch 48 Step 946/1563 Loss: 2.077 | Acc: 32.913% (9974/30304)\n",
            "Epoch 48 Step 947/1563 Loss: 2.077 | Acc: 32.902% (9981/30336)\n",
            "Epoch 48 Step 948/1563 Loss: 2.077 | Acc: 32.900% (9991/30368)\n",
            "Epoch 48 Step 949/1563 Loss: 2.078 | Acc: 32.905% (10003/30400)\n",
            "Epoch 48 Step 950/1563 Loss: 2.078 | Acc: 32.900% (10012/30432)\n",
            "Epoch 48 Step 951/1563 Loss: 2.078 | Acc: 32.895% (10021/30464)\n",
            "Epoch 48 Step 952/1563 Loss: 2.078 | Acc: 32.893% (10031/30496)\n",
            "Epoch 48 Step 953/1563 Loss: 2.078 | Acc: 32.894% (10042/30528)\n",
            "Epoch 48 Step 954/1563 Loss: 2.079 | Acc: 32.886% (10050/30560)\n",
            "Epoch 48 Step 955/1563 Loss: 2.079 | Acc: 32.875% (10057/30592)\n",
            "Epoch 48 Step 956/1563 Loss: 2.079 | Acc: 32.870% (10066/30624)\n",
            "Epoch 48 Step 957/1563 Loss: 2.079 | Acc: 32.861% (10074/30656)\n",
            "Epoch 48 Step 958/1563 Loss: 2.079 | Acc: 32.863% (10085/30688)\n",
            "Epoch 48 Step 959/1563 Loss: 2.079 | Acc: 32.871% (10098/30720)\n",
            "Epoch 48 Step 960/1563 Loss: 2.079 | Acc: 32.863% (10106/30752)\n",
            "Epoch 48 Step 961/1563 Loss: 2.079 | Acc: 32.861% (10116/30784)\n",
            "Epoch 48 Step 962/1563 Loss: 2.079 | Acc: 32.860% (10126/30816)\n",
            "Epoch 48 Step 963/1563 Loss: 2.080 | Acc: 32.845% (10132/30848)\n",
            "Epoch 48 Step 964/1563 Loss: 2.080 | Acc: 32.847% (10143/30880)\n",
            "Epoch 48 Step 965/1563 Loss: 2.080 | Acc: 32.848% (10154/30912)\n",
            "Epoch 48 Step 966/1563 Loss: 2.079 | Acc: 32.846% (10164/30944)\n",
            "Epoch 48 Step 967/1563 Loss: 2.080 | Acc: 32.851% (10176/30976)\n",
            "Epoch 48 Step 968/1563 Loss: 2.080 | Acc: 32.843% (10184/31008)\n",
            "Epoch 48 Step 969/1563 Loss: 2.080 | Acc: 32.841% (10194/31040)\n",
            "Epoch 48 Step 970/1563 Loss: 2.080 | Acc: 32.846% (10206/31072)\n",
            "Epoch 48 Step 971/1563 Loss: 2.080 | Acc: 32.829% (10211/31104)\n",
            "Epoch 48 Step 972/1563 Loss: 2.081 | Acc: 32.817% (10218/31136)\n",
            "Epoch 48 Step 973/1563 Loss: 2.081 | Acc: 32.812% (10227/31168)\n",
            "Epoch 48 Step 974/1563 Loss: 2.081 | Acc: 32.814% (10238/31200)\n",
            "Epoch 48 Step 975/1563 Loss: 2.081 | Acc: 32.806% (10246/31232)\n",
            "Epoch 48 Step 976/1563 Loss: 2.081 | Acc: 32.808% (10257/31264)\n",
            "Epoch 48 Step 977/1563 Loss: 2.080 | Acc: 32.819% (10271/31296)\n",
            "Epoch 48 Step 978/1563 Loss: 2.081 | Acc: 32.808% (10278/31328)\n",
            "Epoch 48 Step 979/1563 Loss: 2.081 | Acc: 32.806% (10288/31360)\n",
            "Epoch 48 Step 980/1563 Loss: 2.081 | Acc: 32.805% (10298/31392)\n",
            "Epoch 48 Step 981/1563 Loss: 2.081 | Acc: 32.803% (10308/31424)\n",
            "Epoch 48 Step 982/1563 Loss: 2.081 | Acc: 32.801% (10318/31456)\n",
            "Epoch 48 Step 983/1563 Loss: 2.081 | Acc: 32.793% (10326/31488)\n",
            "Epoch 48 Step 984/1563 Loss: 2.082 | Acc: 32.773% (10330/31520)\n",
            "Epoch 48 Step 985/1563 Loss: 2.082 | Acc: 32.768% (10339/31552)\n",
            "Epoch 48 Step 986/1563 Loss: 2.082 | Acc: 32.757% (10346/31584)\n",
            "Epoch 48 Step 987/1563 Loss: 2.083 | Acc: 32.752% (10355/31616)\n",
            "Epoch 48 Step 988/1563 Loss: 2.082 | Acc: 32.757% (10367/31648)\n",
            "Epoch 48 Step 989/1563 Loss: 2.083 | Acc: 32.759% (10378/31680)\n",
            "Epoch 48 Step 990/1563 Loss: 2.083 | Acc: 32.757% (10388/31712)\n",
            "Epoch 48 Step 991/1563 Loss: 2.083 | Acc: 32.734% (10391/31744)\n",
            "Epoch 48 Step 992/1563 Loss: 2.084 | Acc: 32.735% (10402/31776)\n",
            "Epoch 48 Step 993/1563 Loss: 2.084 | Acc: 32.734% (10412/31808)\n",
            "Epoch 48 Step 994/1563 Loss: 2.084 | Acc: 32.729% (10421/31840)\n",
            "Epoch 48 Step 995/1563 Loss: 2.084 | Acc: 32.728% (10431/31872)\n",
            "Epoch 48 Step 996/1563 Loss: 2.084 | Acc: 32.726% (10441/31904)\n",
            "Epoch 48 Step 997/1563 Loss: 2.084 | Acc: 32.737% (10455/31936)\n",
            "Epoch 48 Step 998/1563 Loss: 2.084 | Acc: 32.733% (10464/31968)\n",
            "Epoch 48 Step 999/1563 Loss: 2.084 | Acc: 32.731% (10474/32000)\n",
            "Epoch 48 Step 1000/1563 Loss: 2.084 | Acc: 32.727% (10483/32032)\n",
            "Epoch 48 Step 1001/1563 Loss: 2.084 | Acc: 32.728% (10494/32064)\n",
            "Epoch 48 Step 1002/1563 Loss: 2.084 | Acc: 32.721% (10502/32096)\n",
            "Epoch 48 Step 1003/1563 Loss: 2.085 | Acc: 32.722% (10513/32128)\n",
            "Epoch 48 Step 1004/1563 Loss: 2.085 | Acc: 32.711% (10520/32160)\n",
            "Epoch 48 Step 1005/1563 Loss: 2.085 | Acc: 32.710% (10530/32192)\n",
            "Epoch 48 Step 1006/1563 Loss: 2.085 | Acc: 32.702% (10538/32224)\n",
            "Epoch 48 Step 1007/1563 Loss: 2.085 | Acc: 32.710% (10551/32256)\n",
            "Epoch 48 Step 1008/1563 Loss: 2.085 | Acc: 32.724% (10566/32288)\n",
            "Epoch 48 Step 1009/1563 Loss: 2.085 | Acc: 32.720% (10575/32320)\n",
            "Epoch 48 Step 1010/1563 Loss: 2.084 | Acc: 32.727% (10588/32352)\n",
            "Epoch 48 Step 1011/1563 Loss: 2.084 | Acc: 32.729% (10599/32384)\n",
            "Epoch 48 Step 1012/1563 Loss: 2.084 | Acc: 32.749% (10616/32416)\n",
            "Epoch 48 Step 1013/1563 Loss: 2.084 | Acc: 32.754% (10628/32448)\n",
            "Epoch 48 Step 1014/1563 Loss: 2.084 | Acc: 32.752% (10638/32480)\n",
            "Epoch 48 Step 1015/1563 Loss: 2.084 | Acc: 32.757% (10650/32512)\n",
            "Epoch 48 Step 1016/1563 Loss: 2.084 | Acc: 32.759% (10661/32544)\n",
            "Epoch 48 Step 1017/1563 Loss: 2.084 | Acc: 32.763% (10673/32576)\n",
            "Epoch 48 Step 1018/1563 Loss: 2.084 | Acc: 32.762% (10683/32608)\n",
            "Epoch 48 Step 1019/1563 Loss: 2.084 | Acc: 32.767% (10695/32640)\n",
            "Epoch 48 Step 1020/1563 Loss: 2.084 | Acc: 32.768% (10706/32672)\n",
            "Epoch 48 Step 1021/1563 Loss: 2.084 | Acc: 32.779% (10720/32704)\n",
            "Epoch 48 Step 1022/1563 Loss: 2.084 | Acc: 32.780% (10731/32736)\n",
            "Epoch 48 Step 1023/1563 Loss: 2.084 | Acc: 32.782% (10742/32768)\n",
            "Epoch 48 Step 1024/1563 Loss: 2.084 | Acc: 32.780% (10752/32800)\n",
            "Epoch 48 Step 1025/1563 Loss: 2.083 | Acc: 32.794% (10767/32832)\n",
            "Epoch 48 Step 1026/1563 Loss: 2.083 | Acc: 32.799% (10779/32864)\n",
            "Epoch 48 Step 1027/1563 Loss: 2.083 | Acc: 32.806% (10792/32896)\n",
            "Epoch 48 Step 1028/1563 Loss: 2.083 | Acc: 32.805% (10802/32928)\n",
            "Epoch 48 Step 1029/1563 Loss: 2.083 | Acc: 32.800% (10811/32960)\n",
            "Epoch 48 Step 1030/1563 Loss: 2.083 | Acc: 32.799% (10821/32992)\n",
            "Epoch 48 Step 1031/1563 Loss: 2.083 | Acc: 32.803% (10833/33024)\n",
            "Epoch 48 Step 1032/1563 Loss: 2.083 | Acc: 32.802% (10843/33056)\n",
            "Epoch 48 Step 1033/1563 Loss: 2.083 | Acc: 32.800% (10853/33088)\n",
            "Epoch 48 Step 1034/1563 Loss: 2.082 | Acc: 32.820% (10870/33120)\n",
            "Epoch 48 Step 1035/1563 Loss: 2.083 | Acc: 32.822% (10881/33152)\n",
            "Epoch 48 Step 1036/1563 Loss: 2.082 | Acc: 32.832% (10895/33184)\n",
            "Epoch 48 Step 1037/1563 Loss: 2.083 | Acc: 32.825% (10903/33216)\n",
            "Epoch 48 Step 1038/1563 Loss: 2.082 | Acc: 32.829% (10915/33248)\n",
            "Epoch 48 Step 1039/1563 Loss: 2.083 | Acc: 32.822% (10923/33280)\n",
            "Epoch 48 Step 1040/1563 Loss: 2.083 | Acc: 32.823% (10934/33312)\n",
            "Epoch 48 Step 1041/1563 Loss: 2.082 | Acc: 32.833% (10948/33344)\n",
            "Epoch 48 Step 1042/1563 Loss: 2.082 | Acc: 32.835% (10959/33376)\n",
            "Epoch 48 Step 1043/1563 Loss: 2.082 | Acc: 32.836% (10970/33408)\n",
            "Epoch 48 Step 1044/1563 Loss: 2.082 | Acc: 32.835% (10980/33440)\n",
            "Epoch 48 Step 1045/1563 Loss: 2.082 | Acc: 32.836% (10991/33472)\n",
            "Epoch 48 Step 1046/1563 Loss: 2.082 | Acc: 32.841% (11003/33504)\n",
            "Epoch 48 Step 1047/1563 Loss: 2.081 | Acc: 32.842% (11014/33536)\n",
            "Epoch 48 Step 1048/1563 Loss: 2.081 | Acc: 32.847% (11026/33568)\n",
            "Epoch 48 Step 1049/1563 Loss: 2.081 | Acc: 32.845% (11036/33600)\n",
            "Epoch 48 Step 1050/1563 Loss: 2.081 | Acc: 32.847% (11047/33632)\n",
            "Epoch 48 Step 1051/1563 Loss: 2.081 | Acc: 32.842% (11056/33664)\n",
            "Epoch 48 Step 1052/1563 Loss: 2.081 | Acc: 32.835% (11064/33696)\n",
            "Epoch 48 Step 1053/1563 Loss: 2.081 | Acc: 32.821% (11070/33728)\n",
            "Epoch 48 Step 1054/1563 Loss: 2.081 | Acc: 32.826% (11082/33760)\n",
            "Epoch 48 Step 1055/1563 Loss: 2.081 | Acc: 32.830% (11094/33792)\n",
            "Epoch 48 Step 1056/1563 Loss: 2.081 | Acc: 32.829% (11104/33824)\n",
            "Epoch 48 Step 1057/1563 Loss: 2.082 | Acc: 32.810% (11108/33856)\n",
            "Epoch 48 Step 1058/1563 Loss: 2.082 | Acc: 32.808% (11118/33888)\n",
            "Epoch 48 Step 1059/1563 Loss: 2.082 | Acc: 32.798% (11125/33920)\n",
            "Epoch 48 Step 1060/1563 Loss: 2.082 | Acc: 32.799% (11136/33952)\n",
            "Epoch 48 Step 1061/1563 Loss: 2.082 | Acc: 32.807% (11149/33984)\n",
            "Epoch 48 Step 1062/1563 Loss: 2.081 | Acc: 32.811% (11161/34016)\n",
            "Epoch 48 Step 1063/1563 Loss: 2.082 | Acc: 32.801% (11168/34048)\n",
            "Epoch 48 Step 1064/1563 Loss: 2.082 | Acc: 32.802% (11179/34080)\n",
            "Epoch 48 Step 1065/1563 Loss: 2.082 | Acc: 32.807% (11191/34112)\n",
            "Epoch 48 Step 1066/1563 Loss: 2.082 | Acc: 32.799% (11199/34144)\n",
            "Epoch 48 Step 1067/1563 Loss: 2.082 | Acc: 32.804% (11211/34176)\n",
            "Epoch 48 Step 1068/1563 Loss: 2.082 | Acc: 32.793% (11218/34208)\n",
            "Epoch 48 Step 1069/1563 Loss: 2.082 | Acc: 32.810% (11234/34240)\n",
            "Epoch 48 Step 1070/1563 Loss: 2.082 | Acc: 32.817% (11247/34272)\n",
            "Epoch 48 Step 1071/1563 Loss: 2.082 | Acc: 32.818% (11258/34304)\n",
            "Epoch 48 Step 1072/1563 Loss: 2.082 | Acc: 32.820% (11269/34336)\n",
            "Epoch 48 Step 1073/1563 Loss: 2.082 | Acc: 32.815% (11278/34368)\n",
            "Epoch 48 Step 1074/1563 Loss: 2.082 | Acc: 32.831% (11294/34400)\n",
            "Epoch 48 Step 1075/1563 Loss: 2.082 | Acc: 32.830% (11304/34432)\n",
            "Epoch 48 Step 1076/1563 Loss: 2.082 | Acc: 32.828% (11314/34464)\n",
            "Epoch 48 Step 1077/1563 Loss: 2.082 | Acc: 32.821% (11322/34496)\n",
            "Epoch 48 Step 1078/1563 Loss: 2.082 | Acc: 32.817% (11331/34528)\n",
            "Epoch 48 Step 1079/1563 Loss: 2.082 | Acc: 32.827% (11345/34560)\n",
            "Epoch 48 Step 1080/1563 Loss: 2.082 | Acc: 32.823% (11354/34592)\n",
            "Epoch 48 Step 1081/1563 Loss: 2.082 | Acc: 32.824% (11365/34624)\n",
            "Epoch 48 Step 1082/1563 Loss: 2.082 | Acc: 32.825% (11376/34656)\n",
            "Epoch 48 Step 1083/1563 Loss: 2.082 | Acc: 32.836% (11390/34688)\n",
            "Epoch 48 Step 1084/1563 Loss: 2.082 | Acc: 32.837% (11401/34720)\n",
            "Epoch 48 Step 1085/1563 Loss: 2.082 | Acc: 32.830% (11409/34752)\n",
            "Epoch 48 Step 1086/1563 Loss: 2.082 | Acc: 32.831% (11420/34784)\n",
            "Epoch 48 Step 1087/1563 Loss: 2.082 | Acc: 32.827% (11429/34816)\n",
            "Epoch 48 Step 1088/1563 Loss: 2.082 | Acc: 32.846% (11446/34848)\n",
            "Epoch 48 Step 1089/1563 Loss: 2.082 | Acc: 32.841% (11455/34880)\n",
            "Epoch 48 Step 1090/1563 Loss: 2.082 | Acc: 32.834% (11463/34912)\n",
            "Epoch 48 Step 1091/1563 Loss: 2.082 | Acc: 32.830% (11472/34944)\n",
            "Epoch 48 Step 1092/1563 Loss: 2.083 | Acc: 32.837% (11485/34976)\n",
            "Epoch 48 Step 1093/1563 Loss: 2.083 | Acc: 32.838% (11496/35008)\n",
            "Epoch 48 Step 1094/1563 Loss: 2.083 | Acc: 32.834% (11505/35040)\n",
            "Epoch 48 Step 1095/1563 Loss: 2.083 | Acc: 32.824% (11512/35072)\n",
            "Epoch 48 Step 1096/1563 Loss: 2.083 | Acc: 32.817% (11520/35104)\n",
            "Epoch 48 Step 1097/1563 Loss: 2.083 | Acc: 32.812% (11529/35136)\n",
            "Epoch 48 Step 1098/1563 Loss: 2.083 | Acc: 32.814% (11540/35168)\n",
            "Epoch 48 Step 1099/1563 Loss: 2.083 | Acc: 32.807% (11548/35200)\n",
            "Epoch 48 Step 1100/1563 Loss: 2.083 | Acc: 32.808% (11559/35232)\n",
            "Epoch 48 Step 1101/1563 Loss: 2.083 | Acc: 32.798% (11566/35264)\n",
            "Epoch 48 Step 1102/1563 Loss: 2.083 | Acc: 32.794% (11575/35296)\n",
            "Epoch 48 Step 1103/1563 Loss: 2.083 | Acc: 32.793% (11585/35328)\n",
            "Epoch 48 Step 1104/1563 Loss: 2.083 | Acc: 32.791% (11595/35360)\n",
            "Epoch 48 Step 1105/1563 Loss: 2.084 | Acc: 32.784% (11603/35392)\n",
            "Epoch 48 Step 1106/1563 Loss: 2.084 | Acc: 32.783% (11613/35424)\n",
            "Epoch 48 Step 1107/1563 Loss: 2.084 | Acc: 32.779% (11622/35456)\n",
            "Epoch 48 Step 1108/1563 Loss: 2.084 | Acc: 32.780% (11633/35488)\n",
            "Epoch 48 Step 1109/1563 Loss: 2.084 | Acc: 32.782% (11644/35520)\n",
            "Epoch 48 Step 1110/1563 Loss: 2.084 | Acc: 32.763% (11648/35552)\n",
            "Epoch 48 Step 1111/1563 Loss: 2.083 | Acc: 32.768% (11660/35584)\n",
            "Epoch 48 Step 1112/1563 Loss: 2.083 | Acc: 32.772% (11672/35616)\n",
            "Epoch 48 Step 1113/1563 Loss: 2.083 | Acc: 32.773% (11683/35648)\n",
            "Epoch 48 Step 1114/1563 Loss: 2.083 | Acc: 32.755% (11687/35680)\n",
            "Epoch 48 Step 1115/1563 Loss: 2.083 | Acc: 32.751% (11696/35712)\n",
            "Epoch 48 Step 1116/1563 Loss: 2.083 | Acc: 32.755% (11708/35744)\n",
            "Epoch 48 Step 1117/1563 Loss: 2.083 | Acc: 32.751% (11717/35776)\n",
            "Epoch 48 Step 1118/1563 Loss: 2.083 | Acc: 32.738% (11723/35808)\n",
            "Epoch 48 Step 1119/1563 Loss: 2.083 | Acc: 32.740% (11734/35840)\n",
            "Epoch 48 Step 1120/1563 Loss: 2.084 | Acc: 32.727% (11740/35872)\n",
            "Epoch 48 Step 1121/1563 Loss: 2.084 | Acc: 32.726% (11750/35904)\n",
            "Epoch 48 Step 1122/1563 Loss: 2.084 | Acc: 32.708% (11754/35936)\n",
            "Epoch 48 Step 1123/1563 Loss: 2.084 | Acc: 32.704% (11763/35968)\n",
            "Epoch 48 Step 1124/1563 Loss: 2.084 | Acc: 32.703% (11773/36000)\n",
            "Epoch 48 Step 1125/1563 Loss: 2.084 | Acc: 32.693% (11780/36032)\n",
            "Epoch 48 Step 1126/1563 Loss: 2.084 | Acc: 32.695% (11791/36064)\n",
            "Epoch 48 Step 1127/1563 Loss: 2.084 | Acc: 32.693% (11801/36096)\n",
            "Epoch 48 Step 1128/1563 Loss: 2.084 | Acc: 32.687% (11809/36128)\n",
            "Epoch 48 Step 1129/1563 Loss: 2.084 | Acc: 32.688% (11820/36160)\n",
            "Epoch 48 Step 1130/1563 Loss: 2.084 | Acc: 32.681% (11828/36192)\n",
            "Epoch 48 Step 1131/1563 Loss: 2.084 | Acc: 32.677% (11837/36224)\n",
            "Epoch 48 Step 1132/1563 Loss: 2.084 | Acc: 32.676% (11847/36256)\n",
            "Epoch 48 Step 1133/1563 Loss: 2.084 | Acc: 32.666% (11854/36288)\n",
            "Epoch 48 Step 1134/1563 Loss: 2.084 | Acc: 32.668% (11865/36320)\n",
            "Epoch 48 Step 1135/1563 Loss: 2.084 | Acc: 32.664% (11874/36352)\n",
            "Epoch 48 Step 1136/1563 Loss: 2.084 | Acc: 32.665% (11885/36384)\n",
            "Epoch 48 Step 1137/1563 Loss: 2.084 | Acc: 32.656% (11892/36416)\n",
            "Epoch 48 Step 1138/1563 Loss: 2.084 | Acc: 32.660% (11904/36448)\n",
            "Epoch 48 Step 1139/1563 Loss: 2.084 | Acc: 32.664% (11916/36480)\n",
            "Epoch 48 Step 1140/1563 Loss: 2.084 | Acc: 32.669% (11928/36512)\n",
            "Epoch 48 Step 1141/1563 Loss: 2.084 | Acc: 32.676% (11941/36544)\n",
            "Epoch 48 Step 1142/1563 Loss: 2.084 | Acc: 32.677% (11952/36576)\n",
            "Epoch 48 Step 1143/1563 Loss: 2.084 | Acc: 32.668% (11959/36608)\n",
            "Epoch 48 Step 1144/1563 Loss: 2.084 | Acc: 32.675% (11972/36640)\n",
            "Epoch 48 Step 1145/1563 Loss: 2.084 | Acc: 32.673% (11982/36672)\n",
            "Epoch 48 Step 1146/1563 Loss: 2.084 | Acc: 32.678% (11994/36704)\n",
            "Epoch 48 Step 1147/1563 Loss: 2.084 | Acc: 32.687% (12008/36736)\n",
            "Epoch 48 Step 1148/1563 Loss: 2.084 | Acc: 32.681% (12016/36768)\n",
            "Epoch 48 Step 1149/1563 Loss: 2.084 | Acc: 32.682% (12027/36800)\n",
            "Epoch 48 Step 1150/1563 Loss: 2.084 | Acc: 32.684% (12038/36832)\n",
            "Epoch 48 Step 1151/1563 Loss: 2.084 | Acc: 32.680% (12047/36864)\n",
            "Epoch 48 Step 1152/1563 Loss: 2.084 | Acc: 32.676% (12056/36896)\n",
            "Epoch 48 Step 1153/1563 Loss: 2.084 | Acc: 32.677% (12067/36928)\n",
            "Epoch 48 Step 1154/1563 Loss: 2.084 | Acc: 32.670% (12075/36960)\n",
            "Epoch 48 Step 1155/1563 Loss: 2.084 | Acc: 32.675% (12087/36992)\n",
            "Epoch 48 Step 1156/1563 Loss: 2.084 | Acc: 32.679% (12099/37024)\n",
            "Epoch 48 Step 1157/1563 Loss: 2.084 | Acc: 32.675% (12108/37056)\n",
            "Epoch 48 Step 1158/1563 Loss: 2.083 | Acc: 32.679% (12120/37088)\n",
            "Epoch 48 Step 1159/1563 Loss: 2.084 | Acc: 32.672% (12128/37120)\n",
            "Epoch 48 Step 1160/1563 Loss: 2.084 | Acc: 32.668% (12137/37152)\n",
            "Epoch 48 Step 1161/1563 Loss: 2.084 | Acc: 32.673% (12149/37184)\n",
            "Epoch 48 Step 1162/1563 Loss: 2.083 | Acc: 32.671% (12159/37216)\n",
            "Epoch 48 Step 1163/1563 Loss: 2.084 | Acc: 32.659% (12165/37248)\n",
            "Epoch 48 Step 1164/1563 Loss: 2.084 | Acc: 32.650% (12172/37280)\n",
            "Epoch 48 Step 1165/1563 Loss: 2.085 | Acc: 32.641% (12179/37312)\n",
            "Epoch 48 Step 1166/1563 Loss: 2.085 | Acc: 32.634% (12187/37344)\n",
            "Epoch 48 Step 1167/1563 Loss: 2.085 | Acc: 32.628% (12195/37376)\n",
            "Epoch 48 Step 1168/1563 Loss: 2.085 | Acc: 32.627% (12205/37408)\n",
            "Epoch 48 Step 1169/1563 Loss: 2.085 | Acc: 32.631% (12217/37440)\n",
            "Epoch 48 Step 1170/1563 Loss: 2.085 | Acc: 32.619% (12223/37472)\n",
            "Epoch 48 Step 1171/1563 Loss: 2.085 | Acc: 32.615% (12232/37504)\n",
            "Epoch 48 Step 1172/1563 Loss: 2.085 | Acc: 32.617% (12243/37536)\n",
            "Epoch 48 Step 1173/1563 Loss: 2.085 | Acc: 32.613% (12252/37568)\n",
            "Epoch 48 Step 1174/1563 Loss: 2.085 | Acc: 32.609% (12261/37600)\n",
            "Epoch 48 Step 1175/1563 Loss: 2.085 | Acc: 32.603% (12269/37632)\n",
            "Epoch 48 Step 1176/1563 Loss: 2.085 | Acc: 32.604% (12280/37664)\n",
            "Epoch 48 Step 1177/1563 Loss: 2.085 | Acc: 32.598% (12288/37696)\n",
            "Epoch 48 Step 1178/1563 Loss: 2.085 | Acc: 32.599% (12299/37728)\n",
            "Epoch 48 Step 1179/1563 Loss: 2.085 | Acc: 32.590% (12306/37760)\n",
            "Epoch 48 Step 1180/1563 Loss: 2.085 | Acc: 32.589% (12316/37792)\n",
            "Epoch 48 Step 1181/1563 Loss: 2.085 | Acc: 32.588% (12326/37824)\n",
            "Epoch 48 Step 1182/1563 Loss: 2.085 | Acc: 32.589% (12337/37856)\n",
            "Epoch 48 Step 1183/1563 Loss: 2.085 | Acc: 32.604% (12353/37888)\n",
            "Epoch 48 Step 1184/1563 Loss: 2.085 | Acc: 32.611% (12366/37920)\n",
            "Epoch 48 Step 1185/1563 Loss: 2.085 | Acc: 32.612% (12377/37952)\n",
            "Epoch 48 Step 1186/1563 Loss: 2.085 | Acc: 32.614% (12388/37984)\n",
            "Epoch 48 Step 1187/1563 Loss: 2.085 | Acc: 32.605% (12395/38016)\n",
            "Epoch 48 Step 1188/1563 Loss: 2.085 | Acc: 32.593% (12401/38048)\n",
            "Epoch 48 Step 1189/1563 Loss: 2.085 | Acc: 32.595% (12412/38080)\n",
            "Epoch 48 Step 1190/1563 Loss: 2.085 | Acc: 32.593% (12422/38112)\n",
            "Epoch 48 Step 1191/1563 Loss: 2.085 | Acc: 32.592% (12432/38144)\n",
            "Epoch 48 Step 1192/1563 Loss: 2.085 | Acc: 32.578% (12437/38176)\n",
            "Epoch 48 Step 1193/1563 Loss: 2.085 | Acc: 32.582% (12449/38208)\n",
            "Epoch 48 Step 1194/1563 Loss: 2.085 | Acc: 32.592% (12463/38240)\n",
            "Epoch 48 Step 1195/1563 Loss: 2.085 | Acc: 32.601% (12477/38272)\n",
            "Epoch 48 Step 1196/1563 Loss: 2.085 | Acc: 32.600% (12487/38304)\n",
            "Epoch 48 Step 1197/1563 Loss: 2.084 | Acc: 32.604% (12499/38336)\n",
            "Epoch 48 Step 1198/1563 Loss: 2.084 | Acc: 32.605% (12510/38368)\n",
            "Epoch 48 Step 1199/1563 Loss: 2.085 | Acc: 32.602% (12519/38400)\n",
            "Epoch 48 Step 1200/1563 Loss: 2.085 | Acc: 32.603% (12530/38432)\n",
            "Epoch 48 Step 1201/1563 Loss: 2.085 | Acc: 32.597% (12538/38464)\n",
            "Epoch 48 Step 1202/1563 Loss: 2.085 | Acc: 32.606% (12552/38496)\n",
            "Epoch 48 Step 1203/1563 Loss: 2.085 | Acc: 32.610% (12564/38528)\n",
            "Epoch 48 Step 1204/1563 Loss: 2.085 | Acc: 32.612% (12575/38560)\n",
            "Epoch 48 Step 1205/1563 Loss: 2.085 | Acc: 32.608% (12584/38592)\n",
            "Epoch 48 Step 1206/1563 Loss: 2.085 | Acc: 32.604% (12593/38624)\n",
            "Epoch 48 Step 1207/1563 Loss: 2.085 | Acc: 32.606% (12604/38656)\n",
            "Epoch 48 Step 1208/1563 Loss: 2.085 | Acc: 32.607% (12615/38688)\n",
            "Epoch 48 Step 1209/1563 Loss: 2.085 | Acc: 32.588% (12618/38720)\n",
            "Epoch 48 Step 1210/1563 Loss: 2.085 | Acc: 32.597% (12632/38752)\n",
            "Epoch 48 Step 1211/1563 Loss: 2.085 | Acc: 32.596% (12642/38784)\n",
            "Epoch 48 Step 1212/1563 Loss: 2.085 | Acc: 32.597% (12653/38816)\n",
            "Epoch 48 Step 1213/1563 Loss: 2.085 | Acc: 32.607% (12667/38848)\n",
            "Epoch 48 Step 1214/1563 Loss: 2.084 | Acc: 32.616% (12681/38880)\n",
            "Epoch 48 Step 1215/1563 Loss: 2.084 | Acc: 32.620% (12693/38912)\n",
            "Epoch 48 Step 1216/1563 Loss: 2.084 | Acc: 32.626% (12706/38944)\n",
            "Epoch 48 Step 1217/1563 Loss: 2.084 | Acc: 32.625% (12716/38976)\n",
            "Epoch 48 Step 1218/1563 Loss: 2.084 | Acc: 32.622% (12725/39008)\n",
            "Epoch 48 Step 1219/1563 Loss: 2.084 | Acc: 32.613% (12732/39040)\n",
            "Epoch 48 Step 1220/1563 Loss: 2.084 | Acc: 32.612% (12742/39072)\n",
            "Epoch 48 Step 1221/1563 Loss: 2.084 | Acc: 32.626% (12758/39104)\n",
            "Epoch 48 Step 1222/1563 Loss: 2.083 | Acc: 32.637% (12773/39136)\n",
            "Epoch 48 Step 1223/1563 Loss: 2.083 | Acc: 32.636% (12783/39168)\n",
            "Epoch 48 Step 1224/1563 Loss: 2.083 | Acc: 32.645% (12797/39200)\n",
            "Epoch 48 Step 1225/1563 Loss: 2.083 | Acc: 32.644% (12807/39232)\n",
            "Epoch 48 Step 1226/1563 Loss: 2.083 | Acc: 32.643% (12817/39264)\n",
            "Epoch 48 Step 1227/1563 Loss: 2.083 | Acc: 32.639% (12826/39296)\n",
            "Epoch 48 Step 1228/1563 Loss: 2.083 | Acc: 32.636% (12835/39328)\n",
            "Epoch 48 Step 1229/1563 Loss: 2.083 | Acc: 32.640% (12847/39360)\n",
            "Epoch 48 Step 1230/1563 Loss: 2.084 | Acc: 32.626% (12852/39392)\n",
            "Epoch 48 Step 1231/1563 Loss: 2.084 | Acc: 32.625% (12862/39424)\n",
            "Epoch 48 Step 1232/1563 Loss: 2.084 | Acc: 32.624% (12872/39456)\n",
            "Epoch 48 Step 1233/1563 Loss: 2.084 | Acc: 32.615% (12879/39488)\n",
            "Epoch 48 Step 1234/1563 Loss: 2.084 | Acc: 32.616% (12890/39520)\n",
            "Epoch 48 Step 1235/1563 Loss: 2.084 | Acc: 32.620% (12902/39552)\n",
            "Epoch 48 Step 1236/1563 Loss: 2.084 | Acc: 32.624% (12914/39584)\n",
            "Epoch 48 Step 1237/1563 Loss: 2.084 | Acc: 32.611% (12919/39616)\n",
            "Epoch 48 Step 1238/1563 Loss: 2.085 | Acc: 32.604% (12927/39648)\n",
            "Epoch 48 Step 1239/1563 Loss: 2.085 | Acc: 32.596% (12934/39680)\n",
            "Epoch 48 Step 1240/1563 Loss: 2.085 | Acc: 32.602% (12947/39712)\n",
            "Epoch 48 Step 1241/1563 Loss: 2.085 | Acc: 32.601% (12957/39744)\n",
            "Epoch 48 Step 1242/1563 Loss: 2.085 | Acc: 32.598% (12966/39776)\n",
            "Epoch 48 Step 1243/1563 Loss: 2.085 | Acc: 32.604% (12979/39808)\n",
            "Epoch 48 Step 1244/1563 Loss: 2.085 | Acc: 32.600% (12988/39840)\n",
            "Epoch 48 Step 1245/1563 Loss: 2.085 | Acc: 32.599% (12998/39872)\n",
            "Epoch 48 Step 1246/1563 Loss: 2.085 | Acc: 32.593% (13006/39904)\n",
            "Epoch 48 Step 1247/1563 Loss: 2.085 | Acc: 32.590% (13015/39936)\n",
            "Epoch 48 Step 1248/1563 Loss: 2.085 | Acc: 32.586% (13024/39968)\n",
            "Epoch 48 Step 1249/1563 Loss: 2.085 | Acc: 32.570% (13028/40000)\n",
            "Epoch 48 Step 1250/1563 Loss: 2.085 | Acc: 32.556% (13033/40032)\n",
            "Epoch 48 Step 1251/1563 Loss: 2.085 | Acc: 32.555% (13043/40064)\n",
            "Epoch 48 Step 1252/1563 Loss: 2.085 | Acc: 32.554% (13053/40096)\n",
            "Epoch 48 Step 1253/1563 Loss: 2.085 | Acc: 32.558% (13065/40128)\n",
            "Epoch 48 Step 1254/1563 Loss: 2.085 | Acc: 32.560% (13076/40160)\n",
            "Epoch 48 Step 1255/1563 Loss: 2.085 | Acc: 32.569% (13090/40192)\n",
            "Epoch 48 Step 1256/1563 Loss: 2.084 | Acc: 32.585% (13107/40224)\n",
            "Epoch 48 Step 1257/1563 Loss: 2.084 | Acc: 32.591% (13120/40256)\n",
            "Epoch 48 Step 1258/1563 Loss: 2.084 | Acc: 32.590% (13130/40288)\n",
            "Epoch 48 Step 1259/1563 Loss: 2.084 | Acc: 32.584% (13138/40320)\n",
            "Epoch 48 Step 1260/1563 Loss: 2.084 | Acc: 32.588% (13150/40352)\n",
            "Epoch 48 Step 1261/1563 Loss: 2.083 | Acc: 32.612% (13170/40384)\n",
            "Epoch 48 Step 1262/1563 Loss: 2.083 | Acc: 32.608% (13179/40416)\n",
            "Epoch 48 Step 1263/1563 Loss: 2.083 | Acc: 32.600% (13186/40448)\n",
            "Epoch 48 Step 1264/1563 Loss: 2.083 | Acc: 32.621% (13205/40480)\n",
            "Epoch 48 Step 1265/1563 Loss: 2.083 | Acc: 32.620% (13215/40512)\n",
            "Epoch 48 Step 1266/1563 Loss: 2.082 | Acc: 32.624% (13227/40544)\n",
            "Epoch 48 Step 1267/1563 Loss: 2.083 | Acc: 32.620% (13236/40576)\n",
            "Epoch 48 Step 1268/1563 Loss: 2.082 | Acc: 32.622% (13247/40608)\n",
            "Epoch 48 Step 1269/1563 Loss: 2.083 | Acc: 32.613% (13254/40640)\n",
            "Epoch 48 Step 1270/1563 Loss: 2.082 | Acc: 32.615% (13265/40672)\n",
            "Epoch 48 Step 1271/1563 Loss: 2.083 | Acc: 32.604% (13271/40704)\n",
            "Epoch 48 Step 1272/1563 Loss: 2.082 | Acc: 32.603% (13281/40736)\n",
            "Epoch 48 Step 1273/1563 Loss: 2.082 | Acc: 32.611% (13295/40768)\n",
            "Epoch 48 Step 1274/1563 Loss: 2.082 | Acc: 32.613% (13306/40800)\n",
            "Epoch 48 Step 1275/1563 Loss: 2.082 | Acc: 32.629% (13323/40832)\n",
            "Epoch 48 Step 1276/1563 Loss: 2.082 | Acc: 32.620% (13330/40864)\n",
            "Epoch 48 Step 1277/1563 Loss: 2.082 | Acc: 32.624% (13342/40896)\n",
            "Epoch 48 Step 1278/1563 Loss: 2.082 | Acc: 32.630% (13355/40928)\n",
            "Epoch 48 Step 1279/1563 Loss: 2.082 | Acc: 32.632% (13366/40960)\n",
            "Epoch 48 Step 1280/1563 Loss: 2.081 | Acc: 32.638% (13379/40992)\n",
            "Epoch 48 Step 1281/1563 Loss: 2.082 | Acc: 32.639% (13390/41024)\n",
            "Epoch 48 Step 1282/1563 Loss: 2.081 | Acc: 32.643% (13402/41056)\n",
            "Epoch 48 Step 1283/1563 Loss: 2.081 | Acc: 32.645% (13413/41088)\n",
            "Epoch 48 Step 1284/1563 Loss: 2.081 | Acc: 32.643% (13423/41120)\n",
            "Epoch 48 Step 1285/1563 Loss: 2.081 | Acc: 32.635% (13430/41152)\n",
            "Epoch 48 Step 1286/1563 Loss: 2.081 | Acc: 32.632% (13439/41184)\n",
            "Epoch 48 Step 1287/1563 Loss: 2.081 | Acc: 32.631% (13449/41216)\n",
            "Epoch 48 Step 1288/1563 Loss: 2.082 | Acc: 32.625% (13457/41248)\n",
            "Epoch 48 Step 1289/1563 Loss: 2.082 | Acc: 32.631% (13470/41280)\n",
            "Epoch 48 Step 1290/1563 Loss: 2.082 | Acc: 32.635% (13482/41312)\n",
            "Epoch 48 Step 1291/1563 Loss: 2.082 | Acc: 32.643% (13496/41344)\n",
            "Epoch 48 Step 1292/1563 Loss: 2.082 | Acc: 32.647% (13508/41376)\n",
            "Epoch 48 Step 1293/1563 Loss: 2.082 | Acc: 32.648% (13519/41408)\n",
            "Epoch 48 Step 1294/1563 Loss: 2.082 | Acc: 32.640% (13526/41440)\n",
            "Epoch 48 Step 1295/1563 Loss: 2.082 | Acc: 32.632% (13533/41472)\n",
            "Epoch 48 Step 1296/1563 Loss: 2.082 | Acc: 32.631% (13543/41504)\n",
            "Epoch 48 Step 1297/1563 Loss: 2.082 | Acc: 32.637% (13556/41536)\n",
            "Epoch 48 Step 1298/1563 Loss: 2.082 | Acc: 32.643% (13569/41568)\n",
            "Epoch 48 Step 1299/1563 Loss: 2.082 | Acc: 32.637% (13577/41600)\n",
            "Epoch 48 Step 1300/1563 Loss: 2.082 | Acc: 32.634% (13586/41632)\n",
            "Epoch 48 Step 1301/1563 Loss: 2.082 | Acc: 32.635% (13597/41664)\n",
            "Epoch 48 Step 1302/1563 Loss: 2.082 | Acc: 32.639% (13609/41696)\n",
            "Epoch 48 Step 1303/1563 Loss: 2.082 | Acc: 32.647% (13623/41728)\n",
            "Epoch 48 Step 1304/1563 Loss: 2.082 | Acc: 32.639% (13630/41760)\n",
            "Epoch 48 Step 1305/1563 Loss: 2.082 | Acc: 32.640% (13641/41792)\n",
            "Epoch 48 Step 1306/1563 Loss: 2.082 | Acc: 32.627% (13646/41824)\n",
            "Epoch 48 Step 1307/1563 Loss: 2.083 | Acc: 32.612% (13650/41856)\n",
            "Epoch 48 Step 1308/1563 Loss: 2.082 | Acc: 32.606% (13658/41888)\n",
            "Epoch 48 Step 1309/1563 Loss: 2.082 | Acc: 32.600% (13666/41920)\n",
            "Epoch 48 Step 1310/1563 Loss: 2.083 | Acc: 32.597% (13675/41952)\n",
            "Epoch 48 Step 1311/1563 Loss: 2.082 | Acc: 32.608% (13690/41984)\n",
            "Epoch 48 Step 1312/1563 Loss: 2.082 | Acc: 32.611% (13702/42016)\n",
            "Epoch 48 Step 1313/1563 Loss: 2.082 | Acc: 32.613% (13713/42048)\n",
            "Epoch 48 Step 1314/1563 Loss: 2.082 | Acc: 32.616% (13725/42080)\n",
            "Epoch 48 Step 1315/1563 Loss: 2.082 | Acc: 32.615% (13735/42112)\n",
            "Epoch 48 Step 1316/1563 Loss: 2.082 | Acc: 32.617% (13746/42144)\n",
            "Epoch 48 Step 1317/1563 Loss: 2.082 | Acc: 32.609% (13753/42176)\n",
            "Epoch 48 Step 1318/1563 Loss: 2.082 | Acc: 32.605% (13762/42208)\n",
            "Epoch 48 Step 1319/1563 Loss: 2.082 | Acc: 32.597% (13769/42240)\n",
            "Epoch 48 Step 1320/1563 Loss: 2.082 | Acc: 32.601% (13781/42272)\n",
            "Epoch 48 Step 1321/1563 Loss: 2.082 | Acc: 32.595% (13789/42304)\n",
            "Epoch 48 Step 1322/1563 Loss: 2.082 | Acc: 32.594% (13799/42336)\n",
            "Epoch 48 Step 1323/1563 Loss: 2.082 | Acc: 32.598% (13811/42368)\n",
            "Epoch 48 Step 1324/1563 Loss: 2.082 | Acc: 32.597% (13821/42400)\n",
            "Epoch 48 Step 1325/1563 Loss: 2.082 | Acc: 32.607% (13836/42432)\n",
            "Epoch 48 Step 1326/1563 Loss: 2.082 | Acc: 32.599% (13843/42464)\n",
            "Epoch 48 Step 1327/1563 Loss: 2.082 | Acc: 32.596% (13852/42496)\n",
            "Epoch 48 Step 1328/1563 Loss: 2.082 | Acc: 32.593% (13861/42528)\n",
            "Epoch 48 Step 1329/1563 Loss: 2.082 | Acc: 32.596% (13873/42560)\n",
            "Epoch 48 Step 1330/1563 Loss: 2.082 | Acc: 32.591% (13881/42592)\n",
            "Epoch 48 Step 1331/1563 Loss: 2.082 | Acc: 32.587% (13890/42624)\n",
            "Epoch 48 Step 1332/1563 Loss: 2.082 | Acc: 32.589% (13901/42656)\n",
            "Epoch 48 Step 1333/1563 Loss: 2.082 | Acc: 32.583% (13909/42688)\n",
            "Epoch 48 Step 1334/1563 Loss: 2.082 | Acc: 32.584% (13920/42720)\n",
            "Epoch 48 Step 1335/1563 Loss: 2.082 | Acc: 32.581% (13929/42752)\n",
            "Epoch 48 Step 1336/1563 Loss: 2.082 | Acc: 32.575% (13937/42784)\n",
            "Epoch 48 Step 1337/1563 Loss: 2.082 | Acc: 32.572% (13946/42816)\n",
            "Epoch 48 Step 1338/1563 Loss: 2.082 | Acc: 32.566% (13954/42848)\n",
            "Epoch 48 Step 1339/1563 Loss: 2.082 | Acc: 32.561% (13962/42880)\n",
            "Epoch 48 Step 1340/1563 Loss: 2.082 | Acc: 32.569% (13976/42912)\n",
            "Epoch 48 Step 1341/1563 Loss: 2.082 | Acc: 32.563% (13984/42944)\n",
            "Epoch 48 Step 1342/1563 Loss: 2.082 | Acc: 32.560% (13993/42976)\n",
            "Epoch 48 Step 1343/1563 Loss: 2.083 | Acc: 32.557% (14002/43008)\n",
            "Epoch 48 Step 1344/1563 Loss: 2.083 | Acc: 32.556% (14012/43040)\n",
            "Epoch 48 Step 1345/1563 Loss: 2.083 | Acc: 32.555% (14022/43072)\n",
            "Epoch 48 Step 1346/1563 Loss: 2.082 | Acc: 32.552% (14031/43104)\n",
            "Epoch 48 Step 1347/1563 Loss: 2.083 | Acc: 32.548% (14040/43136)\n",
            "Epoch 48 Step 1348/1563 Loss: 2.082 | Acc: 32.554% (14053/43168)\n",
            "Epoch 48 Step 1349/1563 Loss: 2.083 | Acc: 32.546% (14060/43200)\n",
            "Epoch 48 Step 1350/1563 Loss: 2.083 | Acc: 32.545% (14070/43232)\n",
            "Epoch 48 Step 1351/1563 Loss: 2.083 | Acc: 32.542% (14079/43264)\n",
            "Epoch 48 Step 1352/1563 Loss: 2.083 | Acc: 32.541% (14089/43296)\n",
            "Epoch 48 Step 1353/1563 Loss: 2.083 | Acc: 32.538% (14098/43328)\n",
            "Epoch 48 Step 1354/1563 Loss: 2.083 | Acc: 32.539% (14109/43360)\n",
            "Epoch 48 Step 1355/1563 Loss: 2.082 | Acc: 32.543% (14121/43392)\n",
            "Epoch 48 Step 1356/1563 Loss: 2.083 | Acc: 32.544% (14132/43424)\n",
            "Epoch 48 Step 1357/1563 Loss: 2.083 | Acc: 32.550% (14145/43456)\n",
            "Epoch 48 Step 1358/1563 Loss: 2.083 | Acc: 32.549% (14155/43488)\n",
            "Epoch 48 Step 1359/1563 Loss: 2.083 | Acc: 32.546% (14164/43520)\n",
            "Epoch 48 Step 1360/1563 Loss: 2.083 | Acc: 32.543% (14173/43552)\n",
            "Epoch 48 Step 1361/1563 Loss: 2.083 | Acc: 32.544% (14184/43584)\n",
            "Epoch 48 Step 1362/1563 Loss: 2.083 | Acc: 32.543% (14194/43616)\n",
            "Epoch 48 Step 1363/1563 Loss: 2.083 | Acc: 32.538% (14202/43648)\n",
            "Epoch 48 Step 1364/1563 Loss: 2.083 | Acc: 32.527% (14208/43680)\n",
            "Epoch 48 Step 1365/1563 Loss: 2.083 | Acc: 32.529% (14219/43712)\n",
            "Epoch 48 Step 1366/1563 Loss: 2.083 | Acc: 32.519% (14225/43744)\n",
            "Epoch 48 Step 1367/1563 Loss: 2.084 | Acc: 32.511% (14232/43776)\n",
            "Epoch 48 Step 1368/1563 Loss: 2.084 | Acc: 32.510% (14242/43808)\n",
            "Epoch 48 Step 1369/1563 Loss: 2.084 | Acc: 32.505% (14250/43840)\n",
            "Epoch 48 Step 1370/1563 Loss: 2.083 | Acc: 32.510% (14263/43872)\n",
            "Epoch 48 Step 1371/1563 Loss: 2.083 | Acc: 32.505% (14271/43904)\n",
            "Epoch 48 Step 1372/1563 Loss: 2.083 | Acc: 32.513% (14285/43936)\n",
            "Epoch 48 Step 1373/1563 Loss: 2.083 | Acc: 32.524% (14300/43968)\n",
            "Epoch 48 Step 1374/1563 Loss: 2.083 | Acc: 32.525% (14311/44000)\n",
            "Epoch 48 Step 1375/1563 Loss: 2.083 | Acc: 32.520% (14319/44032)\n",
            "Epoch 48 Step 1376/1563 Loss: 2.083 | Acc: 32.516% (14328/44064)\n",
            "Epoch 48 Step 1377/1563 Loss: 2.083 | Acc: 32.515% (14338/44096)\n",
            "Epoch 48 Step 1378/1563 Loss: 2.083 | Acc: 32.510% (14346/44128)\n",
            "Epoch 48 Step 1379/1563 Loss: 2.083 | Acc: 32.511% (14357/44160)\n",
            "Epoch 48 Step 1380/1563 Loss: 2.083 | Acc: 32.517% (14370/44192)\n",
            "Epoch 48 Step 1381/1563 Loss: 2.083 | Acc: 32.514% (14379/44224)\n",
            "Epoch 48 Step 1382/1563 Loss: 2.083 | Acc: 32.500% (14383/44256)\n",
            "Epoch 48 Step 1383/1563 Loss: 2.083 | Acc: 32.499% (14393/44288)\n",
            "Epoch 48 Step 1384/1563 Loss: 2.083 | Acc: 32.502% (14405/44320)\n",
            "Epoch 48 Step 1385/1563 Loss: 2.083 | Acc: 32.497% (14413/44352)\n",
            "Epoch 48 Step 1386/1563 Loss: 2.083 | Acc: 32.503% (14426/44384)\n",
            "Epoch 48 Step 1387/1563 Loss: 2.083 | Acc: 32.504% (14437/44416)\n",
            "Epoch 48 Step 1388/1563 Loss: 2.083 | Acc: 32.492% (14442/44448)\n",
            "Epoch 48 Step 1389/1563 Loss: 2.083 | Acc: 32.498% (14455/44480)\n",
            "Epoch 48 Step 1390/1563 Loss: 2.083 | Acc: 32.501% (14467/44512)\n",
            "Epoch 48 Step 1391/1563 Loss: 2.083 | Acc: 32.500% (14477/44544)\n",
            "Epoch 48 Step 1392/1563 Loss: 2.083 | Acc: 32.500% (14487/44576)\n",
            "Epoch 48 Step 1393/1563 Loss: 2.083 | Acc: 32.499% (14497/44608)\n",
            "Epoch 48 Step 1394/1563 Loss: 2.083 | Acc: 32.513% (14514/44640)\n",
            "Epoch 48 Step 1395/1563 Loss: 2.083 | Acc: 32.508% (14522/44672)\n",
            "Epoch 48 Step 1396/1563 Loss: 2.083 | Acc: 32.514% (14535/44704)\n",
            "Epoch 48 Step 1397/1563 Loss: 2.083 | Acc: 32.513% (14545/44736)\n",
            "Epoch 48 Step 1398/1563 Loss: 2.083 | Acc: 32.521% (14559/44768)\n",
            "Epoch 48 Step 1399/1563 Loss: 2.083 | Acc: 32.518% (14568/44800)\n",
            "Epoch 48 Step 1400/1563 Loss: 2.083 | Acc: 32.530% (14584/44832)\n",
            "Epoch 48 Step 1401/1563 Loss: 2.083 | Acc: 32.527% (14593/44864)\n",
            "Epoch 48 Step 1402/1563 Loss: 2.082 | Acc: 32.531% (14605/44896)\n",
            "Epoch 48 Step 1403/1563 Loss: 2.082 | Acc: 32.539% (14619/44928)\n",
            "Epoch 48 Step 1404/1563 Loss: 2.082 | Acc: 32.542% (14631/44960)\n",
            "Epoch 48 Step 1405/1563 Loss: 2.082 | Acc: 32.550% (14645/44992)\n",
            "Epoch 48 Step 1406/1563 Loss: 2.082 | Acc: 32.554% (14657/45024)\n",
            "Epoch 48 Step 1407/1563 Loss: 2.082 | Acc: 32.548% (14665/45056)\n",
            "Epoch 48 Step 1408/1563 Loss: 2.082 | Acc: 32.541% (14672/45088)\n",
            "Epoch 48 Step 1409/1563 Loss: 2.082 | Acc: 32.535% (14680/45120)\n",
            "Epoch 48 Step 1410/1563 Loss: 2.082 | Acc: 32.530% (14688/45152)\n",
            "Epoch 48 Step 1411/1563 Loss: 2.082 | Acc: 32.531% (14699/45184)\n",
            "Epoch 48 Step 1412/1563 Loss: 2.082 | Acc: 32.533% (14710/45216)\n",
            "Epoch 48 Step 1413/1563 Loss: 2.082 | Acc: 32.538% (14723/45248)\n",
            "Epoch 48 Step 1414/1563 Loss: 2.082 | Acc: 32.540% (14734/45280)\n",
            "Epoch 48 Step 1415/1563 Loss: 2.082 | Acc: 32.543% (14746/45312)\n",
            "Epoch 48 Step 1416/1563 Loss: 2.082 | Acc: 32.549% (14759/45344)\n",
            "Epoch 48 Step 1417/1563 Loss: 2.081 | Acc: 32.555% (14772/45376)\n",
            "Epoch 48 Step 1418/1563 Loss: 2.081 | Acc: 32.565% (14787/45408)\n",
            "Epoch 48 Step 1419/1563 Loss: 2.081 | Acc: 32.555% (14793/45440)\n",
            "Epoch 48 Step 1420/1563 Loss: 2.081 | Acc: 32.556% (14804/45472)\n",
            "Epoch 48 Step 1421/1563 Loss: 2.081 | Acc: 32.551% (14812/45504)\n",
            "Epoch 48 Step 1422/1563 Loss: 2.081 | Acc: 32.554% (14824/45536)\n",
            "Epoch 48 Step 1423/1563 Loss: 2.082 | Acc: 32.543% (14829/45568)\n",
            "Epoch 48 Step 1424/1563 Loss: 2.082 | Acc: 32.531% (14834/45600)\n",
            "Epoch 48 Step 1425/1563 Loss: 2.082 | Acc: 32.530% (14844/45632)\n",
            "Epoch 48 Step 1426/1563 Loss: 2.082 | Acc: 32.542% (14860/45664)\n",
            "Epoch 48 Step 1427/1563 Loss: 2.082 | Acc: 32.539% (14869/45696)\n",
            "Epoch 48 Step 1428/1563 Loss: 2.082 | Acc: 32.545% (14882/45728)\n",
            "Epoch 48 Step 1429/1563 Loss: 2.082 | Acc: 32.548% (14894/45760)\n",
            "Epoch 48 Step 1430/1563 Loss: 2.082 | Acc: 32.547% (14904/45792)\n",
            "Epoch 48 Step 1431/1563 Loss: 2.082 | Acc: 32.544% (14913/45824)\n",
            "Epoch 48 Step 1432/1563 Loss: 2.082 | Acc: 32.548% (14925/45856)\n",
            "Epoch 48 Step 1433/1563 Loss: 2.082 | Acc: 32.540% (14932/45888)\n",
            "Epoch 48 Step 1434/1563 Loss: 2.082 | Acc: 32.539% (14942/45920)\n",
            "Epoch 48 Step 1435/1563 Loss: 2.082 | Acc: 32.534% (14950/45952)\n",
            "Epoch 48 Step 1436/1563 Loss: 2.082 | Acc: 32.537% (14962/45984)\n",
            "Epoch 48 Step 1437/1563 Loss: 2.082 | Acc: 32.528% (14968/46016)\n",
            "Epoch 48 Step 1438/1563 Loss: 2.082 | Acc: 32.529% (14979/46048)\n",
            "Epoch 48 Step 1439/1563 Loss: 2.082 | Acc: 32.533% (14991/46080)\n",
            "Epoch 48 Step 1440/1563 Loss: 2.082 | Acc: 32.536% (15003/46112)\n",
            "Epoch 48 Step 1441/1563 Loss: 2.082 | Acc: 32.546% (15018/46144)\n",
            "Epoch 48 Step 1442/1563 Loss: 2.082 | Acc: 32.554% (15032/46176)\n",
            "Epoch 48 Step 1443/1563 Loss: 2.082 | Acc: 32.551% (15041/46208)\n",
            "Epoch 48 Step 1444/1563 Loss: 2.082 | Acc: 32.554% (15053/46240)\n",
            "Epoch 48 Step 1445/1563 Loss: 2.082 | Acc: 32.560% (15066/46272)\n",
            "Epoch 48 Step 1446/1563 Loss: 2.081 | Acc: 32.570% (15081/46304)\n",
            "Epoch 48 Step 1447/1563 Loss: 2.082 | Acc: 32.577% (15095/46336)\n",
            "Epoch 48 Step 1448/1563 Loss: 2.082 | Acc: 32.579% (15106/46368)\n",
            "Epoch 48 Step 1449/1563 Loss: 2.082 | Acc: 32.573% (15114/46400)\n",
            "Epoch 48 Step 1450/1563 Loss: 2.082 | Acc: 32.568% (15122/46432)\n",
            "Epoch 48 Step 1451/1563 Loss: 2.082 | Acc: 32.569% (15133/46464)\n",
            "Epoch 48 Step 1452/1563 Loss: 2.082 | Acc: 32.571% (15144/46496)\n",
            "Epoch 48 Step 1453/1563 Loss: 2.082 | Acc: 32.567% (15153/46528)\n",
            "Epoch 48 Step 1454/1563 Loss: 2.082 | Acc: 32.562% (15161/46560)\n",
            "Epoch 48 Step 1455/1563 Loss: 2.082 | Acc: 32.570% (15175/46592)\n",
            "Epoch 48 Step 1456/1563 Loss: 2.082 | Acc: 32.575% (15188/46624)\n",
            "Epoch 48 Step 1457/1563 Loss: 2.082 | Acc: 32.577% (15199/46656)\n",
            "Epoch 48 Step 1458/1563 Loss: 2.082 | Acc: 32.580% (15211/46688)\n",
            "Epoch 48 Step 1459/1563 Loss: 2.082 | Acc: 32.575% (15219/46720)\n",
            "Epoch 48 Step 1460/1563 Loss: 2.082 | Acc: 32.576% (15230/46752)\n",
            "Epoch 48 Step 1461/1563 Loss: 2.082 | Acc: 32.580% (15242/46784)\n",
            "Epoch 48 Step 1462/1563 Loss: 2.082 | Acc: 32.576% (15251/46816)\n",
            "Epoch 48 Step 1463/1563 Loss: 2.082 | Acc: 32.576% (15261/46848)\n",
            "Epoch 48 Step 1464/1563 Loss: 2.082 | Acc: 32.577% (15272/46880)\n",
            "Epoch 48 Step 1465/1563 Loss: 2.083 | Acc: 32.567% (15278/46912)\n",
            "Epoch 48 Step 1466/1563 Loss: 2.083 | Acc: 32.579% (15294/46944)\n",
            "Epoch 48 Step 1467/1563 Loss: 2.083 | Acc: 32.578% (15304/46976)\n",
            "Epoch 48 Step 1468/1563 Loss: 2.083 | Acc: 32.571% (15311/47008)\n",
            "Epoch 48 Step 1469/1563 Loss: 2.083 | Acc: 32.560% (15316/47040)\n",
            "Epoch 48 Step 1470/1563 Loss: 2.083 | Acc: 32.552% (15323/47072)\n",
            "Epoch 48 Step 1471/1563 Loss: 2.083 | Acc: 32.551% (15333/47104)\n",
            "Epoch 48 Step 1472/1563 Loss: 2.083 | Acc: 32.544% (15340/47136)\n",
            "Epoch 48 Step 1473/1563 Loss: 2.083 | Acc: 32.543% (15350/47168)\n",
            "Epoch 48 Step 1474/1563 Loss: 2.083 | Acc: 32.540% (15359/47200)\n",
            "Epoch 48 Step 1475/1563 Loss: 2.083 | Acc: 32.548% (15373/47232)\n",
            "Epoch 48 Step 1476/1563 Loss: 2.083 | Acc: 32.553% (15386/47264)\n",
            "Epoch 48 Step 1477/1563 Loss: 2.083 | Acc: 32.559% (15399/47296)\n",
            "Epoch 48 Step 1478/1563 Loss: 2.083 | Acc: 32.556% (15408/47328)\n",
            "Epoch 48 Step 1479/1563 Loss: 2.083 | Acc: 32.549% (15415/47360)\n",
            "Epoch 48 Step 1480/1563 Loss: 2.083 | Acc: 32.552% (15427/47392)\n",
            "Epoch 48 Step 1481/1563 Loss: 2.083 | Acc: 32.545% (15434/47424)\n",
            "Epoch 48 Step 1482/1563 Loss: 2.083 | Acc: 32.546% (15445/47456)\n",
            "Epoch 48 Step 1483/1563 Loss: 2.083 | Acc: 32.551% (15458/47488)\n",
            "Epoch 48 Step 1484/1563 Loss: 2.082 | Acc: 32.563% (15474/47520)\n",
            "Epoch 48 Step 1485/1563 Loss: 2.083 | Acc: 32.556% (15481/47552)\n",
            "Epoch 48 Step 1486/1563 Loss: 2.083 | Acc: 32.559% (15493/47584)\n",
            "Epoch 48 Step 1487/1563 Loss: 2.082 | Acc: 32.560% (15504/47616)\n",
            "Epoch 48 Step 1488/1563 Loss: 2.083 | Acc: 32.555% (15512/47648)\n",
            "Epoch 48 Step 1489/1563 Loss: 2.083 | Acc: 32.559% (15524/47680)\n",
            "Epoch 48 Step 1490/1563 Loss: 2.083 | Acc: 32.558% (15534/47712)\n",
            "Epoch 48 Step 1491/1563 Loss: 2.082 | Acc: 32.567% (15549/47744)\n",
            "Epoch 48 Step 1492/1563 Loss: 2.082 | Acc: 32.562% (15557/47776)\n",
            "Epoch 48 Step 1493/1563 Loss: 2.082 | Acc: 32.568% (15570/47808)\n",
            "Epoch 48 Step 1494/1563 Loss: 2.082 | Acc: 32.563% (15578/47840)\n",
            "Epoch 48 Step 1495/1563 Loss: 2.082 | Acc: 32.566% (15590/47872)\n",
            "Epoch 48 Step 1496/1563 Loss: 2.082 | Acc: 32.567% (15601/47904)\n",
            "Epoch 48 Step 1497/1563 Loss: 2.082 | Acc: 32.577% (15616/47936)\n",
            "Epoch 48 Step 1498/1563 Loss: 2.081 | Acc: 32.576% (15626/47968)\n",
            "Epoch 48 Step 1499/1563 Loss: 2.081 | Acc: 32.583% (15640/48000)\n",
            "Epoch 48 Step 1500/1563 Loss: 2.081 | Acc: 32.578% (15648/48032)\n",
            "Epoch 48 Step 1501/1563 Loss: 2.081 | Acc: 32.579% (15659/48064)\n",
            "Epoch 48 Step 1502/1563 Loss: 2.081 | Acc: 32.583% (15671/48096)\n",
            "Epoch 48 Step 1503/1563 Loss: 2.081 | Acc: 32.580% (15680/48128)\n",
            "Epoch 48 Step 1504/1563 Loss: 2.081 | Acc: 32.583% (15692/48160)\n",
            "Epoch 48 Step 1505/1563 Loss: 2.081 | Acc: 32.578% (15700/48192)\n",
            "Epoch 48 Step 1506/1563 Loss: 2.081 | Acc: 32.573% (15708/48224)\n",
            "Epoch 48 Step 1507/1563 Loss: 2.081 | Acc: 32.568% (15716/48256)\n",
            "Epoch 48 Step 1508/1563 Loss: 2.081 | Acc: 32.563% (15724/48288)\n",
            "Epoch 48 Step 1509/1563 Loss: 2.081 | Acc: 32.558% (15732/48320)\n",
            "Epoch 48 Step 1510/1563 Loss: 2.081 | Acc: 32.553% (15740/48352)\n",
            "Epoch 48 Step 1511/1563 Loss: 2.081 | Acc: 32.550% (15749/48384)\n",
            "Epoch 48 Step 1512/1563 Loss: 2.081 | Acc: 32.543% (15756/48416)\n",
            "Epoch 48 Step 1513/1563 Loss: 2.081 | Acc: 32.548% (15769/48448)\n",
            "Epoch 48 Step 1514/1563 Loss: 2.081 | Acc: 32.543% (15777/48480)\n",
            "Epoch 48 Step 1515/1563 Loss: 2.081 | Acc: 32.542% (15787/48512)\n",
            "Epoch 48 Step 1516/1563 Loss: 2.081 | Acc: 32.544% (15798/48544)\n",
            "Epoch 48 Step 1517/1563 Loss: 2.081 | Acc: 32.541% (15807/48576)\n",
            "Epoch 48 Step 1518/1563 Loss: 2.081 | Acc: 32.540% (15817/48608)\n",
            "Epoch 48 Step 1519/1563 Loss: 2.081 | Acc: 32.537% (15826/48640)\n",
            "Epoch 48 Step 1520/1563 Loss: 2.081 | Acc: 32.548% (15842/48672)\n",
            "Epoch 48 Step 1521/1563 Loss: 2.081 | Acc: 32.548% (15852/48704)\n",
            "Epoch 48 Step 1522/1563 Loss: 2.081 | Acc: 32.555% (15866/48736)\n",
            "Epoch 48 Step 1523/1563 Loss: 2.081 | Acc: 32.560% (15879/48768)\n",
            "Epoch 48 Step 1524/1563 Loss: 2.081 | Acc: 32.561% (15890/48800)\n",
            "Epoch 48 Step 1525/1563 Loss: 2.081 | Acc: 32.561% (15900/48832)\n",
            "Epoch 48 Step 1526/1563 Loss: 2.081 | Acc: 32.556% (15908/48864)\n",
            "Epoch 48 Step 1527/1563 Loss: 2.081 | Acc: 32.561% (15921/48896)\n",
            "Epoch 48 Step 1528/1563 Loss: 2.081 | Acc: 32.566% (15934/48928)\n",
            "Epoch 48 Step 1529/1563 Loss: 2.081 | Acc: 32.569% (15946/48960)\n",
            "Epoch 48 Step 1530/1563 Loss: 2.081 | Acc: 32.573% (15958/48992)\n",
            "Epoch 48 Step 1531/1563 Loss: 2.081 | Acc: 32.566% (15965/49024)\n",
            "Epoch 48 Step 1532/1563 Loss: 2.081 | Acc: 32.563% (15974/49056)\n",
            "Epoch 48 Step 1533/1563 Loss: 2.081 | Acc: 32.554% (15980/49088)\n",
            "Epoch 48 Step 1534/1563 Loss: 2.081 | Acc: 32.549% (15988/49120)\n",
            "Epoch 48 Step 1535/1563 Loss: 2.081 | Acc: 32.546% (15997/49152)\n",
            "Epoch 48 Step 1536/1563 Loss: 2.081 | Acc: 32.553% (16011/49184)\n",
            "Epoch 48 Step 1537/1563 Loss: 2.080 | Acc: 32.554% (16022/49216)\n",
            "Epoch 48 Step 1538/1563 Loss: 2.080 | Acc: 32.560% (16035/49248)\n",
            "Epoch 48 Step 1539/1563 Loss: 2.080 | Acc: 32.553% (16042/49280)\n",
            "Epoch 48 Step 1540/1563 Loss: 2.080 | Acc: 32.564% (16058/49312)\n",
            "Epoch 48 Step 1541/1563 Loss: 2.080 | Acc: 32.561% (16067/49344)\n",
            "Epoch 48 Step 1542/1563 Loss: 2.080 | Acc: 32.556% (16075/49376)\n",
            "Epoch 48 Step 1543/1563 Loss: 2.080 | Acc: 32.553% (16084/49408)\n",
            "Epoch 48 Step 1544/1563 Loss: 2.080 | Acc: 32.544% (16090/49440)\n",
            "Epoch 48 Step 1545/1563 Loss: 2.080 | Acc: 32.538% (16097/49472)\n",
            "Epoch 48 Step 1546/1563 Loss: 2.080 | Acc: 32.549% (16113/49504)\n",
            "Epoch 48 Step 1547/1563 Loss: 2.080 | Acc: 32.554% (16126/49536)\n",
            "Epoch 48 Step 1548/1563 Loss: 2.080 | Acc: 32.557% (16138/49568)\n",
            "Epoch 48 Step 1549/1563 Loss: 2.080 | Acc: 32.562% (16151/49600)\n",
            "Epoch 48 Step 1550/1563 Loss: 2.080 | Acc: 32.564% (16162/49632)\n",
            "Epoch 48 Step 1551/1563 Loss: 2.080 | Acc: 32.567% (16174/49664)\n",
            "Epoch 48 Step 1552/1563 Loss: 2.079 | Acc: 32.576% (16189/49696)\n",
            "Epoch 48 Step 1553/1563 Loss: 2.080 | Acc: 32.577% (16200/49728)\n",
            "Epoch 48 Step 1554/1563 Loss: 2.079 | Acc: 32.586% (16215/49760)\n",
            "Epoch 48 Step 1555/1563 Loss: 2.079 | Acc: 32.584% (16224/49792)\n",
            "Epoch 48 Step 1556/1563 Loss: 2.079 | Acc: 32.581% (16233/49824)\n",
            "Epoch 48 Step 1557/1563 Loss: 2.079 | Acc: 32.582% (16244/49856)\n",
            "Epoch 48 Step 1558/1563 Loss: 2.079 | Acc: 32.583% (16255/49888)\n",
            "Epoch 48 Step 1559/1563 Loss: 2.079 | Acc: 32.582% (16265/49920)\n",
            "Epoch 48 Step 1560/1563 Loss: 2.079 | Acc: 32.583% (16276/49952)\n",
            "Epoch 48 Step 1561/1563 Loss: 2.079 | Acc: 32.578% (16284/49984)\n",
            "Epoch 48 Step 1562/1563 Loss: 2.080 | Acc: 32.574% (16287/50000)\n",
            "Epoch 48 Step 0/313 Test Loss: 1.646 | Test Acc: 43.750% (14/32)\n",
            "Epoch 48 Step 1/313 Test Loss: 1.827 | Test Acc: 42.188% (27/64)\n",
            "Epoch 48 Step 2/313 Test Loss: 1.887 | Test Acc: 39.583% (38/96)\n",
            "Epoch 48 Step 3/313 Test Loss: 1.876 | Test Acc: 38.281% (49/128)\n",
            "Epoch 48 Step 4/313 Test Loss: 1.935 | Test Acc: 36.875% (59/160)\n",
            "Epoch 48 Step 5/313 Test Loss: 1.941 | Test Acc: 36.458% (70/192)\n",
            "Epoch 48 Step 6/313 Test Loss: 2.010 | Test Acc: 34.821% (78/224)\n",
            "Epoch 48 Step 7/313 Test Loss: 2.017 | Test Acc: 33.203% (85/256)\n",
            "Epoch 48 Step 8/313 Test Loss: 2.021 | Test Acc: 32.639% (94/288)\n",
            "Epoch 48 Step 9/313 Test Loss: 1.972 | Test Acc: 34.688% (111/320)\n",
            "Epoch 48 Step 10/313 Test Loss: 1.979 | Test Acc: 34.375% (121/352)\n",
            "Epoch 48 Step 11/313 Test Loss: 2.048 | Test Acc: 33.333% (128/384)\n",
            "Epoch 48 Step 12/313 Test Loss: 2.052 | Test Acc: 33.173% (138/416)\n",
            "Epoch 48 Step 13/313 Test Loss: 2.075 | Test Acc: 32.812% (147/448)\n",
            "Epoch 48 Step 14/313 Test Loss: 2.085 | Test Acc: 32.917% (158/480)\n",
            "Epoch 48 Step 15/313 Test Loss: 2.071 | Test Acc: 33.398% (171/512)\n",
            "Epoch 48 Step 16/313 Test Loss: 2.053 | Test Acc: 33.456% (182/544)\n",
            "Epoch 48 Step 17/313 Test Loss: 2.031 | Test Acc: 33.507% (193/576)\n",
            "Epoch 48 Step 18/313 Test Loss: 2.028 | Test Acc: 33.882% (206/608)\n",
            "Epoch 48 Step 19/313 Test Loss: 2.019 | Test Acc: 34.531% (221/640)\n",
            "Epoch 48 Step 20/313 Test Loss: 2.008 | Test Acc: 34.673% (233/672)\n",
            "Epoch 48 Step 21/313 Test Loss: 2.027 | Test Acc: 34.233% (241/704)\n",
            "Epoch 48 Step 22/313 Test Loss: 2.016 | Test Acc: 34.511% (254/736)\n",
            "Epoch 48 Step 23/313 Test Loss: 2.036 | Test Acc: 34.375% (264/768)\n",
            "Epoch 48 Step 24/313 Test Loss: 2.034 | Test Acc: 34.375% (275/800)\n",
            "Epoch 48 Step 25/313 Test Loss: 2.025 | Test Acc: 34.255% (285/832)\n",
            "Epoch 48 Step 26/313 Test Loss: 2.021 | Test Acc: 34.722% (300/864)\n",
            "Epoch 48 Step 27/313 Test Loss: 2.014 | Test Acc: 35.156% (315/896)\n",
            "Epoch 48 Step 28/313 Test Loss: 2.013 | Test Acc: 35.345% (328/928)\n",
            "Epoch 48 Step 29/313 Test Loss: 2.000 | Test Acc: 35.729% (343/960)\n",
            "Epoch 48 Step 30/313 Test Loss: 2.001 | Test Acc: 35.484% (352/992)\n",
            "Epoch 48 Step 31/313 Test Loss: 2.001 | Test Acc: 35.742% (366/1024)\n",
            "Epoch 48 Step 32/313 Test Loss: 2.007 | Test Acc: 35.417% (374/1056)\n",
            "Epoch 48 Step 33/313 Test Loss: 2.002 | Test Acc: 35.938% (391/1088)\n",
            "Epoch 48 Step 34/313 Test Loss: 1.991 | Test Acc: 35.982% (403/1120)\n",
            "Epoch 48 Step 35/313 Test Loss: 2.004 | Test Acc: 35.503% (409/1152)\n",
            "Epoch 48 Step 36/313 Test Loss: 1.999 | Test Acc: 35.473% (420/1184)\n",
            "Epoch 48 Step 37/313 Test Loss: 2.009 | Test Acc: 35.609% (433/1216)\n",
            "Epoch 48 Step 38/313 Test Loss: 2.019 | Test Acc: 35.176% (439/1248)\n",
            "Epoch 48 Step 39/313 Test Loss: 2.026 | Test Acc: 35.312% (452/1280)\n",
            "Epoch 48 Step 40/313 Test Loss: 2.028 | Test Acc: 35.518% (466/1312)\n",
            "Epoch 48 Step 41/313 Test Loss: 2.026 | Test Acc: 35.417% (476/1344)\n",
            "Epoch 48 Step 42/313 Test Loss: 2.016 | Test Acc: 35.610% (490/1376)\n",
            "Epoch 48 Step 43/313 Test Loss: 2.020 | Test Acc: 35.582% (501/1408)\n",
            "Epoch 48 Step 44/313 Test Loss: 2.021 | Test Acc: 35.556% (512/1440)\n",
            "Epoch 48 Step 45/313 Test Loss: 2.011 | Test Acc: 35.734% (526/1472)\n",
            "Epoch 48 Step 46/313 Test Loss: 2.012 | Test Acc: 35.638% (536/1504)\n",
            "Epoch 48 Step 47/313 Test Loss: 2.012 | Test Acc: 35.612% (547/1536)\n",
            "Epoch 48 Step 48/313 Test Loss: 2.010 | Test Acc: 35.459% (556/1568)\n",
            "Epoch 48 Step 49/313 Test Loss: 2.020 | Test Acc: 35.062% (561/1600)\n",
            "Epoch 48 Step 50/313 Test Loss: 2.017 | Test Acc: 35.110% (573/1632)\n",
            "Epoch 48 Step 51/313 Test Loss: 2.012 | Test Acc: 35.216% (586/1664)\n",
            "Epoch 48 Step 52/313 Test Loss: 2.010 | Test Acc: 35.200% (597/1696)\n",
            "Epoch 48 Step 53/313 Test Loss: 2.019 | Test Acc: 35.127% (607/1728)\n",
            "Epoch 48 Step 54/313 Test Loss: 2.028 | Test Acc: 34.943% (615/1760)\n",
            "Epoch 48 Step 55/313 Test Loss: 2.019 | Test Acc: 35.100% (629/1792)\n",
            "Epoch 48 Step 56/313 Test Loss: 2.024 | Test Acc: 34.923% (637/1824)\n",
            "Epoch 48 Step 57/313 Test Loss: 2.021 | Test Acc: 35.022% (650/1856)\n",
            "Epoch 48 Step 58/313 Test Loss: 2.018 | Test Acc: 35.117% (663/1888)\n",
            "Epoch 48 Step 59/313 Test Loss: 2.019 | Test Acc: 35.104% (674/1920)\n",
            "Epoch 48 Step 60/313 Test Loss: 2.020 | Test Acc: 35.041% (684/1952)\n",
            "Epoch 48 Step 61/313 Test Loss: 2.015 | Test Acc: 35.282% (700/1984)\n",
            "Epoch 48 Step 62/313 Test Loss: 2.022 | Test Acc: 35.119% (708/2016)\n",
            "Epoch 48 Step 63/313 Test Loss: 2.016 | Test Acc: 35.254% (722/2048)\n",
            "Epoch 48 Step 64/313 Test Loss: 2.012 | Test Acc: 35.385% (736/2080)\n",
            "Epoch 48 Step 65/313 Test Loss: 2.014 | Test Acc: 35.369% (747/2112)\n",
            "Epoch 48 Step 66/313 Test Loss: 2.011 | Test Acc: 35.261% (756/2144)\n",
            "Epoch 48 Step 67/313 Test Loss: 2.014 | Test Acc: 35.202% (766/2176)\n",
            "Epoch 48 Step 68/313 Test Loss: 2.016 | Test Acc: 35.326% (780/2208)\n",
            "Epoch 48 Step 69/313 Test Loss: 2.017 | Test Acc: 35.223% (789/2240)\n",
            "Epoch 48 Step 70/313 Test Loss: 2.015 | Test Acc: 35.299% (802/2272)\n",
            "Epoch 48 Step 71/313 Test Loss: 2.017 | Test Acc: 35.113% (809/2304)\n",
            "Epoch 48 Step 72/313 Test Loss: 2.019 | Test Acc: 35.060% (819/2336)\n",
            "Epoch 48 Step 73/313 Test Loss: 2.018 | Test Acc: 35.051% (830/2368)\n",
            "Epoch 48 Step 74/313 Test Loss: 2.016 | Test Acc: 35.000% (840/2400)\n",
            "Epoch 48 Step 75/313 Test Loss: 2.018 | Test Acc: 34.951% (850/2432)\n",
            "Epoch 48 Step 76/313 Test Loss: 2.017 | Test Acc: 34.984% (862/2464)\n",
            "Epoch 48 Step 77/313 Test Loss: 2.014 | Test Acc: 35.176% (878/2496)\n",
            "Epoch 48 Step 78/313 Test Loss: 2.018 | Test Acc: 35.047% (886/2528)\n",
            "Epoch 48 Step 79/313 Test Loss: 2.019 | Test Acc: 35.039% (897/2560)\n",
            "Epoch 48 Step 80/313 Test Loss: 2.019 | Test Acc: 35.069% (909/2592)\n",
            "Epoch 48 Step 81/313 Test Loss: 2.019 | Test Acc: 35.061% (920/2624)\n",
            "Epoch 48 Step 82/313 Test Loss: 2.024 | Test Acc: 35.090% (932/2656)\n",
            "Epoch 48 Step 83/313 Test Loss: 2.018 | Test Acc: 35.193% (946/2688)\n",
            "Epoch 48 Step 84/313 Test Loss: 2.016 | Test Acc: 35.147% (956/2720)\n",
            "Epoch 48 Step 85/313 Test Loss: 2.018 | Test Acc: 35.138% (967/2752)\n",
            "Epoch 48 Step 86/313 Test Loss: 2.018 | Test Acc: 35.201% (980/2784)\n",
            "Epoch 48 Step 87/313 Test Loss: 2.017 | Test Acc: 35.156% (990/2816)\n",
            "Epoch 48 Step 88/313 Test Loss: 2.016 | Test Acc: 35.183% (1002/2848)\n",
            "Epoch 48 Step 89/313 Test Loss: 2.015 | Test Acc: 35.069% (1010/2880)\n",
            "Epoch 48 Step 90/313 Test Loss: 2.018 | Test Acc: 34.959% (1018/2912)\n",
            "Epoch 48 Step 91/313 Test Loss: 2.015 | Test Acc: 35.020% (1031/2944)\n",
            "Epoch 48 Step 92/313 Test Loss: 2.016 | Test Acc: 35.013% (1042/2976)\n",
            "Epoch 48 Step 93/313 Test Loss: 2.014 | Test Acc: 34.940% (1051/3008)\n",
            "Epoch 48 Step 94/313 Test Loss: 2.012 | Test Acc: 35.132% (1068/3040)\n",
            "Epoch 48 Step 95/313 Test Loss: 2.009 | Test Acc: 35.319% (1085/3072)\n",
            "Epoch 48 Step 96/313 Test Loss: 2.007 | Test Acc: 35.341% (1097/3104)\n",
            "Epoch 48 Step 97/313 Test Loss: 2.013 | Test Acc: 35.204% (1104/3136)\n",
            "Epoch 48 Step 98/313 Test Loss: 2.010 | Test Acc: 35.259% (1117/3168)\n",
            "Epoch 48 Step 99/313 Test Loss: 2.008 | Test Acc: 35.281% (1129/3200)\n",
            "Epoch 48 Step 100/313 Test Loss: 2.016 | Test Acc: 35.056% (1133/3232)\n",
            "Epoch 48 Step 101/313 Test Loss: 2.012 | Test Acc: 35.141% (1147/3264)\n",
            "Epoch 48 Step 102/313 Test Loss: 2.005 | Test Acc: 35.316% (1164/3296)\n",
            "Epoch 48 Step 103/313 Test Loss: 2.005 | Test Acc: 35.367% (1177/3328)\n",
            "Epoch 48 Step 104/313 Test Loss: 2.005 | Test Acc: 35.327% (1187/3360)\n",
            "Epoch 48 Step 105/313 Test Loss: 2.002 | Test Acc: 35.436% (1202/3392)\n",
            "Epoch 48 Step 106/313 Test Loss: 2.005 | Test Acc: 35.397% (1212/3424)\n",
            "Epoch 48 Step 107/313 Test Loss: 2.005 | Test Acc: 35.330% (1221/3456)\n",
            "Epoch 48 Step 108/313 Test Loss: 2.005 | Test Acc: 35.264% (1230/3488)\n",
            "Epoch 48 Step 109/313 Test Loss: 2.006 | Test Acc: 35.170% (1238/3520)\n",
            "Epoch 48 Step 110/313 Test Loss: 2.005 | Test Acc: 35.191% (1250/3552)\n",
            "Epoch 48 Step 111/313 Test Loss: 2.000 | Test Acc: 35.379% (1268/3584)\n",
            "Epoch 48 Step 112/313 Test Loss: 2.002 | Test Acc: 35.315% (1277/3616)\n",
            "Epoch 48 Step 113/313 Test Loss: 2.000 | Test Acc: 35.362% (1290/3648)\n",
            "Epoch 48 Step 114/313 Test Loss: 1.996 | Test Acc: 35.380% (1302/3680)\n",
            "Epoch 48 Step 115/313 Test Loss: 1.995 | Test Acc: 35.291% (1310/3712)\n",
            "Epoch 48 Step 116/313 Test Loss: 1.994 | Test Acc: 35.283% (1321/3744)\n",
            "Epoch 48 Step 117/313 Test Loss: 1.995 | Test Acc: 35.275% (1332/3776)\n",
            "Epoch 48 Step 118/313 Test Loss: 1.993 | Test Acc: 35.347% (1346/3808)\n",
            "Epoch 48 Step 119/313 Test Loss: 1.987 | Test Acc: 35.573% (1366/3840)\n",
            "Epoch 48 Step 120/313 Test Loss: 1.986 | Test Acc: 35.640% (1380/3872)\n",
            "Epoch 48 Step 121/313 Test Loss: 1.985 | Test Acc: 35.605% (1390/3904)\n",
            "Epoch 48 Step 122/313 Test Loss: 1.985 | Test Acc: 35.569% (1400/3936)\n",
            "Epoch 48 Step 123/313 Test Loss: 1.982 | Test Acc: 35.635% (1414/3968)\n",
            "Epoch 48 Step 124/313 Test Loss: 1.985 | Test Acc: 35.625% (1425/4000)\n",
            "Epoch 48 Step 125/313 Test Loss: 1.984 | Test Acc: 35.665% (1438/4032)\n",
            "Epoch 48 Step 126/313 Test Loss: 1.985 | Test Acc: 35.679% (1450/4064)\n",
            "Epoch 48 Step 127/313 Test Loss: 1.983 | Test Acc: 35.693% (1462/4096)\n",
            "Epoch 48 Step 128/313 Test Loss: 1.983 | Test Acc: 35.683% (1473/4128)\n",
            "Epoch 48 Step 129/313 Test Loss: 1.984 | Test Acc: 35.625% (1482/4160)\n",
            "Epoch 48 Step 130/313 Test Loss: 1.984 | Test Acc: 35.615% (1493/4192)\n",
            "Epoch 48 Step 131/313 Test Loss: 1.986 | Test Acc: 35.677% (1507/4224)\n",
            "Epoch 48 Step 132/313 Test Loss: 1.989 | Test Acc: 35.620% (1516/4256)\n",
            "Epoch 48 Step 133/313 Test Loss: 1.987 | Test Acc: 35.728% (1532/4288)\n",
            "Epoch 48 Step 134/313 Test Loss: 1.988 | Test Acc: 35.648% (1540/4320)\n",
            "Epoch 48 Step 135/313 Test Loss: 1.988 | Test Acc: 35.593% (1549/4352)\n",
            "Epoch 48 Step 136/313 Test Loss: 1.987 | Test Acc: 35.607% (1561/4384)\n",
            "Epoch 48 Step 137/313 Test Loss: 1.984 | Test Acc: 35.666% (1575/4416)\n",
            "Epoch 48 Step 138/313 Test Loss: 1.985 | Test Acc: 35.634% (1585/4448)\n",
            "Epoch 48 Step 139/313 Test Loss: 1.984 | Test Acc: 35.670% (1598/4480)\n",
            "Epoch 48 Step 140/313 Test Loss: 1.980 | Test Acc: 35.749% (1613/4512)\n",
            "Epoch 48 Step 141/313 Test Loss: 1.979 | Test Acc: 35.783% (1626/4544)\n",
            "Epoch 48 Step 142/313 Test Loss: 1.979 | Test Acc: 35.795% (1638/4576)\n",
            "Epoch 48 Step 143/313 Test Loss: 1.979 | Test Acc: 35.872% (1653/4608)\n",
            "Epoch 48 Step 144/313 Test Loss: 1.977 | Test Acc: 35.905% (1666/4640)\n",
            "Epoch 48 Step 145/313 Test Loss: 1.975 | Test Acc: 35.916% (1678/4672)\n",
            "Epoch 48 Step 146/313 Test Loss: 1.975 | Test Acc: 35.927% (1690/4704)\n",
            "Epoch 48 Step 147/313 Test Loss: 1.975 | Test Acc: 35.874% (1699/4736)\n",
            "Epoch 48 Step 148/313 Test Loss: 1.976 | Test Acc: 35.906% (1712/4768)\n",
            "Epoch 48 Step 149/313 Test Loss: 1.977 | Test Acc: 35.896% (1723/4800)\n",
            "Epoch 48 Step 150/313 Test Loss: 1.975 | Test Acc: 35.906% (1735/4832)\n",
            "Epoch 48 Step 151/313 Test Loss: 1.974 | Test Acc: 35.938% (1748/4864)\n",
            "Epoch 48 Step 152/313 Test Loss: 1.972 | Test Acc: 35.989% (1762/4896)\n",
            "Epoch 48 Step 153/313 Test Loss: 1.968 | Test Acc: 36.039% (1776/4928)\n",
            "Epoch 48 Step 154/313 Test Loss: 1.968 | Test Acc: 36.048% (1788/4960)\n",
            "Epoch 48 Step 155/313 Test Loss: 1.970 | Test Acc: 36.058% (1800/4992)\n",
            "Epoch 48 Step 156/313 Test Loss: 1.970 | Test Acc: 36.047% (1811/5024)\n",
            "Epoch 48 Step 157/313 Test Loss: 1.972 | Test Acc: 35.957% (1818/5056)\n",
            "Epoch 48 Step 158/313 Test Loss: 1.972 | Test Acc: 35.947% (1829/5088)\n",
            "Epoch 48 Step 159/313 Test Loss: 1.972 | Test Acc: 35.977% (1842/5120)\n",
            "Epoch 48 Step 160/313 Test Loss: 1.969 | Test Acc: 36.005% (1855/5152)\n",
            "Epoch 48 Step 161/313 Test Loss: 1.969 | Test Acc: 35.938% (1863/5184)\n",
            "Epoch 48 Step 162/313 Test Loss: 1.970 | Test Acc: 35.909% (1873/5216)\n",
            "Epoch 48 Step 163/313 Test Loss: 1.974 | Test Acc: 35.804% (1879/5248)\n",
            "Epoch 48 Step 164/313 Test Loss: 1.975 | Test Acc: 35.777% (1889/5280)\n",
            "Epoch 48 Step 165/313 Test Loss: 1.975 | Test Acc: 35.825% (1903/5312)\n",
            "Epoch 48 Step 166/313 Test Loss: 1.979 | Test Acc: 35.722% (1909/5344)\n",
            "Epoch 48 Step 167/313 Test Loss: 1.981 | Test Acc: 35.733% (1921/5376)\n",
            "Epoch 48 Step 168/313 Test Loss: 1.984 | Test Acc: 35.688% (1930/5408)\n",
            "Epoch 48 Step 169/313 Test Loss: 1.983 | Test Acc: 35.680% (1941/5440)\n",
            "Epoch 48 Step 170/313 Test Loss: 1.984 | Test Acc: 35.636% (1950/5472)\n",
            "Epoch 48 Step 171/313 Test Loss: 1.985 | Test Acc: 35.556% (1957/5504)\n",
            "Epoch 48 Step 172/313 Test Loss: 1.984 | Test Acc: 35.603% (1971/5536)\n",
            "Epoch 48 Step 173/313 Test Loss: 1.985 | Test Acc: 35.614% (1983/5568)\n",
            "Epoch 48 Step 174/313 Test Loss: 1.986 | Test Acc: 35.589% (1993/5600)\n",
            "Epoch 48 Step 175/313 Test Loss: 1.987 | Test Acc: 35.636% (2007/5632)\n",
            "Epoch 48 Step 176/313 Test Loss: 1.986 | Test Acc: 35.629% (2018/5664)\n",
            "Epoch 48 Step 177/313 Test Loss: 1.985 | Test Acc: 35.639% (2030/5696)\n",
            "Epoch 48 Step 178/313 Test Loss: 1.981 | Test Acc: 35.737% (2047/5728)\n",
            "Epoch 48 Step 179/313 Test Loss: 1.981 | Test Acc: 35.677% (2055/5760)\n",
            "Epoch 48 Step 180/313 Test Loss: 1.980 | Test Acc: 35.739% (2070/5792)\n",
            "Epoch 48 Step 181/313 Test Loss: 1.982 | Test Acc: 35.663% (2077/5824)\n",
            "Epoch 48 Step 182/313 Test Loss: 1.985 | Test Acc: 35.622% (2086/5856)\n",
            "Epoch 48 Step 183/313 Test Loss: 1.985 | Test Acc: 35.632% (2098/5888)\n",
            "Epoch 48 Step 184/313 Test Loss: 1.984 | Test Acc: 35.659% (2111/5920)\n",
            "Epoch 48 Step 185/313 Test Loss: 1.984 | Test Acc: 35.635% (2121/5952)\n",
            "Epoch 48 Step 186/313 Test Loss: 1.983 | Test Acc: 35.712% (2137/5984)\n",
            "Epoch 48 Step 187/313 Test Loss: 1.982 | Test Acc: 35.738% (2150/6016)\n",
            "Epoch 48 Step 188/313 Test Loss: 1.980 | Test Acc: 35.780% (2164/6048)\n",
            "Epoch 48 Step 189/313 Test Loss: 1.980 | Test Acc: 35.789% (2176/6080)\n",
            "Epoch 48 Step 190/313 Test Loss: 1.979 | Test Acc: 35.798% (2188/6112)\n",
            "Epoch 48 Step 191/313 Test Loss: 1.978 | Test Acc: 35.824% (2201/6144)\n",
            "Epoch 48 Step 192/313 Test Loss: 1.978 | Test Acc: 35.800% (2211/6176)\n",
            "Epoch 48 Step 193/313 Test Loss: 1.978 | Test Acc: 35.809% (2223/6208)\n",
            "Epoch 48 Step 194/313 Test Loss: 1.978 | Test Acc: 35.785% (2233/6240)\n",
            "Epoch 48 Step 195/313 Test Loss: 1.978 | Test Acc: 35.826% (2247/6272)\n",
            "Epoch 48 Step 196/313 Test Loss: 1.977 | Test Acc: 35.850% (2260/6304)\n",
            "Epoch 48 Step 197/313 Test Loss: 1.976 | Test Acc: 35.874% (2273/6336)\n",
            "Epoch 48 Step 198/313 Test Loss: 1.973 | Test Acc: 35.977% (2291/6368)\n",
            "Epoch 48 Step 199/313 Test Loss: 1.972 | Test Acc: 35.969% (2302/6400)\n",
            "Epoch 48 Step 200/313 Test Loss: 1.973 | Test Acc: 35.961% (2313/6432)\n",
            "Epoch 48 Step 201/313 Test Loss: 1.974 | Test Acc: 35.938% (2323/6464)\n",
            "Epoch 48 Step 202/313 Test Loss: 1.975 | Test Acc: 35.930% (2334/6496)\n",
            "Epoch 48 Step 203/313 Test Loss: 1.975 | Test Acc: 35.892% (2343/6528)\n",
            "Epoch 48 Step 204/313 Test Loss: 1.977 | Test Acc: 35.869% (2353/6560)\n",
            "Epoch 48 Step 205/313 Test Loss: 1.979 | Test Acc: 35.801% (2360/6592)\n",
            "Epoch 48 Step 206/313 Test Loss: 1.980 | Test Acc: 35.809% (2372/6624)\n",
            "Epoch 48 Step 207/313 Test Loss: 1.980 | Test Acc: 35.787% (2382/6656)\n",
            "Epoch 48 Step 208/313 Test Loss: 1.980 | Test Acc: 35.795% (2394/6688)\n",
            "Epoch 48 Step 209/313 Test Loss: 1.981 | Test Acc: 35.729% (2401/6720)\n",
            "Epoch 48 Step 210/313 Test Loss: 1.979 | Test Acc: 35.708% (2411/6752)\n",
            "Epoch 48 Step 211/313 Test Loss: 1.980 | Test Acc: 35.657% (2419/6784)\n",
            "Epoch 48 Step 212/313 Test Loss: 1.978 | Test Acc: 35.710% (2434/6816)\n",
            "Epoch 48 Step 213/313 Test Loss: 1.976 | Test Acc: 35.748% (2448/6848)\n",
            "Epoch 48 Step 214/313 Test Loss: 1.978 | Test Acc: 35.669% (2454/6880)\n",
            "Epoch 48 Step 215/313 Test Loss: 1.976 | Test Acc: 35.764% (2472/6912)\n",
            "Epoch 48 Step 216/313 Test Loss: 1.978 | Test Acc: 35.729% (2481/6944)\n",
            "Epoch 48 Step 217/313 Test Loss: 1.979 | Test Acc: 35.708% (2491/6976)\n",
            "Epoch 48 Step 218/313 Test Loss: 1.981 | Test Acc: 35.659% (2499/7008)\n",
            "Epoch 48 Step 219/313 Test Loss: 1.982 | Test Acc: 35.639% (2509/7040)\n",
            "Epoch 48 Step 220/313 Test Loss: 1.985 | Test Acc: 35.591% (2517/7072)\n",
            "Epoch 48 Step 221/313 Test Loss: 1.985 | Test Acc: 35.586% (2528/7104)\n",
            "Epoch 48 Step 222/313 Test Loss: 1.987 | Test Acc: 35.538% (2536/7136)\n",
            "Epoch 48 Step 223/313 Test Loss: 1.985 | Test Acc: 35.533% (2547/7168)\n",
            "Epoch 48 Step 224/313 Test Loss: 1.987 | Test Acc: 35.500% (2556/7200)\n",
            "Epoch 48 Step 225/313 Test Loss: 1.987 | Test Acc: 35.495% (2567/7232)\n",
            "Epoch 48 Step 226/313 Test Loss: 1.986 | Test Acc: 35.476% (2577/7264)\n",
            "Epoch 48 Step 227/313 Test Loss: 1.984 | Test Acc: 35.485% (2589/7296)\n",
            "Epoch 48 Step 228/313 Test Loss: 1.984 | Test Acc: 35.494% (2601/7328)\n",
            "Epoch 48 Step 229/313 Test Loss: 1.985 | Test Acc: 35.448% (2609/7360)\n",
            "Epoch 48 Step 230/313 Test Loss: 1.984 | Test Acc: 35.471% (2622/7392)\n",
            "Epoch 48 Step 231/313 Test Loss: 1.984 | Test Acc: 35.493% (2635/7424)\n",
            "Epoch 48 Step 232/313 Test Loss: 1.983 | Test Acc: 35.461% (2644/7456)\n",
            "Epoch 48 Step 233/313 Test Loss: 1.983 | Test Acc: 35.483% (2657/7488)\n",
            "Epoch 48 Step 234/313 Test Loss: 1.982 | Test Acc: 35.492% (2669/7520)\n",
            "Epoch 48 Step 235/313 Test Loss: 1.982 | Test Acc: 35.448% (2677/7552)\n",
            "Epoch 48 Step 236/313 Test Loss: 1.984 | Test Acc: 35.403% (2685/7584)\n",
            "Epoch 48 Step 237/313 Test Loss: 1.983 | Test Acc: 35.412% (2697/7616)\n",
            "Epoch 48 Step 238/313 Test Loss: 1.984 | Test Acc: 35.408% (2708/7648)\n",
            "Epoch 48 Step 239/313 Test Loss: 1.983 | Test Acc: 35.391% (2718/7680)\n",
            "Epoch 48 Step 240/313 Test Loss: 1.982 | Test Acc: 35.464% (2735/7712)\n",
            "Epoch 48 Step 241/313 Test Loss: 1.981 | Test Acc: 35.486% (2748/7744)\n",
            "Epoch 48 Step 242/313 Test Loss: 1.981 | Test Acc: 35.481% (2759/7776)\n",
            "Epoch 48 Step 243/313 Test Loss: 1.979 | Test Acc: 35.476% (2770/7808)\n",
            "Epoch 48 Step 244/313 Test Loss: 1.980 | Test Acc: 35.472% (2781/7840)\n",
            "Epoch 48 Step 245/313 Test Loss: 1.980 | Test Acc: 35.556% (2799/7872)\n",
            "Epoch 48 Step 246/313 Test Loss: 1.981 | Test Acc: 35.552% (2810/7904)\n",
            "Epoch 48 Step 247/313 Test Loss: 1.981 | Test Acc: 35.522% (2819/7936)\n",
            "Epoch 48 Step 248/313 Test Loss: 1.982 | Test Acc: 35.492% (2828/7968)\n",
            "Epoch 48 Step 249/313 Test Loss: 1.981 | Test Acc: 35.500% (2840/8000)\n",
            "Epoch 48 Step 250/313 Test Loss: 1.979 | Test Acc: 35.558% (2856/8032)\n",
            "Epoch 48 Step 251/313 Test Loss: 1.980 | Test Acc: 35.578% (2869/8064)\n",
            "Epoch 48 Step 252/313 Test Loss: 1.979 | Test Acc: 35.561% (2879/8096)\n",
            "Epoch 48 Step 253/313 Test Loss: 1.980 | Test Acc: 35.544% (2889/8128)\n",
            "Epoch 48 Step 254/313 Test Loss: 1.981 | Test Acc: 35.539% (2900/8160)\n",
            "Epoch 48 Step 255/313 Test Loss: 1.980 | Test Acc: 35.522% (2910/8192)\n",
            "Epoch 48 Step 256/313 Test Loss: 1.981 | Test Acc: 35.494% (2919/8224)\n",
            "Epoch 48 Step 257/313 Test Loss: 1.981 | Test Acc: 35.501% (2931/8256)\n",
            "Epoch 48 Step 258/313 Test Loss: 1.981 | Test Acc: 35.497% (2942/8288)\n",
            "Epoch 48 Step 259/313 Test Loss: 1.982 | Test Acc: 35.433% (2948/8320)\n",
            "Epoch 48 Step 260/313 Test Loss: 1.982 | Test Acc: 35.441% (2960/8352)\n",
            "Epoch 48 Step 261/313 Test Loss: 1.981 | Test Acc: 35.460% (2973/8384)\n",
            "Epoch 48 Step 262/313 Test Loss: 1.980 | Test Acc: 35.456% (2984/8416)\n",
            "Epoch 48 Step 263/313 Test Loss: 1.980 | Test Acc: 35.452% (2995/8448)\n",
            "Epoch 48 Step 264/313 Test Loss: 1.981 | Test Acc: 35.401% (3002/8480)\n",
            "Epoch 48 Step 265/313 Test Loss: 1.982 | Test Acc: 35.397% (3013/8512)\n",
            "Epoch 48 Step 266/313 Test Loss: 1.983 | Test Acc: 35.417% (3026/8544)\n",
            "Epoch 48 Step 267/313 Test Loss: 1.984 | Test Acc: 35.366% (3033/8576)\n",
            "Epoch 48 Step 268/313 Test Loss: 1.983 | Test Acc: 35.421% (3049/8608)\n",
            "Epoch 48 Step 269/313 Test Loss: 1.982 | Test Acc: 35.451% (3063/8640)\n",
            "Epoch 48 Step 270/313 Test Loss: 1.983 | Test Acc: 35.424% (3072/8672)\n",
            "Epoch 48 Step 271/313 Test Loss: 1.984 | Test Acc: 35.398% (3081/8704)\n",
            "Epoch 48 Step 272/313 Test Loss: 1.984 | Test Acc: 35.382% (3091/8736)\n",
            "Epoch 48 Step 273/313 Test Loss: 1.984 | Test Acc: 35.413% (3105/8768)\n",
            "Epoch 48 Step 274/313 Test Loss: 1.983 | Test Acc: 35.409% (3116/8800)\n",
            "Epoch 48 Step 275/313 Test Loss: 1.983 | Test Acc: 35.394% (3126/8832)\n",
            "Epoch 48 Step 276/313 Test Loss: 1.984 | Test Acc: 35.356% (3134/8864)\n",
            "Epoch 48 Step 277/313 Test Loss: 1.984 | Test Acc: 35.364% (3146/8896)\n",
            "Epoch 48 Step 278/313 Test Loss: 1.982 | Test Acc: 35.428% (3163/8928)\n",
            "Epoch 48 Step 279/313 Test Loss: 1.982 | Test Acc: 35.446% (3176/8960)\n",
            "Epoch 48 Step 280/313 Test Loss: 1.983 | Test Acc: 35.409% (3184/8992)\n",
            "Epoch 48 Step 281/313 Test Loss: 1.983 | Test Acc: 35.406% (3195/9024)\n",
            "Epoch 48 Step 282/313 Test Loss: 1.983 | Test Acc: 35.402% (3206/9056)\n",
            "Epoch 48 Step 283/313 Test Loss: 1.982 | Test Acc: 35.442% (3221/9088)\n",
            "Epoch 48 Step 284/313 Test Loss: 1.982 | Test Acc: 35.482% (3236/9120)\n",
            "Epoch 48 Step 285/313 Test Loss: 1.983 | Test Acc: 35.490% (3248/9152)\n",
            "Epoch 48 Step 286/313 Test Loss: 1.982 | Test Acc: 35.497% (3260/9184)\n",
            "Epoch 48 Step 287/313 Test Loss: 1.980 | Test Acc: 35.547% (3276/9216)\n",
            "Epoch 48 Step 288/313 Test Loss: 1.979 | Test Acc: 35.554% (3288/9248)\n",
            "Epoch 48 Step 289/313 Test Loss: 1.979 | Test Acc: 35.539% (3298/9280)\n",
            "Epoch 48 Step 290/313 Test Loss: 1.979 | Test Acc: 35.546% (3310/9312)\n",
            "Epoch 48 Step 291/313 Test Loss: 1.979 | Test Acc: 35.563% (3323/9344)\n",
            "Epoch 48 Step 292/313 Test Loss: 1.978 | Test Acc: 35.570% (3335/9376)\n",
            "Epoch 48 Step 293/313 Test Loss: 1.978 | Test Acc: 35.576% (3347/9408)\n",
            "Epoch 48 Step 294/313 Test Loss: 1.977 | Test Acc: 35.561% (3357/9440)\n",
            "Epoch 48 Step 295/313 Test Loss: 1.978 | Test Acc: 35.547% (3367/9472)\n",
            "Epoch 48 Step 296/313 Test Loss: 1.978 | Test Acc: 35.585% (3382/9504)\n",
            "Epoch 48 Step 297/313 Test Loss: 1.978 | Test Acc: 35.591% (3394/9536)\n",
            "Epoch 48 Step 298/313 Test Loss: 1.978 | Test Acc: 35.598% (3406/9568)\n",
            "Epoch 48 Step 299/313 Test Loss: 1.977 | Test Acc: 35.625% (3420/9600)\n",
            "Epoch 48 Step 300/313 Test Loss: 1.977 | Test Acc: 35.610% (3430/9632)\n",
            "Epoch 48 Step 301/313 Test Loss: 1.977 | Test Acc: 35.606% (3441/9664)\n",
            "Epoch 48 Step 302/313 Test Loss: 1.979 | Test Acc: 35.561% (3448/9696)\n",
            "Epoch 48 Step 303/313 Test Loss: 1.978 | Test Acc: 35.588% (3462/9728)\n",
            "Epoch 48 Step 304/313 Test Loss: 1.979 | Test Acc: 35.553% (3470/9760)\n",
            "Epoch 48 Step 305/313 Test Loss: 1.980 | Test Acc: 35.570% (3483/9792)\n",
            "Epoch 48 Step 306/313 Test Loss: 1.980 | Test Acc: 35.596% (3497/9824)\n",
            "Epoch 48 Step 307/313 Test Loss: 1.980 | Test Acc: 35.593% (3508/9856)\n",
            "Epoch 48 Step 308/313 Test Loss: 1.981 | Test Acc: 35.548% (3515/9888)\n",
            "Epoch 48 Step 309/313 Test Loss: 1.982 | Test Acc: 35.504% (3522/9920)\n",
            "Epoch 48 Step 310/313 Test Loss: 1.983 | Test Acc: 35.520% (3535/9952)\n",
            "Epoch 48 Step 311/313 Test Loss: 1.982 | Test Acc: 35.517% (3546/9984)\n",
            "Epoch 48 Step 312/313 Test Loss: 1.981 | Test Acc: 35.530% (3553/10000)\n",
            "\n",
            "Epoch: 49\n",
            "Epoch 49 Step 0/1563 Loss: 2.172 | Acc: 25.000% (8/32)\n",
            "Epoch 49 Step 1/1563 Loss: 2.035 | Acc: 23.438% (15/64)\n",
            "Epoch 49 Step 2/1563 Loss: 2.035 | Acc: 28.125% (27/96)\n",
            "Epoch 49 Step 3/1563 Loss: 2.087 | Acc: 26.562% (34/128)\n",
            "Epoch 49 Step 4/1563 Loss: 2.170 | Acc: 26.250% (42/160)\n",
            "Epoch 49 Step 5/1563 Loss: 2.183 | Acc: 27.083% (52/192)\n",
            "Epoch 49 Step 6/1563 Loss: 2.119 | Acc: 29.911% (67/224)\n",
            "Epoch 49 Step 7/1563 Loss: 2.099 | Acc: 30.469% (78/256)\n",
            "Epoch 49 Step 8/1563 Loss: 2.099 | Acc: 30.208% (87/288)\n",
            "Epoch 49 Step 9/1563 Loss: 2.083 | Acc: 30.625% (98/320)\n",
            "Epoch 49 Step 10/1563 Loss: 2.099 | Acc: 30.114% (106/352)\n",
            "Epoch 49 Step 11/1563 Loss: 2.109 | Acc: 29.688% (114/384)\n",
            "Epoch 49 Step 12/1563 Loss: 2.113 | Acc: 29.087% (121/416)\n",
            "Epoch 49 Step 13/1563 Loss: 2.115 | Acc: 29.018% (130/448)\n",
            "Epoch 49 Step 14/1563 Loss: 2.128 | Acc: 29.167% (140/480)\n",
            "Epoch 49 Step 15/1563 Loss: 2.136 | Acc: 29.688% (152/512)\n",
            "Epoch 49 Step 16/1563 Loss: 2.139 | Acc: 29.779% (162/544)\n",
            "Epoch 49 Step 17/1563 Loss: 2.116 | Acc: 30.035% (173/576)\n",
            "Epoch 49 Step 18/1563 Loss: 2.100 | Acc: 30.921% (188/608)\n",
            "Epoch 49 Step 19/1563 Loss: 2.092 | Acc: 31.406% (201/640)\n",
            "Epoch 49 Step 20/1563 Loss: 2.112 | Acc: 30.655% (206/672)\n",
            "Epoch 49 Step 21/1563 Loss: 2.126 | Acc: 30.540% (215/704)\n",
            "Epoch 49 Step 22/1563 Loss: 2.141 | Acc: 30.571% (225/736)\n",
            "Epoch 49 Step 23/1563 Loss: 2.123 | Acc: 31.120% (239/768)\n",
            "Epoch 49 Step 24/1563 Loss: 2.113 | Acc: 31.125% (249/800)\n",
            "Epoch 49 Step 25/1563 Loss: 2.107 | Acc: 31.010% (258/832)\n",
            "Epoch 49 Step 26/1563 Loss: 2.114 | Acc: 31.019% (268/864)\n",
            "Epoch 49 Step 27/1563 Loss: 2.108 | Acc: 30.804% (276/896)\n",
            "Epoch 49 Step 28/1563 Loss: 2.109 | Acc: 30.819% (286/928)\n",
            "Epoch 49 Step 29/1563 Loss: 2.112 | Acc: 30.625% (294/960)\n",
            "Epoch 49 Step 30/1563 Loss: 2.111 | Acc: 30.444% (302/992)\n",
            "Epoch 49 Step 31/1563 Loss: 2.127 | Acc: 30.078% (308/1024)\n",
            "Epoch 49 Step 32/1563 Loss: 2.111 | Acc: 30.492% (322/1056)\n",
            "Epoch 49 Step 33/1563 Loss: 2.121 | Acc: 30.331% (330/1088)\n",
            "Epoch 49 Step 34/1563 Loss: 2.110 | Acc: 30.536% (342/1120)\n",
            "Epoch 49 Step 35/1563 Loss: 2.119 | Acc: 30.295% (349/1152)\n",
            "Epoch 49 Step 36/1563 Loss: 2.113 | Acc: 30.490% (361/1184)\n",
            "Epoch 49 Step 37/1563 Loss: 2.115 | Acc: 30.674% (373/1216)\n",
            "Epoch 49 Step 38/1563 Loss: 2.114 | Acc: 30.929% (386/1248)\n",
            "Epoch 49 Step 39/1563 Loss: 2.118 | Acc: 31.016% (397/1280)\n",
            "Epoch 49 Step 40/1563 Loss: 2.120 | Acc: 31.250% (410/1312)\n",
            "Epoch 49 Step 41/1563 Loss: 2.123 | Acc: 31.250% (420/1344)\n",
            "Epoch 49 Step 42/1563 Loss: 2.114 | Acc: 31.468% (433/1376)\n",
            "Epoch 49 Step 43/1563 Loss: 2.106 | Acc: 31.818% (448/1408)\n",
            "Epoch 49 Step 44/1563 Loss: 2.116 | Acc: 31.736% (457/1440)\n",
            "Epoch 49 Step 45/1563 Loss: 2.116 | Acc: 31.793% (468/1472)\n",
            "Epoch 49 Step 46/1563 Loss: 2.114 | Acc: 31.848% (479/1504)\n",
            "Epoch 49 Step 47/1563 Loss: 2.110 | Acc: 31.966% (491/1536)\n",
            "Epoch 49 Step 48/1563 Loss: 2.108 | Acc: 32.015% (502/1568)\n",
            "Epoch 49 Step 49/1563 Loss: 2.105 | Acc: 32.250% (516/1600)\n",
            "Epoch 49 Step 50/1563 Loss: 2.094 | Acc: 32.353% (528/1632)\n",
            "Epoch 49 Step 51/1563 Loss: 2.089 | Acc: 32.512% (541/1664)\n",
            "Epoch 49 Step 52/1563 Loss: 2.081 | Acc: 32.842% (557/1696)\n",
            "Epoch 49 Step 53/1563 Loss: 2.083 | Acc: 32.870% (568/1728)\n",
            "Epoch 49 Step 54/1563 Loss: 2.088 | Acc: 32.898% (579/1760)\n",
            "Epoch 49 Step 55/1563 Loss: 2.086 | Acc: 32.812% (588/1792)\n",
            "Epoch 49 Step 56/1563 Loss: 2.090 | Acc: 32.566% (594/1824)\n",
            "Epoch 49 Step 57/1563 Loss: 2.092 | Acc: 32.597% (605/1856)\n",
            "Epoch 49 Step 58/1563 Loss: 2.089 | Acc: 32.786% (619/1888)\n",
            "Epoch 49 Step 59/1563 Loss: 2.087 | Acc: 32.917% (632/1920)\n",
            "Epoch 49 Step 60/1563 Loss: 2.087 | Acc: 32.992% (644/1952)\n",
            "Epoch 49 Step 61/1563 Loss: 2.087 | Acc: 33.065% (656/1984)\n",
            "Epoch 49 Step 62/1563 Loss: 2.086 | Acc: 33.085% (667/2016)\n",
            "Epoch 49 Step 63/1563 Loss: 2.086 | Acc: 33.203% (680/2048)\n",
            "Epoch 49 Step 64/1563 Loss: 2.089 | Acc: 33.125% (689/2080)\n",
            "Epoch 49 Step 65/1563 Loss: 2.092 | Acc: 33.097% (699/2112)\n",
            "Epoch 49 Step 66/1563 Loss: 2.088 | Acc: 33.116% (710/2144)\n",
            "Epoch 49 Step 67/1563 Loss: 2.094 | Acc: 32.996% (718/2176)\n",
            "Epoch 49 Step 68/1563 Loss: 2.094 | Acc: 32.926% (727/2208)\n",
            "Epoch 49 Step 69/1563 Loss: 2.096 | Acc: 32.857% (736/2240)\n",
            "Epoch 49 Step 70/1563 Loss: 2.092 | Acc: 32.835% (746/2272)\n",
            "Epoch 49 Step 71/1563 Loss: 2.094 | Acc: 32.856% (757/2304)\n",
            "Epoch 49 Step 72/1563 Loss: 2.094 | Acc: 32.920% (769/2336)\n",
            "Epoch 49 Step 73/1563 Loss: 2.092 | Acc: 32.855% (778/2368)\n",
            "Epoch 49 Step 74/1563 Loss: 2.090 | Acc: 32.875% (789/2400)\n",
            "Epoch 49 Step 75/1563 Loss: 2.090 | Acc: 32.812% (798/2432)\n",
            "Epoch 49 Step 76/1563 Loss: 2.093 | Acc: 32.752% (807/2464)\n",
            "Epoch 49 Step 77/1563 Loss: 2.090 | Acc: 32.812% (819/2496)\n",
            "Epoch 49 Step 78/1563 Loss: 2.090 | Acc: 32.793% (829/2528)\n",
            "Epoch 49 Step 79/1563 Loss: 2.092 | Acc: 32.734% (838/2560)\n",
            "Epoch 49 Step 80/1563 Loss: 2.093 | Acc: 32.716% (848/2592)\n",
            "Epoch 49 Step 81/1563 Loss: 2.091 | Acc: 32.736% (859/2624)\n",
            "Epoch 49 Step 82/1563 Loss: 2.087 | Acc: 32.756% (870/2656)\n",
            "Epoch 49 Step 83/1563 Loss: 2.090 | Acc: 32.626% (877/2688)\n",
            "Epoch 49 Step 84/1563 Loss: 2.084 | Acc: 32.757% (891/2720)\n",
            "Epoch 49 Step 85/1563 Loss: 2.086 | Acc: 32.885% (905/2752)\n",
            "Epoch 49 Step 86/1563 Loss: 2.089 | Acc: 32.866% (915/2784)\n",
            "Epoch 49 Step 87/1563 Loss: 2.090 | Acc: 32.706% (921/2816)\n",
            "Epoch 49 Step 88/1563 Loss: 2.089 | Acc: 32.795% (934/2848)\n",
            "Epoch 49 Step 89/1563 Loss: 2.088 | Acc: 32.778% (944/2880)\n",
            "Epoch 49 Step 90/1563 Loss: 2.090 | Acc: 32.761% (954/2912)\n",
            "Epoch 49 Step 91/1563 Loss: 2.092 | Acc: 32.609% (960/2944)\n",
            "Epoch 49 Step 92/1563 Loss: 2.090 | Acc: 32.695% (973/2976)\n",
            "Epoch 49 Step 93/1563 Loss: 2.095 | Acc: 32.646% (982/3008)\n",
            "Epoch 49 Step 94/1563 Loss: 2.095 | Acc: 32.664% (993/3040)\n",
            "Epoch 49 Step 95/1563 Loss: 2.098 | Acc: 32.617% (1002/3072)\n",
            "Epoch 49 Step 96/1563 Loss: 2.100 | Acc: 32.635% (1013/3104)\n",
            "Epoch 49 Step 97/1563 Loss: 2.102 | Acc: 32.653% (1024/3136)\n",
            "Epoch 49 Step 98/1563 Loss: 2.106 | Acc: 32.607% (1033/3168)\n",
            "Epoch 49 Step 99/1563 Loss: 2.104 | Acc: 32.594% (1043/3200)\n",
            "Epoch 49 Step 100/1563 Loss: 2.104 | Acc: 32.519% (1051/3232)\n",
            "Epoch 49 Step 101/1563 Loss: 2.103 | Acc: 32.445% (1059/3264)\n",
            "Epoch 49 Step 102/1563 Loss: 2.102 | Acc: 32.464% (1070/3296)\n",
            "Epoch 49 Step 103/1563 Loss: 2.102 | Acc: 32.452% (1080/3328)\n",
            "Epoch 49 Step 104/1563 Loss: 2.100 | Acc: 32.411% (1089/3360)\n",
            "Epoch 49 Step 105/1563 Loss: 2.102 | Acc: 32.311% (1096/3392)\n",
            "Epoch 49 Step 106/1563 Loss: 2.101 | Acc: 32.243% (1104/3424)\n",
            "Epoch 49 Step 107/1563 Loss: 2.102 | Acc: 32.234% (1114/3456)\n",
            "Epoch 49 Step 108/1563 Loss: 2.101 | Acc: 32.196% (1123/3488)\n",
            "Epoch 49 Step 109/1563 Loss: 2.100 | Acc: 32.216% (1134/3520)\n",
            "Epoch 49 Step 110/1563 Loss: 2.103 | Acc: 32.095% (1140/3552)\n",
            "Epoch 49 Step 111/1563 Loss: 2.099 | Acc: 32.227% (1155/3584)\n",
            "Epoch 49 Step 112/1563 Loss: 2.101 | Acc: 32.163% (1163/3616)\n",
            "Epoch 49 Step 113/1563 Loss: 2.105 | Acc: 32.155% (1173/3648)\n",
            "Epoch 49 Step 114/1563 Loss: 2.106 | Acc: 32.092% (1181/3680)\n",
            "Epoch 49 Step 115/1563 Loss: 2.113 | Acc: 31.950% (1186/3712)\n",
            "Epoch 49 Step 116/1563 Loss: 2.109 | Acc: 32.025% (1199/3744)\n",
            "Epoch 49 Step 117/1563 Loss: 2.111 | Acc: 32.044% (1210/3776)\n",
            "Epoch 49 Step 118/1563 Loss: 2.111 | Acc: 32.117% (1223/3808)\n",
            "Epoch 49 Step 119/1563 Loss: 2.110 | Acc: 32.161% (1235/3840)\n",
            "Epoch 49 Step 120/1563 Loss: 2.112 | Acc: 32.102% (1243/3872)\n",
            "Epoch 49 Step 121/1563 Loss: 2.111 | Acc: 32.172% (1256/3904)\n",
            "Epoch 49 Step 122/1563 Loss: 2.110 | Acc: 32.215% (1268/3936)\n",
            "Epoch 49 Step 123/1563 Loss: 2.105 | Acc: 32.359% (1284/3968)\n",
            "Epoch 49 Step 124/1563 Loss: 2.108 | Acc: 32.325% (1293/4000)\n",
            "Epoch 49 Step 125/1563 Loss: 2.107 | Acc: 32.316% (1303/4032)\n",
            "Epoch 49 Step 126/1563 Loss: 2.106 | Acc: 32.308% (1313/4064)\n",
            "Epoch 49 Step 127/1563 Loss: 2.106 | Acc: 32.373% (1326/4096)\n",
            "Epoch 49 Step 128/1563 Loss: 2.103 | Acc: 32.437% (1339/4128)\n",
            "Epoch 49 Step 129/1563 Loss: 2.107 | Acc: 32.332% (1345/4160)\n",
            "Epoch 49 Step 130/1563 Loss: 2.108 | Acc: 32.276% (1353/4192)\n",
            "Epoch 49 Step 131/1563 Loss: 2.111 | Acc: 32.197% (1360/4224)\n",
            "Epoch 49 Step 132/1563 Loss: 2.110 | Acc: 32.260% (1373/4256)\n",
            "Epoch 49 Step 133/1563 Loss: 2.111 | Acc: 32.229% (1382/4288)\n",
            "Epoch 49 Step 134/1563 Loss: 2.110 | Acc: 32.269% (1394/4320)\n",
            "Epoch 49 Step 135/1563 Loss: 2.112 | Acc: 32.261% (1404/4352)\n",
            "Epoch 49 Step 136/1563 Loss: 2.111 | Acc: 32.162% (1410/4384)\n",
            "Epoch 49 Step 137/1563 Loss: 2.112 | Acc: 32.111% (1418/4416)\n",
            "Epoch 49 Step 138/1563 Loss: 2.112 | Acc: 32.104% (1428/4448)\n",
            "Epoch 49 Step 139/1563 Loss: 2.113 | Acc: 32.076% (1437/4480)\n",
            "Epoch 49 Step 140/1563 Loss: 2.111 | Acc: 32.159% (1451/4512)\n",
            "Epoch 49 Step 141/1563 Loss: 2.111 | Acc: 32.174% (1462/4544)\n",
            "Epoch 49 Step 142/1563 Loss: 2.109 | Acc: 32.102% (1469/4576)\n",
            "Epoch 49 Step 143/1563 Loss: 2.109 | Acc: 32.031% (1476/4608)\n",
            "Epoch 49 Step 144/1563 Loss: 2.109 | Acc: 32.112% (1490/4640)\n",
            "Epoch 49 Step 145/1563 Loss: 2.108 | Acc: 32.170% (1503/4672)\n",
            "Epoch 49 Step 146/1563 Loss: 2.106 | Acc: 32.207% (1515/4704)\n",
            "Epoch 49 Step 147/1563 Loss: 2.107 | Acc: 32.095% (1520/4736)\n",
            "Epoch 49 Step 148/1563 Loss: 2.104 | Acc: 32.173% (1534/4768)\n",
            "Epoch 49 Step 149/1563 Loss: 2.106 | Acc: 32.125% (1542/4800)\n",
            "Epoch 49 Step 150/1563 Loss: 2.107 | Acc: 32.119% (1552/4832)\n",
            "Epoch 49 Step 151/1563 Loss: 2.107 | Acc: 32.072% (1560/4864)\n",
            "Epoch 49 Step 152/1563 Loss: 2.107 | Acc: 32.108% (1572/4896)\n",
            "Epoch 49 Step 153/1563 Loss: 2.106 | Acc: 32.123% (1583/4928)\n",
            "Epoch 49 Step 154/1563 Loss: 2.106 | Acc: 32.218% (1598/4960)\n",
            "Epoch 49 Step 155/1563 Loss: 2.109 | Acc: 32.151% (1605/4992)\n",
            "Epoch 49 Step 156/1563 Loss: 2.110 | Acc: 32.146% (1615/5024)\n",
            "Epoch 49 Step 157/1563 Loss: 2.110 | Acc: 32.081% (1622/5056)\n",
            "Epoch 49 Step 158/1563 Loss: 2.110 | Acc: 32.075% (1632/5088)\n",
            "Epoch 49 Step 159/1563 Loss: 2.109 | Acc: 32.051% (1641/5120)\n",
            "Epoch 49 Step 160/1563 Loss: 2.109 | Acc: 31.988% (1648/5152)\n",
            "Epoch 49 Step 161/1563 Loss: 2.112 | Acc: 31.925% (1655/5184)\n",
            "Epoch 49 Step 162/1563 Loss: 2.113 | Acc: 31.921% (1665/5216)\n",
            "Epoch 49 Step 163/1563 Loss: 2.115 | Acc: 31.936% (1676/5248)\n",
            "Epoch 49 Step 164/1563 Loss: 2.117 | Acc: 31.856% (1682/5280)\n",
            "Epoch 49 Step 165/1563 Loss: 2.116 | Acc: 31.834% (1691/5312)\n",
            "Epoch 49 Step 166/1563 Loss: 2.115 | Acc: 31.868% (1703/5344)\n",
            "Epoch 49 Step 167/1563 Loss: 2.115 | Acc: 31.808% (1710/5376)\n",
            "Epoch 49 Step 168/1563 Loss: 2.116 | Acc: 31.805% (1720/5408)\n",
            "Epoch 49 Step 169/1563 Loss: 2.116 | Acc: 31.838% (1732/5440)\n",
            "Epoch 49 Step 170/1563 Loss: 2.116 | Acc: 31.835% (1742/5472)\n",
            "Epoch 49 Step 171/1563 Loss: 2.116 | Acc: 31.795% (1750/5504)\n",
            "Epoch 49 Step 172/1563 Loss: 2.116 | Acc: 31.774% (1759/5536)\n",
            "Epoch 49 Step 173/1563 Loss: 2.114 | Acc: 31.807% (1771/5568)\n",
            "Epoch 49 Step 174/1563 Loss: 2.115 | Acc: 31.786% (1780/5600)\n",
            "Epoch 49 Step 175/1563 Loss: 2.113 | Acc: 31.818% (1792/5632)\n",
            "Epoch 49 Step 176/1563 Loss: 2.112 | Acc: 31.780% (1800/5664)\n",
            "Epoch 49 Step 177/1563 Loss: 2.112 | Acc: 31.777% (1810/5696)\n",
            "Epoch 49 Step 178/1563 Loss: 2.112 | Acc: 31.809% (1822/5728)\n",
            "Epoch 49 Step 179/1563 Loss: 2.110 | Acc: 31.892% (1837/5760)\n",
            "Epoch 49 Step 180/1563 Loss: 2.108 | Acc: 31.941% (1850/5792)\n",
            "Epoch 49 Step 181/1563 Loss: 2.109 | Acc: 31.937% (1860/5824)\n",
            "Epoch 49 Step 182/1563 Loss: 2.108 | Acc: 31.967% (1872/5856)\n",
            "Epoch 49 Step 183/1563 Loss: 2.107 | Acc: 31.963% (1882/5888)\n",
            "Epoch 49 Step 184/1563 Loss: 2.109 | Acc: 31.875% (1887/5920)\n",
            "Epoch 49 Step 185/1563 Loss: 2.108 | Acc: 31.905% (1899/5952)\n",
            "Epoch 49 Step 186/1563 Loss: 2.108 | Acc: 31.885% (1908/5984)\n",
            "Epoch 49 Step 187/1563 Loss: 2.108 | Acc: 31.882% (1918/6016)\n",
            "Epoch 49 Step 188/1563 Loss: 2.109 | Acc: 31.845% (1926/6048)\n",
            "Epoch 49 Step 189/1563 Loss: 2.108 | Acc: 31.859% (1937/6080)\n",
            "Epoch 49 Step 190/1563 Loss: 2.107 | Acc: 31.954% (1953/6112)\n",
            "Epoch 49 Step 191/1563 Loss: 2.107 | Acc: 31.901% (1960/6144)\n",
            "Epoch 49 Step 192/1563 Loss: 2.108 | Acc: 31.898% (1970/6176)\n",
            "Epoch 49 Step 193/1563 Loss: 2.109 | Acc: 31.878% (1979/6208)\n",
            "Epoch 49 Step 194/1563 Loss: 2.109 | Acc: 31.843% (1987/6240)\n",
            "Epoch 49 Step 195/1563 Loss: 2.109 | Acc: 31.840% (1997/6272)\n",
            "Epoch 49 Step 196/1563 Loss: 2.108 | Acc: 31.789% (2004/6304)\n",
            "Epoch 49 Step 197/1563 Loss: 2.109 | Acc: 31.755% (2012/6336)\n",
            "Epoch 49 Step 198/1563 Loss: 2.109 | Acc: 31.737% (2021/6368)\n",
            "Epoch 49 Step 199/1563 Loss: 2.106 | Acc: 31.766% (2033/6400)\n",
            "Epoch 49 Step 200/1563 Loss: 2.106 | Acc: 31.763% (2043/6432)\n",
            "Epoch 49 Step 201/1563 Loss: 2.107 | Acc: 31.714% (2050/6464)\n",
            "Epoch 49 Step 202/1563 Loss: 2.107 | Acc: 31.681% (2058/6496)\n",
            "Epoch 49 Step 203/1563 Loss: 2.107 | Acc: 31.633% (2065/6528)\n",
            "Epoch 49 Step 204/1563 Loss: 2.107 | Acc: 31.601% (2073/6560)\n",
            "Epoch 49 Step 205/1563 Loss: 2.104 | Acc: 31.690% (2089/6592)\n",
            "Epoch 49 Step 206/1563 Loss: 2.103 | Acc: 31.688% (2099/6624)\n",
            "Epoch 49 Step 207/1563 Loss: 2.103 | Acc: 31.686% (2109/6656)\n",
            "Epoch 49 Step 208/1563 Loss: 2.102 | Acc: 31.699% (2120/6688)\n",
            "Epoch 49 Step 209/1563 Loss: 2.102 | Acc: 31.711% (2131/6720)\n",
            "Epoch 49 Step 210/1563 Loss: 2.101 | Acc: 31.709% (2141/6752)\n",
            "Epoch 49 Step 211/1563 Loss: 2.102 | Acc: 31.707% (2151/6784)\n",
            "Epoch 49 Step 212/1563 Loss: 2.100 | Acc: 31.749% (2164/6816)\n",
            "Epoch 49 Step 213/1563 Loss: 2.100 | Acc: 31.746% (2174/6848)\n",
            "Epoch 49 Step 214/1563 Loss: 2.100 | Acc: 31.773% (2186/6880)\n",
            "Epoch 49 Step 215/1563 Loss: 2.100 | Acc: 31.771% (2196/6912)\n",
            "Epoch 49 Step 216/1563 Loss: 2.100 | Acc: 31.797% (2208/6944)\n",
            "Epoch 49 Step 217/1563 Loss: 2.101 | Acc: 31.766% (2216/6976)\n",
            "Epoch 49 Step 218/1563 Loss: 2.099 | Acc: 31.821% (2230/7008)\n",
            "Epoch 49 Step 219/1563 Loss: 2.099 | Acc: 31.832% (2241/7040)\n",
            "Epoch 49 Step 220/1563 Loss: 2.097 | Acc: 31.900% (2256/7072)\n",
            "Epoch 49 Step 221/1563 Loss: 2.096 | Acc: 31.912% (2267/7104)\n",
            "Epoch 49 Step 222/1563 Loss: 2.096 | Acc: 31.909% (2277/7136)\n",
            "Epoch 49 Step 223/1563 Loss: 2.095 | Acc: 31.920% (2288/7168)\n",
            "Epoch 49 Step 224/1563 Loss: 2.093 | Acc: 31.931% (2299/7200)\n",
            "Epoch 49 Step 225/1563 Loss: 2.094 | Acc: 31.955% (2311/7232)\n",
            "Epoch 49 Step 226/1563 Loss: 2.094 | Acc: 31.952% (2321/7264)\n",
            "Epoch 49 Step 227/1563 Loss: 2.095 | Acc: 31.908% (2328/7296)\n",
            "Epoch 49 Step 228/1563 Loss: 2.095 | Acc: 31.919% (2339/7328)\n",
            "Epoch 49 Step 229/1563 Loss: 2.096 | Acc: 31.902% (2348/7360)\n",
            "Epoch 49 Step 230/1563 Loss: 2.095 | Acc: 31.913% (2359/7392)\n",
            "Epoch 49 Step 231/1563 Loss: 2.095 | Acc: 31.910% (2369/7424)\n",
            "Epoch 49 Step 232/1563 Loss: 2.096 | Acc: 31.934% (2381/7456)\n",
            "Epoch 49 Step 233/1563 Loss: 2.096 | Acc: 31.971% (2394/7488)\n",
            "Epoch 49 Step 234/1563 Loss: 2.096 | Acc: 31.981% (2405/7520)\n",
            "Epoch 49 Step 235/1563 Loss: 2.095 | Acc: 31.965% (2414/7552)\n",
            "Epoch 49 Step 236/1563 Loss: 2.094 | Acc: 31.988% (2426/7584)\n",
            "Epoch 49 Step 237/1563 Loss: 2.094 | Acc: 31.972% (2435/7616)\n",
            "Epoch 49 Step 238/1563 Loss: 2.095 | Acc: 31.969% (2445/7648)\n",
            "Epoch 49 Step 239/1563 Loss: 2.094 | Acc: 31.992% (2457/7680)\n",
            "Epoch 49 Step 240/1563 Loss: 2.095 | Acc: 31.989% (2467/7712)\n",
            "Epoch 49 Step 241/1563 Loss: 2.095 | Acc: 31.999% (2478/7744)\n",
            "Epoch 49 Step 242/1563 Loss: 2.097 | Acc: 31.970% (2486/7776)\n",
            "Epoch 49 Step 243/1563 Loss: 2.096 | Acc: 32.006% (2499/7808)\n",
            "Epoch 49 Step 244/1563 Loss: 2.096 | Acc: 32.003% (2509/7840)\n",
            "Epoch 49 Step 245/1563 Loss: 2.096 | Acc: 32.012% (2520/7872)\n",
            "Epoch 49 Step 246/1563 Loss: 2.094 | Acc: 32.034% (2532/7904)\n",
            "Epoch 49 Step 247/1563 Loss: 2.093 | Acc: 32.056% (2544/7936)\n",
            "Epoch 49 Step 248/1563 Loss: 2.092 | Acc: 32.053% (2554/7968)\n",
            "Epoch 49 Step 249/1563 Loss: 2.092 | Acc: 32.062% (2565/8000)\n",
            "Epoch 49 Step 250/1563 Loss: 2.089 | Acc: 32.134% (2581/8032)\n",
            "Epoch 49 Step 251/1563 Loss: 2.090 | Acc: 32.143% (2592/8064)\n",
            "Epoch 49 Step 252/1563 Loss: 2.091 | Acc: 32.139% (2602/8096)\n",
            "Epoch 49 Step 253/1563 Loss: 2.092 | Acc: 32.124% (2611/8128)\n",
            "Epoch 49 Step 254/1563 Loss: 2.093 | Acc: 32.120% (2621/8160)\n",
            "Epoch 49 Step 255/1563 Loss: 2.094 | Acc: 32.056% (2626/8192)\n",
            "Epoch 49 Step 256/1563 Loss: 2.094 | Acc: 32.004% (2632/8224)\n",
            "Epoch 49 Step 257/1563 Loss: 2.095 | Acc: 31.977% (2640/8256)\n",
            "Epoch 49 Step 258/1563 Loss: 2.096 | Acc: 31.950% (2648/8288)\n",
            "Epoch 49 Step 259/1563 Loss: 2.097 | Acc: 31.911% (2655/8320)\n",
            "Epoch 49 Step 260/1563 Loss: 2.097 | Acc: 31.909% (2665/8352)\n",
            "Epoch 49 Step 261/1563 Loss: 2.098 | Acc: 31.870% (2672/8384)\n",
            "Epoch 49 Step 262/1563 Loss: 2.096 | Acc: 31.915% (2686/8416)\n",
            "Epoch 49 Step 263/1563 Loss: 2.096 | Acc: 31.948% (2699/8448)\n",
            "Epoch 49 Step 264/1563 Loss: 2.096 | Acc: 31.922% (2707/8480)\n",
            "Epoch 49 Step 265/1563 Loss: 2.094 | Acc: 31.908% (2716/8512)\n",
            "Epoch 49 Step 266/1563 Loss: 2.095 | Acc: 31.894% (2725/8544)\n",
            "Epoch 49 Step 267/1563 Loss: 2.094 | Acc: 31.915% (2737/8576)\n",
            "Epoch 49 Step 268/1563 Loss: 2.094 | Acc: 31.924% (2748/8608)\n",
            "Epoch 49 Step 269/1563 Loss: 2.094 | Acc: 31.933% (2759/8640)\n",
            "Epoch 49 Step 270/1563 Loss: 2.095 | Acc: 31.884% (2765/8672)\n",
            "Epoch 49 Step 271/1563 Loss: 2.093 | Acc: 31.905% (2777/8704)\n",
            "Epoch 49 Step 272/1563 Loss: 2.093 | Acc: 31.891% (2786/8736)\n",
            "Epoch 49 Step 273/1563 Loss: 2.091 | Acc: 31.934% (2800/8768)\n",
            "Epoch 49 Step 274/1563 Loss: 2.092 | Acc: 31.909% (2808/8800)\n",
            "Epoch 49 Step 275/1563 Loss: 2.090 | Acc: 31.929% (2820/8832)\n",
            "Epoch 49 Step 276/1563 Loss: 2.090 | Acc: 31.938% (2831/8864)\n",
            "Epoch 49 Step 277/1563 Loss: 2.091 | Acc: 31.879% (2836/8896)\n",
            "Epoch 49 Step 278/1563 Loss: 2.092 | Acc: 31.844% (2843/8928)\n",
            "Epoch 49 Step 279/1563 Loss: 2.091 | Acc: 31.842% (2853/8960)\n",
            "Epoch 49 Step 280/1563 Loss: 2.091 | Acc: 31.773% (2857/8992)\n",
            "Epoch 49 Step 281/1563 Loss: 2.090 | Acc: 31.760% (2866/9024)\n",
            "Epoch 49 Step 282/1563 Loss: 2.091 | Acc: 31.747% (2875/9056)\n",
            "Epoch 49 Step 283/1563 Loss: 2.093 | Acc: 31.701% (2881/9088)\n",
            "Epoch 49 Step 284/1563 Loss: 2.092 | Acc: 31.721% (2893/9120)\n",
            "Epoch 49 Step 285/1563 Loss: 2.092 | Acc: 31.720% (2903/9152)\n",
            "Epoch 49 Step 286/1563 Loss: 2.093 | Acc: 31.686% (2910/9184)\n",
            "Epoch 49 Step 287/1563 Loss: 2.092 | Acc: 31.706% (2922/9216)\n",
            "Epoch 49 Step 288/1563 Loss: 2.092 | Acc: 31.715% (2933/9248)\n",
            "Epoch 49 Step 289/1563 Loss: 2.091 | Acc: 31.735% (2945/9280)\n",
            "Epoch 49 Step 290/1563 Loss: 2.090 | Acc: 31.765% (2958/9312)\n",
            "Epoch 49 Step 291/1563 Loss: 2.091 | Acc: 31.753% (2967/9344)\n",
            "Epoch 49 Step 292/1563 Loss: 2.092 | Acc: 31.719% (2974/9376)\n",
            "Epoch 49 Step 293/1563 Loss: 2.090 | Acc: 31.771% (2989/9408)\n",
            "Epoch 49 Step 294/1563 Loss: 2.089 | Acc: 31.801% (3002/9440)\n",
            "Epoch 49 Step 295/1563 Loss: 2.088 | Acc: 31.820% (3014/9472)\n",
            "Epoch 49 Step 296/1563 Loss: 2.089 | Acc: 31.797% (3022/9504)\n",
            "Epoch 49 Step 297/1563 Loss: 2.091 | Acc: 31.764% (3029/9536)\n",
            "Epoch 49 Step 298/1563 Loss: 2.093 | Acc: 31.699% (3033/9568)\n",
            "Epoch 49 Step 299/1563 Loss: 2.093 | Acc: 31.698% (3043/9600)\n",
            "Epoch 49 Step 300/1563 Loss: 2.093 | Acc: 31.696% (3053/9632)\n",
            "Epoch 49 Step 301/1563 Loss: 2.092 | Acc: 31.705% (3064/9664)\n",
            "Epoch 49 Step 302/1563 Loss: 2.092 | Acc: 31.704% (3074/9696)\n",
            "Epoch 49 Step 303/1563 Loss: 2.091 | Acc: 31.754% (3089/9728)\n",
            "Epoch 49 Step 304/1563 Loss: 2.091 | Acc: 31.773% (3101/9760)\n",
            "Epoch 49 Step 305/1563 Loss: 2.090 | Acc: 31.781% (3112/9792)\n",
            "Epoch 49 Step 306/1563 Loss: 2.091 | Acc: 31.718% (3116/9824)\n",
            "Epoch 49 Step 307/1563 Loss: 2.091 | Acc: 31.727% (3127/9856)\n",
            "Epoch 49 Step 308/1563 Loss: 2.090 | Acc: 31.735% (3138/9888)\n",
            "Epoch 49 Step 309/1563 Loss: 2.091 | Acc: 31.724% (3147/9920)\n",
            "Epoch 49 Step 310/1563 Loss: 2.091 | Acc: 31.692% (3154/9952)\n",
            "Epoch 49 Step 311/1563 Loss: 2.091 | Acc: 31.661% (3161/9984)\n",
            "Epoch 49 Step 312/1563 Loss: 2.090 | Acc: 31.679% (3173/10016)\n",
            "Epoch 49 Step 313/1563 Loss: 2.089 | Acc: 31.708% (3186/10048)\n",
            "Epoch 49 Step 314/1563 Loss: 2.089 | Acc: 31.696% (3195/10080)\n",
            "Epoch 49 Step 315/1563 Loss: 2.089 | Acc: 31.675% (3203/10112)\n",
            "Epoch 49 Step 316/1563 Loss: 2.089 | Acc: 31.694% (3215/10144)\n",
            "Epoch 49 Step 317/1563 Loss: 2.089 | Acc: 31.682% (3224/10176)\n",
            "Epoch 49 Step 318/1563 Loss: 2.090 | Acc: 31.681% (3234/10208)\n",
            "Epoch 49 Step 319/1563 Loss: 2.090 | Acc: 31.650% (3241/10240)\n",
            "Epoch 49 Step 320/1563 Loss: 2.092 | Acc: 31.610% (3247/10272)\n",
            "Epoch 49 Step 321/1563 Loss: 2.093 | Acc: 31.570% (3253/10304)\n",
            "Epoch 49 Step 322/1563 Loss: 2.092 | Acc: 31.598% (3266/10336)\n",
            "Epoch 49 Step 323/1563 Loss: 2.094 | Acc: 31.559% (3272/10368)\n",
            "Epoch 49 Step 324/1563 Loss: 2.093 | Acc: 31.567% (3283/10400)\n",
            "Epoch 49 Step 325/1563 Loss: 2.094 | Acc: 31.566% (3293/10432)\n",
            "Epoch 49 Step 326/1563 Loss: 2.094 | Acc: 31.546% (3301/10464)\n",
            "Epoch 49 Step 327/1563 Loss: 2.096 | Acc: 31.517% (3308/10496)\n",
            "Epoch 49 Step 328/1563 Loss: 2.096 | Acc: 31.478% (3314/10528)\n",
            "Epoch 49 Step 329/1563 Loss: 2.096 | Acc: 31.487% (3325/10560)\n",
            "Epoch 49 Step 330/1563 Loss: 2.096 | Acc: 31.495% (3336/10592)\n",
            "Epoch 49 Step 331/1563 Loss: 2.095 | Acc: 31.495% (3346/10624)\n",
            "Epoch 49 Step 332/1563 Loss: 2.096 | Acc: 31.485% (3355/10656)\n",
            "Epoch 49 Step 333/1563 Loss: 2.095 | Acc: 31.559% (3373/10688)\n",
            "Epoch 49 Step 334/1563 Loss: 2.094 | Acc: 31.576% (3385/10720)\n",
            "Epoch 49 Step 335/1563 Loss: 2.096 | Acc: 31.538% (3391/10752)\n",
            "Epoch 49 Step 336/1563 Loss: 2.095 | Acc: 31.537% (3401/10784)\n",
            "Epoch 49 Step 337/1563 Loss: 2.097 | Acc: 31.500% (3407/10816)\n",
            "Epoch 49 Step 338/1563 Loss: 2.099 | Acc: 31.453% (3412/10848)\n",
            "Epoch 49 Step 339/1563 Loss: 2.099 | Acc: 31.452% (3422/10880)\n",
            "Epoch 49 Step 340/1563 Loss: 2.099 | Acc: 31.442% (3431/10912)\n",
            "Epoch 49 Step 341/1563 Loss: 2.098 | Acc: 31.488% (3446/10944)\n",
            "Epoch 49 Step 342/1563 Loss: 2.098 | Acc: 31.514% (3459/10976)\n",
            "Epoch 49 Step 343/1563 Loss: 2.098 | Acc: 31.477% (3465/11008)\n",
            "Epoch 49 Step 344/1563 Loss: 2.096 | Acc: 31.504% (3478/11040)\n",
            "Epoch 49 Step 345/1563 Loss: 2.096 | Acc: 31.521% (3490/11072)\n",
            "Epoch 49 Step 346/1563 Loss: 2.096 | Acc: 31.529% (3501/11104)\n",
            "Epoch 49 Step 347/1563 Loss: 2.095 | Acc: 31.537% (3512/11136)\n",
            "Epoch 49 Step 348/1563 Loss: 2.096 | Acc: 31.510% (3519/11168)\n",
            "Epoch 49 Step 349/1563 Loss: 2.096 | Acc: 31.509% (3529/11200)\n",
            "Epoch 49 Step 350/1563 Loss: 2.095 | Acc: 31.553% (3544/11232)\n",
            "Epoch 49 Step 351/1563 Loss: 2.094 | Acc: 31.534% (3552/11264)\n",
            "Epoch 49 Step 352/1563 Loss: 2.093 | Acc: 31.551% (3564/11296)\n",
            "Epoch 49 Step 353/1563 Loss: 2.093 | Acc: 31.568% (3576/11328)\n",
            "Epoch 49 Step 354/1563 Loss: 2.092 | Acc: 31.629% (3593/11360)\n",
            "Epoch 49 Step 355/1563 Loss: 2.091 | Acc: 31.645% (3605/11392)\n",
            "Epoch 49 Step 356/1563 Loss: 2.090 | Acc: 31.661% (3617/11424)\n",
            "Epoch 49 Step 357/1563 Loss: 2.089 | Acc: 31.660% (3627/11456)\n",
            "Epoch 49 Step 358/1563 Loss: 2.088 | Acc: 31.650% (3636/11488)\n",
            "Epoch 49 Step 359/1563 Loss: 2.088 | Acc: 31.675% (3649/11520)\n",
            "Epoch 49 Step 360/1563 Loss: 2.088 | Acc: 31.666% (3658/11552)\n",
            "Epoch 49 Step 361/1563 Loss: 2.087 | Acc: 31.690% (3671/11584)\n",
            "Epoch 49 Step 362/1563 Loss: 2.086 | Acc: 31.723% (3685/11616)\n",
            "Epoch 49 Step 363/1563 Loss: 2.086 | Acc: 31.705% (3693/11648)\n",
            "Epoch 49 Step 364/1563 Loss: 2.084 | Acc: 31.738% (3707/11680)\n",
            "Epoch 49 Step 365/1563 Loss: 2.084 | Acc: 31.711% (3714/11712)\n",
            "Epoch 49 Step 366/1563 Loss: 2.084 | Acc: 31.727% (3726/11744)\n",
            "Epoch 49 Step 367/1563 Loss: 2.085 | Acc: 31.717% (3735/11776)\n",
            "Epoch 49 Step 368/1563 Loss: 2.084 | Acc: 31.741% (3748/11808)\n",
            "Epoch 49 Step 369/1563 Loss: 2.083 | Acc: 31.723% (3756/11840)\n",
            "Epoch 49 Step 370/1563 Loss: 2.083 | Acc: 31.722% (3766/11872)\n",
            "Epoch 49 Step 371/1563 Loss: 2.083 | Acc: 31.720% (3776/11904)\n",
            "Epoch 49 Step 372/1563 Loss: 2.082 | Acc: 31.769% (3792/11936)\n",
            "Epoch 49 Step 373/1563 Loss: 2.083 | Acc: 31.760% (3801/11968)\n",
            "Epoch 49 Step 374/1563 Loss: 2.083 | Acc: 31.775% (3813/12000)\n",
            "Epoch 49 Step 375/1563 Loss: 2.083 | Acc: 31.774% (3823/12032)\n",
            "Epoch 49 Step 376/1563 Loss: 2.084 | Acc: 31.739% (3829/12064)\n",
            "Epoch 49 Step 377/1563 Loss: 2.084 | Acc: 31.746% (3840/12096)\n",
            "Epoch 49 Step 378/1563 Loss: 2.084 | Acc: 31.728% (3848/12128)\n",
            "Epoch 49 Step 379/1563 Loss: 2.085 | Acc: 31.719% (3857/12160)\n",
            "Epoch 49 Step 380/1563 Loss: 2.086 | Acc: 31.718% (3867/12192)\n",
            "Epoch 49 Step 381/1563 Loss: 2.086 | Acc: 31.708% (3876/12224)\n",
            "Epoch 49 Step 382/1563 Loss: 2.086 | Acc: 31.764% (3893/12256)\n",
            "Epoch 49 Step 383/1563 Loss: 2.085 | Acc: 31.803% (3908/12288)\n",
            "Epoch 49 Step 384/1563 Loss: 2.085 | Acc: 31.810% (3919/12320)\n",
            "Epoch 49 Step 385/1563 Loss: 2.085 | Acc: 31.776% (3925/12352)\n",
            "Epoch 49 Step 386/1563 Loss: 2.085 | Acc: 31.751% (3932/12384)\n",
            "Epoch 49 Step 387/1563 Loss: 2.084 | Acc: 31.790% (3947/12416)\n",
            "Epoch 49 Step 388/1563 Loss: 2.083 | Acc: 31.820% (3961/12448)\n",
            "Epoch 49 Step 389/1563 Loss: 2.084 | Acc: 31.811% (3970/12480)\n",
            "Epoch 49 Step 390/1563 Loss: 2.084 | Acc: 31.785% (3977/12512)\n",
            "Epoch 49 Step 391/1563 Loss: 2.083 | Acc: 31.824% (3992/12544)\n",
            "Epoch 49 Step 392/1563 Loss: 2.082 | Acc: 31.846% (4005/12576)\n",
            "Epoch 49 Step 393/1563 Loss: 2.082 | Acc: 31.837% (4014/12608)\n",
            "Epoch 49 Step 394/1563 Loss: 2.081 | Acc: 31.867% (4028/12640)\n",
            "Epoch 49 Step 395/1563 Loss: 2.082 | Acc: 31.850% (4036/12672)\n",
            "Epoch 49 Step 396/1563 Loss: 2.083 | Acc: 31.848% (4046/12704)\n",
            "Epoch 49 Step 397/1563 Loss: 2.083 | Acc: 31.855% (4057/12736)\n",
            "Epoch 49 Step 398/1563 Loss: 2.082 | Acc: 31.884% (4071/12768)\n",
            "Epoch 49 Step 399/1563 Loss: 2.081 | Acc: 31.945% (4089/12800)\n",
            "Epoch 49 Step 400/1563 Loss: 2.082 | Acc: 31.944% (4099/12832)\n",
            "Epoch 49 Step 401/1563 Loss: 2.082 | Acc: 31.981% (4114/12864)\n",
            "Epoch 49 Step 402/1563 Loss: 2.082 | Acc: 32.002% (4127/12896)\n",
            "Epoch 49 Step 403/1563 Loss: 2.082 | Acc: 32.000% (4137/12928)\n",
            "Epoch 49 Step 404/1563 Loss: 2.082 | Acc: 31.983% (4145/12960)\n",
            "Epoch 49 Step 405/1563 Loss: 2.081 | Acc: 31.997% (4157/12992)\n",
            "Epoch 49 Step 406/1563 Loss: 2.081 | Acc: 31.979% (4165/13024)\n",
            "Epoch 49 Step 407/1563 Loss: 2.081 | Acc: 31.970% (4174/13056)\n",
            "Epoch 49 Step 408/1563 Loss: 2.080 | Acc: 32.022% (4191/13088)\n",
            "Epoch 49 Step 409/1563 Loss: 2.080 | Acc: 31.997% (4198/13120)\n",
            "Epoch 49 Step 410/1563 Loss: 2.079 | Acc: 31.988% (4207/13152)\n",
            "Epoch 49 Step 411/1563 Loss: 2.079 | Acc: 31.978% (4216/13184)\n",
            "Epoch 49 Step 412/1563 Loss: 2.078 | Acc: 32.014% (4231/13216)\n",
            "Epoch 49 Step 413/1563 Loss: 2.078 | Acc: 32.035% (4244/13248)\n",
            "Epoch 49 Step 414/1563 Loss: 2.079 | Acc: 32.018% (4252/13280)\n",
            "Epoch 49 Step 415/1563 Loss: 2.080 | Acc: 31.994% (4259/13312)\n",
            "Epoch 49 Step 416/1563 Loss: 2.080 | Acc: 31.969% (4266/13344)\n",
            "Epoch 49 Step 417/1563 Loss: 2.081 | Acc: 31.968% (4276/13376)\n",
            "Epoch 49 Step 418/1563 Loss: 2.081 | Acc: 31.951% (4284/13408)\n",
            "Epoch 49 Step 419/1563 Loss: 2.081 | Acc: 31.920% (4290/13440)\n",
            "Epoch 49 Step 420/1563 Loss: 2.081 | Acc: 31.911% (4299/13472)\n",
            "Epoch 49 Step 421/1563 Loss: 2.080 | Acc: 31.916% (4310/13504)\n",
            "Epoch 49 Step 422/1563 Loss: 2.080 | Acc: 31.900% (4318/13536)\n",
            "Epoch 49 Step 423/1563 Loss: 2.081 | Acc: 31.884% (4326/13568)\n",
            "Epoch 49 Step 424/1563 Loss: 2.081 | Acc: 31.882% (4336/13600)\n",
            "Epoch 49 Step 425/1563 Loss: 2.081 | Acc: 31.903% (4349/13632)\n",
            "Epoch 49 Step 426/1563 Loss: 2.080 | Acc: 31.909% (4360/13664)\n",
            "Epoch 49 Step 427/1563 Loss: 2.081 | Acc: 31.871% (4365/13696)\n",
            "Epoch 49 Step 428/1563 Loss: 2.081 | Acc: 31.869% (4375/13728)\n",
            "Epoch 49 Step 429/1563 Loss: 2.080 | Acc: 31.839% (4381/13760)\n",
            "Epoch 49 Step 430/1563 Loss: 2.081 | Acc: 31.830% (4390/13792)\n",
            "Epoch 49 Step 431/1563 Loss: 2.081 | Acc: 31.850% (4403/13824)\n",
            "Epoch 49 Step 432/1563 Loss: 2.080 | Acc: 31.856% (4414/13856)\n",
            "Epoch 49 Step 433/1563 Loss: 2.080 | Acc: 31.884% (4428/13888)\n",
            "Epoch 49 Step 434/1563 Loss: 2.080 | Acc: 31.882% (4438/13920)\n",
            "Epoch 49 Step 435/1563 Loss: 2.078 | Acc: 31.909% (4452/13952)\n",
            "Epoch 49 Step 436/1563 Loss: 2.079 | Acc: 31.886% (4459/13984)\n",
            "Epoch 49 Step 437/1563 Loss: 2.079 | Acc: 31.885% (4469/14016)\n",
            "Epoch 49 Step 438/1563 Loss: 2.079 | Acc: 31.876% (4478/14048)\n",
            "Epoch 49 Step 439/1563 Loss: 2.079 | Acc: 31.868% (4487/14080)\n",
            "Epoch 49 Step 440/1563 Loss: 2.078 | Acc: 31.909% (4503/14112)\n",
            "Epoch 49 Step 441/1563 Loss: 2.078 | Acc: 31.908% (4513/14144)\n",
            "Epoch 49 Step 442/1563 Loss: 2.077 | Acc: 31.920% (4525/14176)\n",
            "Epoch 49 Step 443/1563 Loss: 2.076 | Acc: 31.947% (4539/14208)\n",
            "Epoch 49 Step 444/1563 Loss: 2.076 | Acc: 31.966% (4552/14240)\n",
            "Epoch 49 Step 445/1563 Loss: 2.076 | Acc: 31.979% (4564/14272)\n",
            "Epoch 49 Step 446/1563 Loss: 2.075 | Acc: 31.956% (4571/14304)\n",
            "Epoch 49 Step 447/1563 Loss: 2.076 | Acc: 31.948% (4580/14336)\n",
            "Epoch 49 Step 448/1563 Loss: 2.076 | Acc: 31.939% (4589/14368)\n",
            "Epoch 49 Step 449/1563 Loss: 2.076 | Acc: 31.951% (4601/14400)\n",
            "Epoch 49 Step 450/1563 Loss: 2.075 | Acc: 31.936% (4609/14432)\n",
            "Epoch 49 Step 451/1563 Loss: 2.075 | Acc: 31.921% (4617/14464)\n",
            "Epoch 49 Step 452/1563 Loss: 2.074 | Acc: 31.940% (4630/14496)\n",
            "Epoch 49 Step 453/1563 Loss: 2.075 | Acc: 31.904% (4635/14528)\n",
            "Epoch 49 Step 454/1563 Loss: 2.076 | Acc: 31.889% (4643/14560)\n",
            "Epoch 49 Step 455/1563 Loss: 2.075 | Acc: 31.908% (4656/14592)\n",
            "Epoch 49 Step 456/1563 Loss: 2.076 | Acc: 31.865% (4660/14624)\n",
            "Epoch 49 Step 457/1563 Loss: 2.075 | Acc: 31.898% (4675/14656)\n",
            "Epoch 49 Step 458/1563 Loss: 2.076 | Acc: 31.883% (4683/14688)\n",
            "Epoch 49 Step 459/1563 Loss: 2.076 | Acc: 31.875% (4692/14720)\n",
            "Epoch 49 Step 460/1563 Loss: 2.076 | Acc: 31.874% (4702/14752)\n",
            "Epoch 49 Step 461/1563 Loss: 2.075 | Acc: 31.906% (4717/14784)\n",
            "Epoch 49 Step 462/1563 Loss: 2.075 | Acc: 31.905% (4727/14816)\n",
            "Epoch 49 Step 463/1563 Loss: 2.076 | Acc: 31.883% (4734/14848)\n",
            "Epoch 49 Step 464/1563 Loss: 2.077 | Acc: 31.868% (4742/14880)\n",
            "Epoch 49 Step 465/1563 Loss: 2.076 | Acc: 31.874% (4753/14912)\n",
            "Epoch 49 Step 466/1563 Loss: 2.075 | Acc: 31.912% (4769/14944)\n",
            "Epoch 49 Step 467/1563 Loss: 2.074 | Acc: 31.938% (4783/14976)\n",
            "Epoch 49 Step 468/1563 Loss: 2.074 | Acc: 31.943% (4794/15008)\n",
            "Epoch 49 Step 469/1563 Loss: 2.074 | Acc: 31.941% (4804/15040)\n",
            "Epoch 49 Step 470/1563 Loss: 2.074 | Acc: 31.933% (4813/15072)\n",
            "Epoch 49 Step 471/1563 Loss: 2.074 | Acc: 31.958% (4827/15104)\n",
            "Epoch 49 Step 472/1563 Loss: 2.074 | Acc: 31.957% (4837/15136)\n",
            "Epoch 49 Step 473/1563 Loss: 2.074 | Acc: 31.988% (4852/15168)\n",
            "Epoch 49 Step 474/1563 Loss: 2.073 | Acc: 31.993% (4863/15200)\n",
            "Epoch 49 Step 475/1563 Loss: 2.074 | Acc: 31.998% (4874/15232)\n",
            "Epoch 49 Step 476/1563 Loss: 2.075 | Acc: 31.977% (4881/15264)\n",
            "Epoch 49 Step 477/1563 Loss: 2.074 | Acc: 31.976% (4891/15296)\n",
            "Epoch 49 Step 478/1563 Loss: 2.074 | Acc: 31.987% (4903/15328)\n",
            "Epoch 49 Step 479/1563 Loss: 2.073 | Acc: 31.992% (4914/15360)\n",
            "Epoch 49 Step 480/1563 Loss: 2.073 | Acc: 32.004% (4926/15392)\n",
            "Epoch 49 Step 481/1563 Loss: 2.073 | Acc: 32.015% (4938/15424)\n",
            "Epoch 49 Step 482/1563 Loss: 2.072 | Acc: 32.046% (4953/15456)\n",
            "Epoch 49 Step 483/1563 Loss: 2.073 | Acc: 32.031% (4961/15488)\n",
            "Epoch 49 Step 484/1563 Loss: 2.073 | Acc: 32.030% (4971/15520)\n",
            "Epoch 49 Step 485/1563 Loss: 2.074 | Acc: 32.009% (4978/15552)\n",
            "Epoch 49 Step 486/1563 Loss: 2.074 | Acc: 32.020% (4990/15584)\n",
            "Epoch 49 Step 487/1563 Loss: 2.073 | Acc: 32.012% (4999/15616)\n",
            "Epoch 49 Step 488/1563 Loss: 2.073 | Acc: 31.998% (5007/15648)\n",
            "Epoch 49 Step 489/1563 Loss: 2.073 | Acc: 32.003% (5018/15680)\n",
            "Epoch 49 Step 490/1563 Loss: 2.073 | Acc: 32.014% (5030/15712)\n",
            "Epoch 49 Step 491/1563 Loss: 2.072 | Acc: 32.031% (5043/15744)\n",
            "Epoch 49 Step 492/1563 Loss: 2.072 | Acc: 32.030% (5053/15776)\n",
            "Epoch 49 Step 493/1563 Loss: 2.073 | Acc: 32.022% (5062/15808)\n",
            "Epoch 49 Step 494/1563 Loss: 2.074 | Acc: 32.008% (5070/15840)\n",
            "Epoch 49 Step 495/1563 Loss: 2.074 | Acc: 32.000% (5079/15872)\n",
            "Epoch 49 Step 496/1563 Loss: 2.074 | Acc: 32.023% (5093/15904)\n",
            "Epoch 49 Step 497/1563 Loss: 2.074 | Acc: 32.022% (5103/15936)\n",
            "Epoch 49 Step 498/1563 Loss: 2.075 | Acc: 32.033% (5115/15968)\n",
            "Epoch 49 Step 499/1563 Loss: 2.075 | Acc: 32.050% (5128/16000)\n",
            "Epoch 49 Step 500/1563 Loss: 2.076 | Acc: 32.030% (5135/16032)\n",
            "Epoch 49 Step 501/1563 Loss: 2.077 | Acc: 32.028% (5145/16064)\n",
            "Epoch 49 Step 502/1563 Loss: 2.076 | Acc: 32.033% (5156/16096)\n",
            "Epoch 49 Step 503/1563 Loss: 2.077 | Acc: 32.037% (5167/16128)\n",
            "Epoch 49 Step 504/1563 Loss: 2.077 | Acc: 32.042% (5178/16160)\n",
            "Epoch 49 Step 505/1563 Loss: 2.078 | Acc: 32.047% (5189/16192)\n",
            "Epoch 49 Step 506/1563 Loss: 2.077 | Acc: 32.076% (5204/16224)\n",
            "Epoch 49 Step 507/1563 Loss: 2.077 | Acc: 32.074% (5214/16256)\n",
            "Epoch 49 Step 508/1563 Loss: 2.077 | Acc: 32.073% (5224/16288)\n",
            "Epoch 49 Step 509/1563 Loss: 2.077 | Acc: 32.077% (5235/16320)\n",
            "Epoch 49 Step 510/1563 Loss: 2.077 | Acc: 32.069% (5244/16352)\n",
            "Epoch 49 Step 511/1563 Loss: 2.077 | Acc: 32.056% (5252/16384)\n",
            "Epoch 49 Step 512/1563 Loss: 2.077 | Acc: 32.054% (5262/16416)\n",
            "Epoch 49 Step 513/1563 Loss: 2.077 | Acc: 32.071% (5275/16448)\n",
            "Epoch 49 Step 514/1563 Loss: 2.077 | Acc: 32.087% (5288/16480)\n",
            "Epoch 49 Step 515/1563 Loss: 2.077 | Acc: 32.086% (5298/16512)\n",
            "Epoch 49 Step 516/1563 Loss: 2.076 | Acc: 32.102% (5311/16544)\n",
            "Epoch 49 Step 517/1563 Loss: 2.076 | Acc: 32.076% (5317/16576)\n",
            "Epoch 49 Step 518/1563 Loss: 2.077 | Acc: 32.075% (5327/16608)\n",
            "Epoch 49 Step 519/1563 Loss: 2.078 | Acc: 32.055% (5334/16640)\n",
            "Epoch 49 Step 520/1563 Loss: 2.078 | Acc: 32.072% (5347/16672)\n",
            "Epoch 49 Step 521/1563 Loss: 2.078 | Acc: 32.058% (5355/16704)\n",
            "Epoch 49 Step 522/1563 Loss: 2.078 | Acc: 32.069% (5367/16736)\n",
            "Epoch 49 Step 523/1563 Loss: 2.079 | Acc: 32.043% (5373/16768)\n",
            "Epoch 49 Step 524/1563 Loss: 2.078 | Acc: 32.071% (5388/16800)\n",
            "Epoch 49 Step 525/1563 Loss: 2.079 | Acc: 32.046% (5394/16832)\n",
            "Epoch 49 Step 526/1563 Loss: 2.079 | Acc: 32.051% (5405/16864)\n",
            "Epoch 49 Step 527/1563 Loss: 2.079 | Acc: 32.061% (5417/16896)\n",
            "Epoch 49 Step 528/1563 Loss: 2.079 | Acc: 32.047% (5425/16928)\n",
            "Epoch 49 Step 529/1563 Loss: 2.080 | Acc: 32.034% (5433/16960)\n",
            "Epoch 49 Step 530/1563 Loss: 2.079 | Acc: 32.044% (5445/16992)\n",
            "Epoch 49 Step 531/1563 Loss: 2.079 | Acc: 32.025% (5452/17024)\n",
            "Epoch 49 Step 532/1563 Loss: 2.078 | Acc: 32.047% (5466/17056)\n",
            "Epoch 49 Step 533/1563 Loss: 2.078 | Acc: 32.040% (5475/17088)\n",
            "Epoch 49 Step 534/1563 Loss: 2.078 | Acc: 32.033% (5484/17120)\n",
            "Epoch 49 Step 535/1563 Loss: 2.078 | Acc: 32.014% (5491/17152)\n",
            "Epoch 49 Step 536/1563 Loss: 2.079 | Acc: 32.012% (5501/17184)\n",
            "Epoch 49 Step 537/1563 Loss: 2.078 | Acc: 32.017% (5512/17216)\n",
            "Epoch 49 Step 538/1563 Loss: 2.078 | Acc: 32.004% (5520/17248)\n",
            "Epoch 49 Step 539/1563 Loss: 2.078 | Acc: 31.991% (5528/17280)\n",
            "Epoch 49 Step 540/1563 Loss: 2.079 | Acc: 31.978% (5536/17312)\n",
            "Epoch 49 Step 541/1563 Loss: 2.078 | Acc: 31.976% (5546/17344)\n",
            "Epoch 49 Step 542/1563 Loss: 2.079 | Acc: 31.969% (5555/17376)\n",
            "Epoch 49 Step 543/1563 Loss: 2.079 | Acc: 31.968% (5565/17408)\n",
            "Epoch 49 Step 544/1563 Loss: 2.080 | Acc: 31.972% (5576/17440)\n",
            "Epoch 49 Step 545/1563 Loss: 2.079 | Acc: 31.988% (5589/17472)\n",
            "Epoch 49 Step 546/1563 Loss: 2.080 | Acc: 31.976% (5597/17504)\n",
            "Epoch 49 Step 547/1563 Loss: 2.079 | Acc: 31.997% (5611/17536)\n",
            "Epoch 49 Step 548/1563 Loss: 2.078 | Acc: 32.007% (5623/17568)\n",
            "Epoch 49 Step 549/1563 Loss: 2.078 | Acc: 32.011% (5634/17600)\n",
            "Epoch 49 Step 550/1563 Loss: 2.079 | Acc: 32.010% (5644/17632)\n",
            "Epoch 49 Step 551/1563 Loss: 2.079 | Acc: 32.014% (5655/17664)\n",
            "Epoch 49 Step 552/1563 Loss: 2.079 | Acc: 32.019% (5666/17696)\n",
            "Epoch 49 Step 553/1563 Loss: 2.079 | Acc: 32.017% (5676/17728)\n",
            "Epoch 49 Step 554/1563 Loss: 2.078 | Acc: 31.999% (5683/17760)\n",
            "Epoch 49 Step 555/1563 Loss: 2.078 | Acc: 32.014% (5696/17792)\n",
            "Epoch 49 Step 556/1563 Loss: 2.078 | Acc: 32.030% (5709/17824)\n",
            "Epoch 49 Step 557/1563 Loss: 2.079 | Acc: 32.023% (5718/17856)\n",
            "Epoch 49 Step 558/1563 Loss: 2.079 | Acc: 32.033% (5730/17888)\n",
            "Epoch 49 Step 559/1563 Loss: 2.079 | Acc: 32.015% (5737/17920)\n",
            "Epoch 49 Step 560/1563 Loss: 2.078 | Acc: 32.019% (5748/17952)\n",
            "Epoch 49 Step 561/1563 Loss: 2.079 | Acc: 32.017% (5758/17984)\n",
            "Epoch 49 Step 562/1563 Loss: 2.079 | Acc: 32.005% (5766/18016)\n",
            "Epoch 49 Step 563/1563 Loss: 2.079 | Acc: 32.009% (5777/18048)\n",
            "Epoch 49 Step 564/1563 Loss: 2.079 | Acc: 32.013% (5788/18080)\n",
            "Epoch 49 Step 565/1563 Loss: 2.079 | Acc: 32.006% (5797/18112)\n",
            "Epoch 49 Step 566/1563 Loss: 2.080 | Acc: 32.011% (5808/18144)\n",
            "Epoch 49 Step 567/1563 Loss: 2.080 | Acc: 31.998% (5816/18176)\n",
            "Epoch 49 Step 568/1563 Loss: 2.080 | Acc: 32.002% (5827/18208)\n",
            "Epoch 49 Step 569/1563 Loss: 2.080 | Acc: 31.990% (5835/18240)\n",
            "Epoch 49 Step 570/1563 Loss: 2.080 | Acc: 31.989% (5845/18272)\n",
            "Epoch 49 Step 571/1563 Loss: 2.080 | Acc: 32.015% (5860/18304)\n",
            "Epoch 49 Step 572/1563 Loss: 2.080 | Acc: 32.003% (5868/18336)\n",
            "Epoch 49 Step 573/1563 Loss: 2.080 | Acc: 31.985% (5875/18368)\n",
            "Epoch 49 Step 574/1563 Loss: 2.079 | Acc: 32.005% (5889/18400)\n",
            "Epoch 49 Step 575/1563 Loss: 2.079 | Acc: 31.993% (5897/18432)\n",
            "Epoch 49 Step 576/1563 Loss: 2.080 | Acc: 31.976% (5904/18464)\n",
            "Epoch 49 Step 577/1563 Loss: 2.079 | Acc: 31.964% (5912/18496)\n",
            "Epoch 49 Step 578/1563 Loss: 2.079 | Acc: 31.968% (5923/18528)\n",
            "Epoch 49 Step 579/1563 Loss: 2.080 | Acc: 31.956% (5931/18560)\n",
            "Epoch 49 Step 580/1563 Loss: 2.081 | Acc: 31.922% (5935/18592)\n",
            "Epoch 49 Step 581/1563 Loss: 2.081 | Acc: 31.927% (5946/18624)\n",
            "Epoch 49 Step 582/1563 Loss: 2.080 | Acc: 31.915% (5954/18656)\n",
            "Epoch 49 Step 583/1563 Loss: 2.081 | Acc: 31.908% (5963/18688)\n",
            "Epoch 49 Step 584/1563 Loss: 2.081 | Acc: 31.902% (5972/18720)\n",
            "Epoch 49 Step 585/1563 Loss: 2.081 | Acc: 31.895% (5981/18752)\n",
            "Epoch 49 Step 586/1563 Loss: 2.081 | Acc: 31.894% (5991/18784)\n",
            "Epoch 49 Step 587/1563 Loss: 2.081 | Acc: 31.888% (6000/18816)\n",
            "Epoch 49 Step 588/1563 Loss: 2.080 | Acc: 31.903% (6013/18848)\n",
            "Epoch 49 Step 589/1563 Loss: 2.080 | Acc: 31.896% (6022/18880)\n",
            "Epoch 49 Step 590/1563 Loss: 2.080 | Acc: 31.895% (6032/18912)\n",
            "Epoch 49 Step 591/1563 Loss: 2.081 | Acc: 31.889% (6041/18944)\n",
            "Epoch 49 Step 592/1563 Loss: 2.081 | Acc: 31.893% (6052/18976)\n",
            "Epoch 49 Step 593/1563 Loss: 2.081 | Acc: 31.902% (6064/19008)\n",
            "Epoch 49 Step 594/1563 Loss: 2.080 | Acc: 31.928% (6079/19040)\n",
            "Epoch 49 Step 595/1563 Loss: 2.079 | Acc: 31.937% (6091/19072)\n",
            "Epoch 49 Step 596/1563 Loss: 2.080 | Acc: 31.941% (6102/19104)\n",
            "Epoch 49 Step 597/1563 Loss: 2.080 | Acc: 31.935% (6111/19136)\n",
            "Epoch 49 Step 598/1563 Loss: 2.079 | Acc: 31.939% (6122/19168)\n",
            "Epoch 49 Step 599/1563 Loss: 2.079 | Acc: 31.932% (6131/19200)\n",
            "Epoch 49 Step 600/1563 Loss: 2.079 | Acc: 31.947% (6144/19232)\n",
            "Epoch 49 Step 601/1563 Loss: 2.080 | Acc: 31.940% (6153/19264)\n",
            "Epoch 49 Step 602/1563 Loss: 2.079 | Acc: 31.955% (6166/19296)\n",
            "Epoch 49 Step 603/1563 Loss: 2.079 | Acc: 31.938% (6173/19328)\n",
            "Epoch 49 Step 604/1563 Loss: 2.079 | Acc: 31.942% (6184/19360)\n",
            "Epoch 49 Step 605/1563 Loss: 2.079 | Acc: 31.936% (6193/19392)\n",
            "Epoch 49 Step 606/1563 Loss: 2.078 | Acc: 31.955% (6207/19424)\n",
            "Epoch 49 Step 607/1563 Loss: 2.078 | Acc: 31.944% (6215/19456)\n",
            "Epoch 49 Step 608/1563 Loss: 2.078 | Acc: 31.938% (6224/19488)\n",
            "Epoch 49 Step 609/1563 Loss: 2.078 | Acc: 31.926% (6232/19520)\n",
            "Epoch 49 Step 610/1563 Loss: 2.077 | Acc: 31.951% (6247/19552)\n",
            "Epoch 49 Step 611/1563 Loss: 2.077 | Acc: 31.960% (6259/19584)\n",
            "Epoch 49 Step 612/1563 Loss: 2.077 | Acc: 31.979% (6273/19616)\n",
            "Epoch 49 Step 613/1563 Loss: 2.076 | Acc: 31.988% (6285/19648)\n",
            "Epoch 49 Step 614/1563 Loss: 2.076 | Acc: 31.977% (6293/19680)\n",
            "Epoch 49 Step 615/1563 Loss: 2.075 | Acc: 32.006% (6309/19712)\n",
            "Epoch 49 Step 616/1563 Loss: 2.075 | Acc: 32.000% (6318/19744)\n",
            "Epoch 49 Step 617/1563 Loss: 2.075 | Acc: 32.008% (6330/19776)\n",
            "Epoch 49 Step 618/1563 Loss: 2.075 | Acc: 31.997% (6338/19808)\n",
            "Epoch 49 Step 619/1563 Loss: 2.075 | Acc: 32.001% (6349/19840)\n",
            "Epoch 49 Step 620/1563 Loss: 2.074 | Acc: 32.010% (6361/19872)\n",
            "Epoch 49 Step 621/1563 Loss: 2.074 | Acc: 32.019% (6373/19904)\n",
            "Epoch 49 Step 622/1563 Loss: 2.074 | Acc: 32.012% (6382/19936)\n",
            "Epoch 49 Step 623/1563 Loss: 2.075 | Acc: 32.001% (6390/19968)\n",
            "Epoch 49 Step 624/1563 Loss: 2.074 | Acc: 32.020% (6404/20000)\n",
            "Epoch 49 Step 625/1563 Loss: 2.075 | Acc: 32.024% (6415/20032)\n",
            "Epoch 49 Step 626/1563 Loss: 2.075 | Acc: 32.028% (6426/20064)\n",
            "Epoch 49 Step 627/1563 Loss: 2.074 | Acc: 32.026% (6436/20096)\n",
            "Epoch 49 Step 628/1563 Loss: 2.075 | Acc: 32.015% (6444/20128)\n",
            "Epoch 49 Step 629/1563 Loss: 2.075 | Acc: 32.019% (6455/20160)\n",
            "Epoch 49 Step 630/1563 Loss: 2.075 | Acc: 32.023% (6466/20192)\n",
            "Epoch 49 Step 631/1563 Loss: 2.075 | Acc: 32.021% (6476/20224)\n",
            "Epoch 49 Step 632/1563 Loss: 2.076 | Acc: 32.010% (6484/20256)\n",
            "Epoch 49 Step 633/1563 Loss: 2.076 | Acc: 32.014% (6495/20288)\n",
            "Epoch 49 Step 634/1563 Loss: 2.075 | Acc: 32.028% (6508/20320)\n",
            "Epoch 49 Step 635/1563 Loss: 2.075 | Acc: 32.026% (6518/20352)\n",
            "Epoch 49 Step 636/1563 Loss: 2.076 | Acc: 32.020% (6527/20384)\n",
            "Epoch 49 Step 637/1563 Loss: 2.076 | Acc: 32.029% (6539/20416)\n",
            "Epoch 49 Step 638/1563 Loss: 2.075 | Acc: 32.032% (6550/20448)\n",
            "Epoch 49 Step 639/1563 Loss: 2.075 | Acc: 32.026% (6559/20480)\n",
            "Epoch 49 Step 640/1563 Loss: 2.076 | Acc: 32.011% (6566/20512)\n",
            "Epoch 49 Step 641/1563 Loss: 2.076 | Acc: 31.995% (6573/20544)\n",
            "Epoch 49 Step 642/1563 Loss: 2.075 | Acc: 32.028% (6590/20576)\n",
            "Epoch 49 Step 643/1563 Loss: 2.075 | Acc: 32.026% (6600/20608)\n",
            "Epoch 49 Step 644/1563 Loss: 2.075 | Acc: 32.016% (6608/20640)\n",
            "Epoch 49 Step 645/1563 Loss: 2.075 | Acc: 32.039% (6623/20672)\n",
            "Epoch 49 Step 646/1563 Loss: 2.075 | Acc: 32.052% (6636/20704)\n",
            "Epoch 49 Step 647/1563 Loss: 2.075 | Acc: 32.055% (6647/20736)\n",
            "Epoch 49 Step 648/1563 Loss: 2.075 | Acc: 32.064% (6659/20768)\n",
            "Epoch 49 Step 649/1563 Loss: 2.076 | Acc: 32.067% (6670/20800)\n",
            "Epoch 49 Step 650/1563 Loss: 2.076 | Acc: 32.032% (6673/20832)\n",
            "Epoch 49 Step 651/1563 Loss: 2.076 | Acc: 32.065% (6690/20864)\n",
            "Epoch 49 Step 652/1563 Loss: 2.076 | Acc: 32.054% (6698/20896)\n",
            "Epoch 49 Step 653/1563 Loss: 2.075 | Acc: 32.086% (6715/20928)\n",
            "Epoch 49 Step 654/1563 Loss: 2.076 | Acc: 32.047% (6717/20960)\n",
            "Epoch 49 Step 655/1563 Loss: 2.075 | Acc: 32.069% (6732/20992)\n",
            "Epoch 49 Step 656/1563 Loss: 2.075 | Acc: 32.049% (6738/21024)\n",
            "Epoch 49 Step 657/1563 Loss: 2.075 | Acc: 32.053% (6749/21056)\n",
            "Epoch 49 Step 658/1563 Loss: 2.075 | Acc: 32.061% (6761/21088)\n",
            "Epoch 49 Step 659/1563 Loss: 2.075 | Acc: 32.060% (6771/21120)\n",
            "Epoch 49 Step 660/1563 Loss: 2.075 | Acc: 32.054% (6780/21152)\n",
            "Epoch 49 Step 661/1563 Loss: 2.075 | Acc: 32.057% (6791/21184)\n",
            "Epoch 49 Step 662/1563 Loss: 2.075 | Acc: 32.075% (6805/21216)\n",
            "Epoch 49 Step 663/1563 Loss: 2.075 | Acc: 32.078% (6816/21248)\n",
            "Epoch 49 Step 664/1563 Loss: 2.075 | Acc: 32.091% (6829/21280)\n",
            "Epoch 49 Step 665/1563 Loss: 2.074 | Acc: 32.123% (6846/21312)\n",
            "Epoch 49 Step 666/1563 Loss: 2.073 | Acc: 32.131% (6858/21344)\n",
            "Epoch 49 Step 667/1563 Loss: 2.073 | Acc: 32.134% (6869/21376)\n",
            "Epoch 49 Step 668/1563 Loss: 2.073 | Acc: 32.133% (6879/21408)\n",
            "Epoch 49 Step 669/1563 Loss: 2.073 | Acc: 32.136% (6890/21440)\n",
            "Epoch 49 Step 670/1563 Loss: 2.072 | Acc: 32.135% (6900/21472)\n",
            "Epoch 49 Step 671/1563 Loss: 2.072 | Acc: 32.148% (6913/21504)\n",
            "Epoch 49 Step 672/1563 Loss: 2.072 | Acc: 32.160% (6926/21536)\n",
            "Epoch 49 Step 673/1563 Loss: 2.072 | Acc: 32.168% (6938/21568)\n",
            "Epoch 49 Step 674/1563 Loss: 2.073 | Acc: 32.176% (6950/21600)\n",
            "Epoch 49 Step 675/1563 Loss: 2.072 | Acc: 32.170% (6959/21632)\n",
            "Epoch 49 Step 676/1563 Loss: 2.073 | Acc: 32.173% (6970/21664)\n",
            "Epoch 49 Step 677/1563 Loss: 2.072 | Acc: 32.176% (6981/21696)\n",
            "Epoch 49 Step 678/1563 Loss: 2.072 | Acc: 32.184% (6993/21728)\n",
            "Epoch 49 Step 679/1563 Loss: 2.072 | Acc: 32.188% (7004/21760)\n",
            "Epoch 49 Step 680/1563 Loss: 2.072 | Acc: 32.177% (7012/21792)\n",
            "Epoch 49 Step 681/1563 Loss: 2.073 | Acc: 32.166% (7020/21824)\n",
            "Epoch 49 Step 682/1563 Loss: 2.073 | Acc: 32.151% (7027/21856)\n",
            "Epoch 49 Step 683/1563 Loss: 2.073 | Acc: 32.150% (7037/21888)\n",
            "Epoch 49 Step 684/1563 Loss: 2.073 | Acc: 32.140% (7045/21920)\n",
            "Epoch 49 Step 685/1563 Loss: 2.073 | Acc: 32.147% (7057/21952)\n",
            "Epoch 49 Step 686/1563 Loss: 2.073 | Acc: 32.146% (7067/21984)\n",
            "Epoch 49 Step 687/1563 Loss: 2.073 | Acc: 32.136% (7075/22016)\n",
            "Epoch 49 Step 688/1563 Loss: 2.073 | Acc: 32.130% (7084/22048)\n",
            "Epoch 49 Step 689/1563 Loss: 2.073 | Acc: 32.138% (7096/22080)\n",
            "Epoch 49 Step 690/1563 Loss: 2.074 | Acc: 32.118% (7102/22112)\n",
            "Epoch 49 Step 691/1563 Loss: 2.074 | Acc: 32.131% (7115/22144)\n",
            "Epoch 49 Step 692/1563 Loss: 2.074 | Acc: 32.143% (7128/22176)\n",
            "Epoch 49 Step 693/1563 Loss: 2.074 | Acc: 32.142% (7138/22208)\n",
            "Epoch 49 Step 694/1563 Loss: 2.074 | Acc: 32.149% (7150/22240)\n",
            "Epoch 49 Step 695/1563 Loss: 2.073 | Acc: 32.161% (7163/22272)\n",
            "Epoch 49 Step 696/1563 Loss: 2.073 | Acc: 32.178% (7177/22304)\n",
            "Epoch 49 Step 697/1563 Loss: 2.073 | Acc: 32.163% (7184/22336)\n",
            "Epoch 49 Step 698/1563 Loss: 2.073 | Acc: 32.162% (7194/22368)\n",
            "Epoch 49 Step 699/1563 Loss: 2.073 | Acc: 32.165% (7205/22400)\n",
            "Epoch 49 Step 700/1563 Loss: 2.073 | Acc: 32.168% (7216/22432)\n",
            "Epoch 49 Step 701/1563 Loss: 2.073 | Acc: 32.194% (7232/22464)\n",
            "Epoch 49 Step 702/1563 Loss: 2.073 | Acc: 32.192% (7242/22496)\n",
            "Epoch 49 Step 703/1563 Loss: 2.073 | Acc: 32.195% (7253/22528)\n",
            "Epoch 49 Step 704/1563 Loss: 2.073 | Acc: 32.190% (7262/22560)\n",
            "Epoch 49 Step 705/1563 Loss: 2.072 | Acc: 32.193% (7273/22592)\n",
            "Epoch 49 Step 706/1563 Loss: 2.072 | Acc: 32.196% (7284/22624)\n",
            "Epoch 49 Step 707/1563 Loss: 2.072 | Acc: 32.195% (7294/22656)\n",
            "Epoch 49 Step 708/1563 Loss: 2.072 | Acc: 32.184% (7302/22688)\n",
            "Epoch 49 Step 709/1563 Loss: 2.072 | Acc: 32.188% (7313/22720)\n",
            "Epoch 49 Step 710/1563 Loss: 2.072 | Acc: 32.177% (7321/22752)\n",
            "Epoch 49 Step 711/1563 Loss: 2.073 | Acc: 32.180% (7332/22784)\n",
            "Epoch 49 Step 712/1563 Loss: 2.073 | Acc: 32.179% (7342/22816)\n",
            "Epoch 49 Step 713/1563 Loss: 2.072 | Acc: 32.191% (7355/22848)\n",
            "Epoch 49 Step 714/1563 Loss: 2.072 | Acc: 32.194% (7366/22880)\n",
            "Epoch 49 Step 715/1563 Loss: 2.072 | Acc: 32.197% (7377/22912)\n",
            "Epoch 49 Step 716/1563 Loss: 2.072 | Acc: 32.196% (7387/22944)\n",
            "Epoch 49 Step 717/1563 Loss: 2.071 | Acc: 32.186% (7395/22976)\n",
            "Epoch 49 Step 718/1563 Loss: 2.072 | Acc: 32.180% (7404/23008)\n",
            "Epoch 49 Step 719/1563 Loss: 2.072 | Acc: 32.179% (7414/23040)\n",
            "Epoch 49 Step 720/1563 Loss: 2.072 | Acc: 32.186% (7426/23072)\n",
            "Epoch 49 Step 721/1563 Loss: 2.072 | Acc: 32.189% (7437/23104)\n",
            "Epoch 49 Step 722/1563 Loss: 2.072 | Acc: 32.175% (7444/23136)\n",
            "Epoch 49 Step 723/1563 Loss: 2.072 | Acc: 32.169% (7453/23168)\n",
            "Epoch 49 Step 724/1563 Loss: 2.072 | Acc: 32.168% (7463/23200)\n",
            "Epoch 49 Step 725/1563 Loss: 2.072 | Acc: 32.175% (7475/23232)\n",
            "Epoch 49 Step 726/1563 Loss: 2.072 | Acc: 32.170% (7484/23264)\n",
            "Epoch 49 Step 727/1563 Loss: 2.071 | Acc: 32.169% (7494/23296)\n",
            "Epoch 49 Step 728/1563 Loss: 2.072 | Acc: 32.154% (7501/23328)\n",
            "Epoch 49 Step 729/1563 Loss: 2.072 | Acc: 32.145% (7509/23360)\n",
            "Epoch 49 Step 730/1563 Loss: 2.072 | Acc: 32.148% (7520/23392)\n",
            "Epoch 49 Step 731/1563 Loss: 2.072 | Acc: 32.142% (7529/23424)\n",
            "Epoch 49 Step 732/1563 Loss: 2.071 | Acc: 32.158% (7543/23456)\n",
            "Epoch 49 Step 733/1563 Loss: 2.072 | Acc: 32.157% (7553/23488)\n",
            "Epoch 49 Step 734/1563 Loss: 2.072 | Acc: 32.160% (7564/23520)\n",
            "Epoch 49 Step 735/1563 Loss: 2.072 | Acc: 32.146% (7571/23552)\n",
            "Epoch 49 Step 736/1563 Loss: 2.072 | Acc: 32.157% (7584/23584)\n",
            "Epoch 49 Step 737/1563 Loss: 2.071 | Acc: 32.173% (7598/23616)\n",
            "Epoch 49 Step 738/1563 Loss: 2.072 | Acc: 32.163% (7606/23648)\n",
            "Epoch 49 Step 739/1563 Loss: 2.072 | Acc: 32.162% (7616/23680)\n",
            "Epoch 49 Step 740/1563 Loss: 2.072 | Acc: 32.140% (7621/23712)\n",
            "Epoch 49 Step 741/1563 Loss: 2.072 | Acc: 32.151% (7634/23744)\n",
            "Epoch 49 Step 742/1563 Loss: 2.072 | Acc: 32.158% (7646/23776)\n",
            "Epoch 49 Step 743/1563 Loss: 2.072 | Acc: 32.166% (7658/23808)\n",
            "Epoch 49 Step 744/1563 Loss: 2.072 | Acc: 32.156% (7666/23840)\n",
            "Epoch 49 Step 745/1563 Loss: 2.073 | Acc: 32.146% (7674/23872)\n",
            "Epoch 49 Step 746/1563 Loss: 2.073 | Acc: 32.141% (7683/23904)\n",
            "Epoch 49 Step 747/1563 Loss: 2.072 | Acc: 32.140% (7693/23936)\n",
            "Epoch 49 Step 748/1563 Loss: 2.072 | Acc: 32.147% (7705/23968)\n",
            "Epoch 49 Step 749/1563 Loss: 2.072 | Acc: 32.163% (7719/24000)\n",
            "Epoch 49 Step 750/1563 Loss: 2.072 | Acc: 32.153% (7727/24032)\n",
            "Epoch 49 Step 751/1563 Loss: 2.072 | Acc: 32.160% (7739/24064)\n",
            "Epoch 49 Step 752/1563 Loss: 2.072 | Acc: 32.163% (7750/24096)\n",
            "Epoch 49 Step 753/1563 Loss: 2.072 | Acc: 32.158% (7759/24128)\n",
            "Epoch 49 Step 754/1563 Loss: 2.072 | Acc: 32.165% (7771/24160)\n",
            "Epoch 49 Step 755/1563 Loss: 2.071 | Acc: 32.164% (7781/24192)\n",
            "Epoch 49 Step 756/1563 Loss: 2.071 | Acc: 32.166% (7792/24224)\n",
            "Epoch 49 Step 757/1563 Loss: 2.071 | Acc: 32.173% (7804/24256)\n",
            "Epoch 49 Step 758/1563 Loss: 2.071 | Acc: 32.176% (7815/24288)\n",
            "Epoch 49 Step 759/1563 Loss: 2.071 | Acc: 32.159% (7821/24320)\n",
            "Epoch 49 Step 760/1563 Loss: 2.071 | Acc: 32.153% (7830/24352)\n",
            "Epoch 49 Step 761/1563 Loss: 2.071 | Acc: 32.152% (7840/24384)\n",
            "Epoch 49 Step 762/1563 Loss: 2.071 | Acc: 32.147% (7849/24416)\n",
            "Epoch 49 Step 763/1563 Loss: 2.070 | Acc: 32.158% (7862/24448)\n",
            "Epoch 49 Step 764/1563 Loss: 2.071 | Acc: 32.173% (7876/24480)\n",
            "Epoch 49 Step 765/1563 Loss: 2.071 | Acc: 32.164% (7884/24512)\n",
            "Epoch 49 Step 766/1563 Loss: 2.071 | Acc: 32.171% (7896/24544)\n",
            "Epoch 49 Step 767/1563 Loss: 2.072 | Acc: 32.157% (7903/24576)\n",
            "Epoch 49 Step 768/1563 Loss: 2.071 | Acc: 32.164% (7915/24608)\n",
            "Epoch 49 Step 769/1563 Loss: 2.071 | Acc: 32.155% (7923/24640)\n",
            "Epoch 49 Step 770/1563 Loss: 2.070 | Acc: 32.166% (7936/24672)\n",
            "Epoch 49 Step 771/1563 Loss: 2.070 | Acc: 32.173% (7948/24704)\n",
            "Epoch 49 Step 772/1563 Loss: 2.070 | Acc: 32.172% (7958/24736)\n",
            "Epoch 49 Step 773/1563 Loss: 2.070 | Acc: 32.171% (7968/24768)\n",
            "Epoch 49 Step 774/1563 Loss: 2.070 | Acc: 32.165% (7977/24800)\n",
            "Epoch 49 Step 775/1563 Loss: 2.071 | Acc: 32.152% (7984/24832)\n",
            "Epoch 49 Step 776/1563 Loss: 2.070 | Acc: 32.151% (7994/24864)\n",
            "Epoch 49 Step 777/1563 Loss: 2.071 | Acc: 32.142% (8002/24896)\n",
            "Epoch 49 Step 778/1563 Loss: 2.071 | Acc: 32.133% (8010/24928)\n",
            "Epoch 49 Step 779/1563 Loss: 2.070 | Acc: 32.151% (8025/24960)\n",
            "Epoch 49 Step 780/1563 Loss: 2.070 | Acc: 32.162% (8038/24992)\n",
            "Epoch 49 Step 781/1563 Loss: 2.071 | Acc: 32.161% (8048/25024)\n",
            "Epoch 49 Step 782/1563 Loss: 2.070 | Acc: 32.176% (8062/25056)\n",
            "Epoch 49 Step 783/1563 Loss: 2.070 | Acc: 32.167% (8070/25088)\n",
            "Epoch 49 Step 784/1563 Loss: 2.069 | Acc: 32.182% (8084/25120)\n",
            "Epoch 49 Step 785/1563 Loss: 2.069 | Acc: 32.188% (8096/25152)\n",
            "Epoch 49 Step 786/1563 Loss: 2.069 | Acc: 32.183% (8105/25184)\n",
            "Epoch 49 Step 787/1563 Loss: 2.070 | Acc: 32.170% (8112/25216)\n",
            "Epoch 49 Step 788/1563 Loss: 2.070 | Acc: 32.161% (8120/25248)\n",
            "Epoch 49 Step 789/1563 Loss: 2.070 | Acc: 32.156% (8129/25280)\n",
            "Epoch 49 Step 790/1563 Loss: 2.070 | Acc: 32.147% (8137/25312)\n",
            "Epoch 49 Step 791/1563 Loss: 2.070 | Acc: 32.154% (8149/25344)\n",
            "Epoch 49 Step 792/1563 Loss: 2.069 | Acc: 32.152% (8159/25376)\n",
            "Epoch 49 Step 793/1563 Loss: 2.070 | Acc: 32.147% (8168/25408)\n",
            "Epoch 49 Step 794/1563 Loss: 2.070 | Acc: 32.162% (8182/25440)\n",
            "Epoch 49 Step 795/1563 Loss: 2.069 | Acc: 32.184% (8198/25472)\n",
            "Epoch 49 Step 796/1563 Loss: 2.068 | Acc: 32.199% (8212/25504)\n",
            "Epoch 49 Step 797/1563 Loss: 2.069 | Acc: 32.198% (8222/25536)\n",
            "Epoch 49 Step 798/1563 Loss: 2.068 | Acc: 32.204% (8234/25568)\n",
            "Epoch 49 Step 799/1563 Loss: 2.068 | Acc: 32.219% (8248/25600)\n",
            "Epoch 49 Step 800/1563 Loss: 2.068 | Acc: 32.233% (8262/25632)\n",
            "Epoch 49 Step 801/1563 Loss: 2.068 | Acc: 32.236% (8273/25664)\n",
            "Epoch 49 Step 802/1563 Loss: 2.068 | Acc: 32.242% (8285/25696)\n",
            "Epoch 49 Step 803/1563 Loss: 2.068 | Acc: 32.249% (8297/25728)\n",
            "Epoch 49 Step 804/1563 Loss: 2.067 | Acc: 32.255% (8309/25760)\n",
            "Epoch 49 Step 805/1563 Loss: 2.067 | Acc: 32.274% (8324/25792)\n",
            "Epoch 49 Step 806/1563 Loss: 2.067 | Acc: 32.276% (8335/25824)\n",
            "Epoch 49 Step 807/1563 Loss: 2.068 | Acc: 32.287% (8348/25856)\n",
            "Epoch 49 Step 808/1563 Loss: 2.067 | Acc: 32.301% (8362/25888)\n",
            "Epoch 49 Step 809/1563 Loss: 2.067 | Acc: 32.307% (8374/25920)\n",
            "Epoch 49 Step 810/1563 Loss: 2.067 | Acc: 32.310% (8385/25952)\n",
            "Epoch 49 Step 811/1563 Loss: 2.067 | Acc: 32.328% (8400/25984)\n",
            "Epoch 49 Step 812/1563 Loss: 2.067 | Acc: 32.315% (8407/26016)\n",
            "Epoch 49 Step 813/1563 Loss: 2.068 | Acc: 32.321% (8419/26048)\n",
            "Epoch 49 Step 814/1563 Loss: 2.068 | Acc: 32.312% (8427/26080)\n",
            "Epoch 49 Step 815/1563 Loss: 2.068 | Acc: 32.318% (8439/26112)\n",
            "Epoch 49 Step 816/1563 Loss: 2.068 | Acc: 32.302% (8445/26144)\n",
            "Epoch 49 Step 817/1563 Loss: 2.068 | Acc: 32.297% (8454/26176)\n",
            "Epoch 49 Step 818/1563 Loss: 2.068 | Acc: 32.299% (8465/26208)\n",
            "Epoch 49 Step 819/1563 Loss: 2.068 | Acc: 32.321% (8481/26240)\n",
            "Epoch 49 Step 820/1563 Loss: 2.068 | Acc: 32.320% (8491/26272)\n",
            "Epoch 49 Step 821/1563 Loss: 2.067 | Acc: 32.326% (8503/26304)\n",
            "Epoch 49 Step 822/1563 Loss: 2.067 | Acc: 32.332% (8515/26336)\n",
            "Epoch 49 Step 823/1563 Loss: 2.068 | Acc: 32.335% (8526/26368)\n",
            "Epoch 49 Step 824/1563 Loss: 2.068 | Acc: 32.333% (8536/26400)\n",
            "Epoch 49 Step 825/1563 Loss: 2.067 | Acc: 32.343% (8549/26432)\n",
            "Epoch 49 Step 826/1563 Loss: 2.067 | Acc: 32.334% (8557/26464)\n",
            "Epoch 49 Step 827/1563 Loss: 2.068 | Acc: 32.318% (8563/26496)\n",
            "Epoch 49 Step 828/1563 Loss: 2.067 | Acc: 32.324% (8575/26528)\n",
            "Epoch 49 Step 829/1563 Loss: 2.067 | Acc: 32.327% (8586/26560)\n",
            "Epoch 49 Step 830/1563 Loss: 2.068 | Acc: 32.326% (8596/26592)\n",
            "Epoch 49 Step 831/1563 Loss: 2.068 | Acc: 32.328% (8607/26624)\n",
            "Epoch 49 Step 832/1563 Loss: 2.068 | Acc: 32.338% (8620/26656)\n",
            "Epoch 49 Step 833/1563 Loss: 2.068 | Acc: 32.344% (8632/26688)\n",
            "Epoch 49 Step 834/1563 Loss: 2.068 | Acc: 32.339% (8641/26720)\n",
            "Epoch 49 Step 835/1563 Loss: 2.068 | Acc: 32.323% (8647/26752)\n",
            "Epoch 49 Step 836/1563 Loss: 2.067 | Acc: 32.348% (8664/26784)\n",
            "Epoch 49 Step 837/1563 Loss: 2.067 | Acc: 32.339% (8672/26816)\n",
            "Epoch 49 Step 838/1563 Loss: 2.067 | Acc: 32.341% (8683/26848)\n",
            "Epoch 49 Step 839/1563 Loss: 2.067 | Acc: 32.333% (8691/26880)\n",
            "Epoch 49 Step 840/1563 Loss: 2.067 | Acc: 32.328% (8700/26912)\n",
            "Epoch 49 Step 841/1563 Loss: 2.067 | Acc: 32.337% (8713/26944)\n",
            "Epoch 49 Step 842/1563 Loss: 2.067 | Acc: 32.347% (8726/26976)\n",
            "Epoch 49 Step 843/1563 Loss: 2.067 | Acc: 32.346% (8736/27008)\n",
            "Epoch 49 Step 844/1563 Loss: 2.068 | Acc: 32.337% (8744/27040)\n",
            "Epoch 49 Step 845/1563 Loss: 2.068 | Acc: 32.336% (8754/27072)\n",
            "Epoch 49 Step 846/1563 Loss: 2.068 | Acc: 32.346% (8767/27104)\n",
            "Epoch 49 Step 847/1563 Loss: 2.068 | Acc: 32.356% (8780/27136)\n",
            "Epoch 49 Step 848/1563 Loss: 2.068 | Acc: 32.351% (8789/27168)\n",
            "Epoch 49 Step 849/1563 Loss: 2.068 | Acc: 32.353% (8800/27200)\n",
            "Epoch 49 Step 850/1563 Loss: 2.068 | Acc: 32.341% (8807/27232)\n",
            "Epoch 49 Step 851/1563 Loss: 2.068 | Acc: 32.358% (8822/27264)\n",
            "Epoch 49 Step 852/1563 Loss: 2.067 | Acc: 32.367% (8835/27296)\n",
            "Epoch 49 Step 853/1563 Loss: 2.067 | Acc: 32.388% (8851/27328)\n",
            "Epoch 49 Step 854/1563 Loss: 2.067 | Acc: 32.398% (8864/27360)\n",
            "Epoch 49 Step 855/1563 Loss: 2.067 | Acc: 32.396% (8874/27392)\n",
            "Epoch 49 Step 856/1563 Loss: 2.067 | Acc: 32.380% (8880/27424)\n",
            "Epoch 49 Step 857/1563 Loss: 2.067 | Acc: 32.390% (8893/27456)\n",
            "Epoch 49 Step 858/1563 Loss: 2.067 | Acc: 32.381% (8901/27488)\n",
            "Epoch 49 Step 859/1563 Loss: 2.067 | Acc: 32.376% (8910/27520)\n",
            "Epoch 49 Step 860/1563 Loss: 2.068 | Acc: 32.368% (8918/27552)\n",
            "Epoch 49 Step 861/1563 Loss: 2.068 | Acc: 32.363% (8927/27584)\n",
            "Epoch 49 Step 862/1563 Loss: 2.068 | Acc: 32.369% (8939/27616)\n",
            "Epoch 49 Step 863/1563 Loss: 2.067 | Acc: 32.386% (8954/27648)\n",
            "Epoch 49 Step 864/1563 Loss: 2.067 | Acc: 32.395% (8967/27680)\n",
            "Epoch 49 Step 865/1563 Loss: 2.067 | Acc: 32.401% (8979/27712)\n",
            "Epoch 49 Step 866/1563 Loss: 2.066 | Acc: 32.403% (8990/27744)\n",
            "Epoch 49 Step 867/1563 Loss: 2.067 | Acc: 32.384% (8995/27776)\n",
            "Epoch 49 Step 868/1563 Loss: 2.066 | Acc: 32.394% (9008/27808)\n",
            "Epoch 49 Step 869/1563 Loss: 2.066 | Acc: 32.399% (9020/27840)\n",
            "Epoch 49 Step 870/1563 Loss: 2.066 | Acc: 32.412% (9034/27872)\n",
            "Epoch 49 Step 871/1563 Loss: 2.066 | Acc: 32.415% (9045/27904)\n",
            "Epoch 49 Step 872/1563 Loss: 2.066 | Acc: 32.424% (9058/27936)\n",
            "Epoch 49 Step 873/1563 Loss: 2.066 | Acc: 32.437% (9072/27968)\n",
            "Epoch 49 Step 874/1563 Loss: 2.066 | Acc: 32.446% (9085/28000)\n",
            "Epoch 49 Step 875/1563 Loss: 2.066 | Acc: 32.441% (9094/28032)\n",
            "Epoch 49 Step 876/1563 Loss: 2.066 | Acc: 32.447% (9106/28064)\n",
            "Epoch 49 Step 877/1563 Loss: 2.065 | Acc: 32.453% (9118/28096)\n",
            "Epoch 49 Step 878/1563 Loss: 2.065 | Acc: 32.462% (9131/28128)\n",
            "Epoch 49 Step 879/1563 Loss: 2.066 | Acc: 32.457% (9140/28160)\n",
            "Epoch 49 Step 880/1563 Loss: 2.066 | Acc: 32.467% (9153/28192)\n",
            "Epoch 49 Step 881/1563 Loss: 2.066 | Acc: 32.483% (9168/28224)\n",
            "Epoch 49 Step 882/1563 Loss: 2.066 | Acc: 32.496% (9182/28256)\n",
            "Epoch 49 Step 883/1563 Loss: 2.066 | Acc: 32.498% (9193/28288)\n",
            "Epoch 49 Step 884/1563 Loss: 2.066 | Acc: 32.489% (9201/28320)\n",
            "Epoch 49 Step 885/1563 Loss: 2.066 | Acc: 32.495% (9213/28352)\n",
            "Epoch 49 Step 886/1563 Loss: 2.066 | Acc: 32.494% (9223/28384)\n",
            "Epoch 49 Step 887/1563 Loss: 2.066 | Acc: 32.499% (9235/28416)\n",
            "Epoch 49 Step 888/1563 Loss: 2.066 | Acc: 32.501% (9246/28448)\n",
            "Epoch 49 Step 889/1563 Loss: 2.066 | Acc: 32.496% (9255/28480)\n",
            "Epoch 49 Step 890/1563 Loss: 2.066 | Acc: 32.502% (9267/28512)\n",
            "Epoch 49 Step 891/1563 Loss: 2.066 | Acc: 32.508% (9279/28544)\n",
            "Epoch 49 Step 892/1563 Loss: 2.067 | Acc: 32.506% (9289/28576)\n",
            "Epoch 49 Step 893/1563 Loss: 2.067 | Acc: 32.498% (9297/28608)\n",
            "Epoch 49 Step 894/1563 Loss: 2.067 | Acc: 32.497% (9307/28640)\n",
            "Epoch 49 Step 895/1563 Loss: 2.067 | Acc: 32.499% (9318/28672)\n",
            "Epoch 49 Step 896/1563 Loss: 2.067 | Acc: 32.497% (9328/28704)\n",
            "Epoch 49 Step 897/1563 Loss: 2.067 | Acc: 32.499% (9339/28736)\n",
            "Epoch 49 Step 898/1563 Loss: 2.067 | Acc: 32.494% (9348/28768)\n",
            "Epoch 49 Step 899/1563 Loss: 2.067 | Acc: 32.490% (9357/28800)\n",
            "Epoch 49 Step 900/1563 Loss: 2.066 | Acc: 32.506% (9372/28832)\n",
            "Epoch 49 Step 901/1563 Loss: 2.066 | Acc: 32.511% (9384/28864)\n",
            "Epoch 49 Step 902/1563 Loss: 2.066 | Acc: 32.513% (9395/28896)\n",
            "Epoch 49 Step 903/1563 Loss: 2.066 | Acc: 32.515% (9406/28928)\n",
            "Epoch 49 Step 904/1563 Loss: 2.066 | Acc: 32.521% (9418/28960)\n",
            "Epoch 49 Step 905/1563 Loss: 2.066 | Acc: 32.512% (9426/28992)\n",
            "Epoch 49 Step 906/1563 Loss: 2.066 | Acc: 32.518% (9438/29024)\n",
            "Epoch 49 Step 907/1563 Loss: 2.066 | Acc: 32.530% (9452/29056)\n",
            "Epoch 49 Step 908/1563 Loss: 2.066 | Acc: 32.532% (9463/29088)\n",
            "Epoch 49 Step 909/1563 Loss: 2.066 | Acc: 32.534% (9474/29120)\n",
            "Epoch 49 Step 910/1563 Loss: 2.066 | Acc: 32.536% (9485/29152)\n",
            "Epoch 49 Step 911/1563 Loss: 2.066 | Acc: 32.528% (9493/29184)\n",
            "Epoch 49 Step 912/1563 Loss: 2.065 | Acc: 32.554% (9511/29216)\n",
            "Epoch 49 Step 913/1563 Loss: 2.065 | Acc: 32.563% (9524/29248)\n",
            "Epoch 49 Step 914/1563 Loss: 2.065 | Acc: 32.575% (9538/29280)\n",
            "Epoch 49 Step 915/1563 Loss: 2.065 | Acc: 32.581% (9550/29312)\n",
            "Epoch 49 Step 916/1563 Loss: 2.065 | Acc: 32.582% (9561/29344)\n",
            "Epoch 49 Step 917/1563 Loss: 2.064 | Acc: 32.581% (9571/29376)\n",
            "Epoch 49 Step 918/1563 Loss: 2.064 | Acc: 32.583% (9582/29408)\n",
            "Epoch 49 Step 919/1563 Loss: 2.064 | Acc: 32.595% (9596/29440)\n",
            "Epoch 49 Step 920/1563 Loss: 2.064 | Acc: 32.600% (9608/29472)\n",
            "Epoch 49 Step 921/1563 Loss: 2.063 | Acc: 32.616% (9623/29504)\n",
            "Epoch 49 Step 922/1563 Loss: 2.063 | Acc: 32.618% (9634/29536)\n",
            "Epoch 49 Step 923/1563 Loss: 2.063 | Acc: 32.613% (9643/29568)\n",
            "Epoch 49 Step 924/1563 Loss: 2.063 | Acc: 32.608% (9652/29600)\n",
            "Epoch 49 Step 925/1563 Loss: 2.063 | Acc: 32.603% (9661/29632)\n",
            "Epoch 49 Step 926/1563 Loss: 2.063 | Acc: 32.602% (9671/29664)\n",
            "Epoch 49 Step 927/1563 Loss: 2.063 | Acc: 32.604% (9682/29696)\n",
            "Epoch 49 Step 928/1563 Loss: 2.063 | Acc: 32.592% (9689/29728)\n",
            "Epoch 49 Step 929/1563 Loss: 2.063 | Acc: 32.584% (9697/29760)\n",
            "Epoch 49 Step 930/1563 Loss: 2.063 | Acc: 32.586% (9708/29792)\n",
            "Epoch 49 Step 931/1563 Loss: 2.062 | Acc: 32.584% (9718/29824)\n",
            "Epoch 49 Step 932/1563 Loss: 2.062 | Acc: 32.596% (9732/29856)\n",
            "Epoch 49 Step 933/1563 Loss: 2.062 | Acc: 32.608% (9746/29888)\n",
            "Epoch 49 Step 934/1563 Loss: 2.062 | Acc: 32.604% (9755/29920)\n",
            "Epoch 49 Step 935/1563 Loss: 2.063 | Acc: 32.599% (9764/29952)\n",
            "Epoch 49 Step 936/1563 Loss: 2.063 | Acc: 32.604% (9776/29984)\n",
            "Epoch 49 Step 937/1563 Loss: 2.063 | Acc: 32.609% (9788/30016)\n",
            "Epoch 49 Step 938/1563 Loss: 2.062 | Acc: 32.624% (9803/30048)\n",
            "Epoch 49 Step 939/1563 Loss: 2.062 | Acc: 32.636% (9817/30080)\n",
            "Epoch 49 Step 940/1563 Loss: 2.062 | Acc: 32.638% (9828/30112)\n",
            "Epoch 49 Step 941/1563 Loss: 2.063 | Acc: 32.627% (9835/30144)\n",
            "Epoch 49 Step 942/1563 Loss: 2.062 | Acc: 32.645% (9851/30176)\n",
            "Epoch 49 Step 943/1563 Loss: 2.062 | Acc: 32.644% (9861/30208)\n",
            "Epoch 49 Step 944/1563 Loss: 2.063 | Acc: 32.639% (9870/30240)\n",
            "Epoch 49 Step 945/1563 Loss: 2.062 | Acc: 32.647% (9883/30272)\n",
            "Epoch 49 Step 946/1563 Loss: 2.062 | Acc: 32.643% (9892/30304)\n",
            "Epoch 49 Step 947/1563 Loss: 2.062 | Acc: 32.651% (9905/30336)\n",
            "Epoch 49 Step 948/1563 Loss: 2.062 | Acc: 32.656% (9917/30368)\n",
            "Epoch 49 Step 949/1563 Loss: 2.062 | Acc: 32.658% (9928/30400)\n",
            "Epoch 49 Step 950/1563 Loss: 2.061 | Acc: 32.673% (9943/30432)\n",
            "Epoch 49 Step 951/1563 Loss: 2.061 | Acc: 32.678% (9955/30464)\n",
            "Epoch 49 Step 952/1563 Loss: 2.061 | Acc: 32.673% (9964/30496)\n",
            "Epoch 49 Step 953/1563 Loss: 2.061 | Acc: 32.668% (9973/30528)\n",
            "Epoch 49 Step 954/1563 Loss: 2.061 | Acc: 32.687% (9989/30560)\n",
            "Epoch 49 Step 955/1563 Loss: 2.062 | Acc: 32.672% (9995/30592)\n",
            "Epoch 49 Step 956/1563 Loss: 2.062 | Acc: 32.674% (10006/30624)\n",
            "Epoch 49 Step 957/1563 Loss: 2.062 | Acc: 32.689% (10021/30656)\n",
            "Epoch 49 Step 958/1563 Loss: 2.061 | Acc: 32.684% (10030/30688)\n",
            "Epoch 49 Step 959/1563 Loss: 2.062 | Acc: 32.676% (10038/30720)\n",
            "Epoch 49 Step 960/1563 Loss: 2.062 | Acc: 32.684% (10051/30752)\n",
            "Epoch 49 Step 961/1563 Loss: 2.062 | Acc: 32.686% (10062/30784)\n",
            "Epoch 49 Step 962/1563 Loss: 2.061 | Acc: 32.697% (10076/30816)\n",
            "Epoch 49 Step 963/1563 Loss: 2.061 | Acc: 32.689% (10084/30848)\n",
            "Epoch 49 Step 964/1563 Loss: 2.061 | Acc: 32.698% (10097/30880)\n",
            "Epoch 49 Step 965/1563 Loss: 2.061 | Acc: 32.703% (10109/30912)\n",
            "Epoch 49 Step 966/1563 Loss: 2.060 | Acc: 32.714% (10123/30944)\n",
            "Epoch 49 Step 967/1563 Loss: 2.061 | Acc: 32.716% (10134/30976)\n",
            "Epoch 49 Step 968/1563 Loss: 2.060 | Acc: 32.721% (10146/31008)\n",
            "Epoch 49 Step 969/1563 Loss: 2.061 | Acc: 32.706% (10152/31040)\n",
            "Epoch 49 Step 970/1563 Loss: 2.061 | Acc: 32.705% (10162/31072)\n",
            "Epoch 49 Step 971/1563 Loss: 2.061 | Acc: 32.722% (10178/31104)\n",
            "Epoch 49 Step 972/1563 Loss: 2.060 | Acc: 32.721% (10188/31136)\n",
            "Epoch 49 Step 973/1563 Loss: 2.060 | Acc: 32.732% (10202/31168)\n",
            "Epoch 49 Step 974/1563 Loss: 2.060 | Acc: 32.744% (10216/31200)\n",
            "Epoch 49 Step 975/1563 Loss: 2.060 | Acc: 32.739% (10225/31232)\n",
            "Epoch 49 Step 976/1563 Loss: 2.060 | Acc: 32.737% (10235/31264)\n",
            "Epoch 49 Step 977/1563 Loss: 2.060 | Acc: 32.733% (10244/31296)\n",
            "Epoch 49 Step 978/1563 Loss: 2.061 | Acc: 32.725% (10252/31328)\n",
            "Epoch 49 Step 979/1563 Loss: 2.061 | Acc: 32.723% (10262/31360)\n",
            "Epoch 49 Step 980/1563 Loss: 2.061 | Acc: 32.719% (10271/31392)\n",
            "Epoch 49 Step 981/1563 Loss: 2.060 | Acc: 32.730% (10285/31424)\n",
            "Epoch 49 Step 982/1563 Loss: 2.060 | Acc: 32.735% (10297/31456)\n",
            "Epoch 49 Step 983/1563 Loss: 2.060 | Acc: 32.743% (10310/31488)\n",
            "Epoch 49 Step 984/1563 Loss: 2.060 | Acc: 32.754% (10324/31520)\n",
            "Epoch 49 Step 985/1563 Loss: 2.059 | Acc: 32.755% (10335/31552)\n",
            "Epoch 49 Step 986/1563 Loss: 2.059 | Acc: 32.757% (10346/31584)\n",
            "Epoch 49 Step 987/1563 Loss: 2.059 | Acc: 32.752% (10355/31616)\n",
            "Epoch 49 Step 988/1563 Loss: 2.060 | Acc: 32.741% (10362/31648)\n",
            "Epoch 49 Step 989/1563 Loss: 2.060 | Acc: 32.740% (10372/31680)\n",
            "Epoch 49 Step 990/1563 Loss: 2.060 | Acc: 32.751% (10386/31712)\n",
            "Epoch 49 Step 991/1563 Loss: 2.060 | Acc: 32.746% (10395/31744)\n",
            "Epoch 49 Step 992/1563 Loss: 2.060 | Acc: 32.751% (10407/31776)\n",
            "Epoch 49 Step 993/1563 Loss: 2.059 | Acc: 32.762% (10421/31808)\n",
            "Epoch 49 Step 994/1563 Loss: 2.059 | Acc: 32.764% (10432/31840)\n",
            "Epoch 49 Step 995/1563 Loss: 2.059 | Acc: 32.759% (10441/31872)\n",
            "Epoch 49 Step 996/1563 Loss: 2.060 | Acc: 32.767% (10454/31904)\n",
            "Epoch 49 Step 997/1563 Loss: 2.060 | Acc: 32.766% (10464/31936)\n",
            "Epoch 49 Step 998/1563 Loss: 2.059 | Acc: 32.758% (10472/31968)\n",
            "Epoch 49 Step 999/1563 Loss: 2.060 | Acc: 32.753% (10481/32000)\n",
            "Epoch 49 Step 1000/1563 Loss: 2.059 | Acc: 32.758% (10493/32032)\n",
            "Epoch 49 Step 1001/1563 Loss: 2.060 | Acc: 32.753% (10502/32064)\n",
            "Epoch 49 Step 1002/1563 Loss: 2.060 | Acc: 32.761% (10515/32096)\n",
            "Epoch 49 Step 1003/1563 Loss: 2.059 | Acc: 32.778% (10531/32128)\n",
            "Epoch 49 Step 1004/1563 Loss: 2.060 | Acc: 32.771% (10539/32160)\n",
            "Epoch 49 Step 1005/1563 Loss: 2.060 | Acc: 32.763% (10547/32192)\n",
            "Epoch 49 Step 1006/1563 Loss: 2.060 | Acc: 32.764% (10558/32224)\n",
            "Epoch 49 Step 1007/1563 Loss: 2.060 | Acc: 32.757% (10566/32256)\n",
            "Epoch 49 Step 1008/1563 Loss: 2.060 | Acc: 32.746% (10573/32288)\n",
            "Epoch 49 Step 1009/1563 Loss: 2.060 | Acc: 32.760% (10588/32320)\n",
            "Epoch 49 Step 1010/1563 Loss: 2.060 | Acc: 32.771% (10602/32352)\n",
            "Epoch 49 Step 1011/1563 Loss: 2.060 | Acc: 32.757% (10608/32384)\n",
            "Epoch 49 Step 1012/1563 Loss: 2.059 | Acc: 32.771% (10623/32416)\n",
            "Epoch 49 Step 1013/1563 Loss: 2.060 | Acc: 32.760% (10630/32448)\n",
            "Epoch 49 Step 1014/1563 Loss: 2.060 | Acc: 32.749% (10637/32480)\n",
            "Epoch 49 Step 1015/1563 Loss: 2.060 | Acc: 32.763% (10652/32512)\n",
            "Epoch 49 Step 1016/1563 Loss: 2.060 | Acc: 32.759% (10661/32544)\n",
            "Epoch 49 Step 1017/1563 Loss: 2.059 | Acc: 32.776% (10677/32576)\n",
            "Epoch 49 Step 1018/1563 Loss: 2.059 | Acc: 32.771% (10686/32608)\n",
            "Epoch 49 Step 1019/1563 Loss: 2.059 | Acc: 32.773% (10697/32640)\n",
            "Epoch 49 Step 1020/1563 Loss: 2.059 | Acc: 32.786% (10712/32672)\n",
            "Epoch 49 Step 1021/1563 Loss: 2.059 | Acc: 32.788% (10723/32704)\n",
            "Epoch 49 Step 1022/1563 Loss: 2.059 | Acc: 32.780% (10731/32736)\n",
            "Epoch 49 Step 1023/1563 Loss: 2.059 | Acc: 32.776% (10740/32768)\n",
            "Epoch 49 Step 1024/1563 Loss: 2.059 | Acc: 32.787% (10754/32800)\n",
            "Epoch 49 Step 1025/1563 Loss: 2.058 | Acc: 32.791% (10766/32832)\n",
            "Epoch 49 Step 1026/1563 Loss: 2.058 | Acc: 32.784% (10774/32864)\n",
            "Epoch 49 Step 1027/1563 Loss: 2.058 | Acc: 32.797% (10789/32896)\n",
            "Epoch 49 Step 1028/1563 Loss: 2.058 | Acc: 32.784% (10795/32928)\n",
            "Epoch 49 Step 1029/1563 Loss: 2.058 | Acc: 32.767% (10800/32960)\n",
            "Epoch 49 Step 1030/1563 Loss: 2.058 | Acc: 32.753% (10806/32992)\n",
            "Epoch 49 Step 1031/1563 Loss: 2.058 | Acc: 32.764% (10820/33024)\n",
            "Epoch 49 Step 1032/1563 Loss: 2.058 | Acc: 32.766% (10831/33056)\n",
            "Epoch 49 Step 1033/1563 Loss: 2.058 | Acc: 32.755% (10838/33088)\n",
            "Epoch 49 Step 1034/1563 Loss: 2.058 | Acc: 32.748% (10846/33120)\n",
            "Epoch 49 Step 1035/1563 Loss: 2.058 | Acc: 32.752% (10858/33152)\n",
            "Epoch 49 Step 1036/1563 Loss: 2.058 | Acc: 32.760% (10871/33184)\n",
            "Epoch 49 Step 1037/1563 Loss: 2.058 | Acc: 32.746% (10877/33216)\n",
            "Epoch 49 Step 1038/1563 Loss: 2.058 | Acc: 32.754% (10890/33248)\n",
            "Epoch 49 Step 1039/1563 Loss: 2.058 | Acc: 32.746% (10898/33280)\n",
            "Epoch 49 Step 1040/1563 Loss: 2.058 | Acc: 32.736% (10905/33312)\n",
            "Epoch 49 Step 1041/1563 Loss: 2.058 | Acc: 32.738% (10916/33344)\n",
            "Epoch 49 Step 1042/1563 Loss: 2.058 | Acc: 32.751% (10931/33376)\n",
            "Epoch 49 Step 1043/1563 Loss: 2.058 | Acc: 32.744% (10939/33408)\n",
            "Epoch 49 Step 1044/1563 Loss: 2.058 | Acc: 32.736% (10947/33440)\n",
            "Epoch 49 Step 1045/1563 Loss: 2.058 | Acc: 32.744% (10960/33472)\n",
            "Epoch 49 Step 1046/1563 Loss: 2.058 | Acc: 32.742% (10970/33504)\n",
            "Epoch 49 Step 1047/1563 Loss: 2.058 | Acc: 32.744% (10981/33536)\n",
            "Epoch 49 Step 1048/1563 Loss: 2.058 | Acc: 32.731% (10987/33568)\n",
            "Epoch 49 Step 1049/1563 Loss: 2.057 | Acc: 32.741% (11001/33600)\n",
            "Epoch 49 Step 1050/1563 Loss: 2.057 | Acc: 32.749% (11014/33632)\n",
            "Epoch 49 Step 1051/1563 Loss: 2.057 | Acc: 32.738% (11021/33664)\n",
            "Epoch 49 Step 1052/1563 Loss: 2.057 | Acc: 32.731% (11029/33696)\n",
            "Epoch 49 Step 1053/1563 Loss: 2.057 | Acc: 32.729% (11039/33728)\n",
            "Epoch 49 Step 1054/1563 Loss: 2.058 | Acc: 32.713% (11044/33760)\n",
            "Epoch 49 Step 1055/1563 Loss: 2.058 | Acc: 32.727% (11059/33792)\n",
            "Epoch 49 Step 1056/1563 Loss: 2.058 | Acc: 32.722% (11068/33824)\n",
            "Epoch 49 Step 1057/1563 Loss: 2.058 | Acc: 32.721% (11078/33856)\n",
            "Epoch 49 Step 1058/1563 Loss: 2.058 | Acc: 32.717% (11087/33888)\n",
            "Epoch 49 Step 1059/1563 Loss: 2.058 | Acc: 32.703% (11093/33920)\n",
            "Epoch 49 Step 1060/1563 Loss: 2.058 | Acc: 32.696% (11101/33952)\n",
            "Epoch 49 Step 1061/1563 Loss: 2.058 | Acc: 32.695% (11111/33984)\n",
            "Epoch 49 Step 1062/1563 Loss: 2.058 | Acc: 32.693% (11121/34016)\n",
            "Epoch 49 Step 1063/1563 Loss: 2.058 | Acc: 32.695% (11132/34048)\n",
            "Epoch 49 Step 1064/1563 Loss: 2.058 | Acc: 32.688% (11140/34080)\n",
            "Epoch 49 Step 1065/1563 Loss: 2.059 | Acc: 32.678% (11147/34112)\n",
            "Epoch 49 Step 1066/1563 Loss: 2.060 | Acc: 32.659% (11151/34144)\n",
            "Epoch 49 Step 1067/1563 Loss: 2.059 | Acc: 32.672% (11166/34176)\n",
            "Epoch 49 Step 1068/1563 Loss: 2.059 | Acc: 32.668% (11175/34208)\n",
            "Epoch 49 Step 1069/1563 Loss: 2.059 | Acc: 32.658% (11182/34240)\n",
            "Epoch 49 Step 1070/1563 Loss: 2.059 | Acc: 32.659% (11193/34272)\n",
            "Epoch 49 Step 1071/1563 Loss: 2.059 | Acc: 32.655% (11202/34304)\n",
            "Epoch 49 Step 1072/1563 Loss: 2.059 | Acc: 32.654% (11212/34336)\n",
            "Epoch 49 Step 1073/1563 Loss: 2.059 | Acc: 32.652% (11222/34368)\n",
            "Epoch 49 Step 1074/1563 Loss: 2.059 | Acc: 32.637% (11227/34400)\n",
            "Epoch 49 Step 1075/1563 Loss: 2.060 | Acc: 32.632% (11236/34432)\n",
            "Epoch 49 Step 1076/1563 Loss: 2.059 | Acc: 32.640% (11249/34464)\n",
            "Epoch 49 Step 1077/1563 Loss: 2.059 | Acc: 32.633% (11257/34496)\n",
            "Epoch 49 Step 1078/1563 Loss: 2.060 | Acc: 32.637% (11269/34528)\n",
            "Epoch 49 Step 1079/1563 Loss: 2.060 | Acc: 32.636% (11279/34560)\n",
            "Epoch 49 Step 1080/1563 Loss: 2.060 | Acc: 32.638% (11290/34592)\n",
            "Epoch 49 Step 1081/1563 Loss: 2.060 | Acc: 32.645% (11303/34624)\n",
            "Epoch 49 Step 1082/1563 Loss: 2.060 | Acc: 32.641% (11312/34656)\n",
            "Epoch 49 Step 1083/1563 Loss: 2.060 | Acc: 32.637% (11321/34688)\n",
            "Epoch 49 Step 1084/1563 Loss: 2.060 | Acc: 32.653% (11337/34720)\n",
            "Epoch 49 Step 1085/1563 Loss: 2.060 | Acc: 32.643% (11344/34752)\n",
            "Epoch 49 Step 1086/1563 Loss: 2.060 | Acc: 32.641% (11354/34784)\n",
            "Epoch 49 Step 1087/1563 Loss: 2.059 | Acc: 32.640% (11364/34816)\n",
            "Epoch 49 Step 1088/1563 Loss: 2.059 | Acc: 32.639% (11374/34848)\n",
            "Epoch 49 Step 1089/1563 Loss: 2.059 | Acc: 32.629% (11381/34880)\n",
            "Epoch 49 Step 1090/1563 Loss: 2.060 | Acc: 32.622% (11389/34912)\n",
            "Epoch 49 Step 1091/1563 Loss: 2.060 | Acc: 32.629% (11402/34944)\n",
            "Epoch 49 Step 1092/1563 Loss: 2.059 | Acc: 32.642% (11417/34976)\n",
            "Epoch 49 Step 1093/1563 Loss: 2.059 | Acc: 32.627% (11422/35008)\n",
            "Epoch 49 Step 1094/1563 Loss: 2.059 | Acc: 32.631% (11434/35040)\n",
            "Epoch 49 Step 1095/1563 Loss: 2.059 | Acc: 32.633% (11445/35072)\n",
            "Epoch 49 Step 1096/1563 Loss: 2.060 | Acc: 32.632% (11455/35104)\n",
            "Epoch 49 Step 1097/1563 Loss: 2.060 | Acc: 32.642% (11469/35136)\n",
            "Epoch 49 Step 1098/1563 Loss: 2.060 | Acc: 32.643% (11480/35168)\n",
            "Epoch 49 Step 1099/1563 Loss: 2.060 | Acc: 32.642% (11490/35200)\n",
            "Epoch 49 Step 1100/1563 Loss: 2.060 | Acc: 32.649% (11503/35232)\n",
            "Epoch 49 Step 1101/1563 Loss: 2.060 | Acc: 32.642% (11511/35264)\n",
            "Epoch 49 Step 1102/1563 Loss: 2.060 | Acc: 32.647% (11523/35296)\n",
            "Epoch 49 Step 1103/1563 Loss: 2.060 | Acc: 32.648% (11534/35328)\n",
            "Epoch 49 Step 1104/1563 Loss: 2.061 | Acc: 32.644% (11543/35360)\n",
            "Epoch 49 Step 1105/1563 Loss: 2.061 | Acc: 32.649% (11555/35392)\n",
            "Epoch 49 Step 1106/1563 Loss: 2.061 | Acc: 32.650% (11566/35424)\n",
            "Epoch 49 Step 1107/1563 Loss: 2.061 | Acc: 32.643% (11574/35456)\n",
            "Epoch 49 Step 1108/1563 Loss: 2.061 | Acc: 32.645% (11585/35488)\n",
            "Epoch 49 Step 1109/1563 Loss: 2.061 | Acc: 32.641% (11594/35520)\n",
            "Epoch 49 Step 1110/1563 Loss: 2.061 | Acc: 32.640% (11604/35552)\n",
            "Epoch 49 Step 1111/1563 Loss: 2.061 | Acc: 32.630% (11611/35584)\n",
            "Epoch 49 Step 1112/1563 Loss: 2.061 | Acc: 32.617% (11617/35616)\n",
            "Epoch 49 Step 1113/1563 Loss: 2.061 | Acc: 32.613% (11626/35648)\n",
            "Epoch 49 Step 1114/1563 Loss: 2.061 | Acc: 32.615% (11637/35680)\n",
            "Epoch 49 Step 1115/1563 Loss: 2.060 | Acc: 32.616% (11648/35712)\n",
            "Epoch 49 Step 1116/1563 Loss: 2.061 | Acc: 32.618% (11659/35744)\n",
            "Epoch 49 Step 1117/1563 Loss: 2.061 | Acc: 32.611% (11667/35776)\n",
            "Epoch 49 Step 1118/1563 Loss: 2.060 | Acc: 32.613% (11678/35808)\n",
            "Epoch 49 Step 1119/1563 Loss: 2.060 | Acc: 32.612% (11688/35840)\n",
            "Epoch 49 Step 1120/1563 Loss: 2.061 | Acc: 32.602% (11695/35872)\n",
            "Epoch 49 Step 1121/1563 Loss: 2.061 | Acc: 32.604% (11706/35904)\n",
            "Epoch 49 Step 1122/1563 Loss: 2.061 | Acc: 32.597% (11714/35936)\n",
            "Epoch 49 Step 1123/1563 Loss: 2.061 | Acc: 32.598% (11725/35968)\n",
            "Epoch 49 Step 1124/1563 Loss: 2.061 | Acc: 32.592% (11733/36000)\n",
            "Epoch 49 Step 1125/1563 Loss: 2.061 | Acc: 32.596% (11745/36032)\n",
            "Epoch 49 Step 1126/1563 Loss: 2.061 | Acc: 32.592% (11754/36064)\n",
            "Epoch 49 Step 1127/1563 Loss: 2.061 | Acc: 32.596% (11766/36096)\n",
            "Epoch 49 Step 1128/1563 Loss: 2.060 | Acc: 32.609% (11781/36128)\n",
            "Epoch 49 Step 1129/1563 Loss: 2.060 | Acc: 32.602% (11789/36160)\n",
            "Epoch 49 Step 1130/1563 Loss: 2.060 | Acc: 32.607% (11801/36192)\n",
            "Epoch 49 Step 1131/1563 Loss: 2.061 | Acc: 32.603% (11810/36224)\n",
            "Epoch 49 Step 1132/1563 Loss: 2.060 | Acc: 32.604% (11821/36256)\n",
            "Epoch 49 Step 1133/1563 Loss: 2.060 | Acc: 32.620% (11837/36288)\n",
            "Epoch 49 Step 1134/1563 Loss: 2.060 | Acc: 32.613% (11845/36320)\n",
            "Epoch 49 Step 1135/1563 Loss: 2.061 | Acc: 32.612% (11855/36352)\n",
            "Epoch 49 Step 1136/1563 Loss: 2.061 | Acc: 32.610% (11865/36384)\n",
            "Epoch 49 Step 1137/1563 Loss: 2.060 | Acc: 32.620% (11879/36416)\n",
            "Epoch 49 Step 1138/1563 Loss: 2.060 | Acc: 32.630% (11893/36448)\n",
            "Epoch 49 Step 1139/1563 Loss: 2.060 | Acc: 32.629% (11903/36480)\n",
            "Epoch 49 Step 1140/1563 Loss: 2.060 | Acc: 32.641% (11918/36512)\n",
            "Epoch 49 Step 1141/1563 Loss: 2.060 | Acc: 32.640% (11928/36544)\n",
            "Epoch 49 Step 1142/1563 Loss: 2.060 | Acc: 32.636% (11937/36576)\n",
            "Epoch 49 Step 1143/1563 Loss: 2.060 | Acc: 32.632% (11946/36608)\n",
            "Epoch 49 Step 1144/1563 Loss: 2.060 | Acc: 32.634% (11957/36640)\n",
            "Epoch 49 Step 1145/1563 Loss: 2.060 | Acc: 32.630% (11966/36672)\n",
            "Epoch 49 Step 1146/1563 Loss: 2.060 | Acc: 32.623% (11974/36704)\n",
            "Epoch 49 Step 1147/1563 Loss: 2.061 | Acc: 32.617% (11982/36736)\n",
            "Epoch 49 Step 1148/1563 Loss: 2.061 | Acc: 32.618% (11993/36768)\n",
            "Epoch 49 Step 1149/1563 Loss: 2.061 | Acc: 32.609% (12000/36800)\n",
            "Epoch 49 Step 1150/1563 Loss: 2.061 | Acc: 32.602% (12008/36832)\n",
            "Epoch 49 Step 1151/1563 Loss: 2.061 | Acc: 32.593% (12015/36864)\n",
            "Epoch 49 Step 1152/1563 Loss: 2.062 | Acc: 32.592% (12025/36896)\n",
            "Epoch 49 Step 1153/1563 Loss: 2.061 | Acc: 32.599% (12038/36928)\n",
            "Epoch 49 Step 1154/1563 Loss: 2.062 | Acc: 32.606% (12051/36960)\n",
            "Epoch 49 Step 1155/1563 Loss: 2.062 | Acc: 32.591% (12056/36992)\n",
            "Epoch 49 Step 1156/1563 Loss: 2.062 | Acc: 32.598% (12069/37024)\n",
            "Epoch 49 Step 1157/1563 Loss: 2.062 | Acc: 32.599% (12080/37056)\n",
            "Epoch 49 Step 1158/1563 Loss: 2.061 | Acc: 32.612% (12095/37088)\n",
            "Epoch 49 Step 1159/1563 Loss: 2.061 | Acc: 32.610% (12105/37120)\n",
            "Epoch 49 Step 1160/1563 Loss: 2.062 | Acc: 32.590% (12108/37152)\n",
            "Epoch 49 Step 1161/1563 Loss: 2.062 | Acc: 32.595% (12120/37184)\n",
            "Epoch 49 Step 1162/1563 Loss: 2.062 | Acc: 32.591% (12129/37216)\n",
            "Epoch 49 Step 1163/1563 Loss: 2.062 | Acc: 32.595% (12141/37248)\n",
            "Epoch 49 Step 1164/1563 Loss: 2.062 | Acc: 32.599% (12153/37280)\n",
            "Epoch 49 Step 1165/1563 Loss: 2.062 | Acc: 32.609% (12167/37312)\n",
            "Epoch 49 Step 1166/1563 Loss: 2.062 | Acc: 32.616% (12180/37344)\n",
            "Epoch 49 Step 1167/1563 Loss: 2.062 | Acc: 32.617% (12191/37376)\n",
            "Epoch 49 Step 1168/1563 Loss: 2.062 | Acc: 32.613% (12200/37408)\n",
            "Epoch 49 Step 1169/1563 Loss: 2.062 | Acc: 32.612% (12210/37440)\n",
            "Epoch 49 Step 1170/1563 Loss: 2.062 | Acc: 32.606% (12218/37472)\n",
            "Epoch 49 Step 1171/1563 Loss: 2.062 | Acc: 32.602% (12227/37504)\n",
            "Epoch 49 Step 1172/1563 Loss: 2.062 | Acc: 32.601% (12237/37536)\n",
            "Epoch 49 Step 1173/1563 Loss: 2.062 | Acc: 32.597% (12246/37568)\n",
            "Epoch 49 Step 1174/1563 Loss: 2.062 | Acc: 32.593% (12255/37600)\n",
            "Epoch 49 Step 1175/1563 Loss: 2.062 | Acc: 32.597% (12267/37632)\n",
            "Epoch 49 Step 1176/1563 Loss: 2.062 | Acc: 32.604% (12280/37664)\n",
            "Epoch 49 Step 1177/1563 Loss: 2.062 | Acc: 32.614% (12294/37696)\n",
            "Epoch 49 Step 1178/1563 Loss: 2.062 | Acc: 32.602% (12300/37728)\n",
            "Epoch 49 Step 1179/1563 Loss: 2.062 | Acc: 32.601% (12310/37760)\n",
            "Epoch 49 Step 1180/1563 Loss: 2.062 | Acc: 32.615% (12326/37792)\n",
            "Epoch 49 Step 1181/1563 Loss: 2.061 | Acc: 32.625% (12340/37824)\n",
            "Epoch 49 Step 1182/1563 Loss: 2.061 | Acc: 32.637% (12355/37856)\n",
            "Epoch 49 Step 1183/1563 Loss: 2.061 | Acc: 32.644% (12368/37888)\n",
            "Epoch 49 Step 1184/1563 Loss: 2.061 | Acc: 32.642% (12378/37920)\n",
            "Epoch 49 Step 1185/1563 Loss: 2.060 | Acc: 32.641% (12388/37952)\n",
            "Epoch 49 Step 1186/1563 Loss: 2.061 | Acc: 32.640% (12398/37984)\n",
            "Epoch 49 Step 1187/1563 Loss: 2.060 | Acc: 32.636% (12407/38016)\n",
            "Epoch 49 Step 1188/1563 Loss: 2.060 | Acc: 32.632% (12416/38048)\n",
            "Epoch 49 Step 1189/1563 Loss: 2.061 | Acc: 32.623% (12423/38080)\n",
            "Epoch 49 Step 1190/1563 Loss: 2.060 | Acc: 32.625% (12434/38112)\n",
            "Epoch 49 Step 1191/1563 Loss: 2.061 | Acc: 32.616% (12441/38144)\n",
            "Epoch 49 Step 1192/1563 Loss: 2.061 | Acc: 32.609% (12449/38176)\n",
            "Epoch 49 Step 1193/1563 Loss: 2.061 | Acc: 32.598% (12455/38208)\n",
            "Epoch 49 Step 1194/1563 Loss: 2.061 | Acc: 32.592% (12463/38240)\n",
            "Epoch 49 Step 1195/1563 Loss: 2.061 | Acc: 32.601% (12477/38272)\n",
            "Epoch 49 Step 1196/1563 Loss: 2.061 | Acc: 32.602% (12488/38304)\n",
            "Epoch 49 Step 1197/1563 Loss: 2.061 | Acc: 32.604% (12499/38336)\n",
            "Epoch 49 Step 1198/1563 Loss: 2.061 | Acc: 32.613% (12513/38368)\n",
            "Epoch 49 Step 1199/1563 Loss: 2.061 | Acc: 32.607% (12521/38400)\n",
            "Epoch 49 Step 1200/1563 Loss: 2.061 | Acc: 32.590% (12525/38432)\n",
            "Epoch 49 Step 1201/1563 Loss: 2.061 | Acc: 32.579% (12531/38464)\n",
            "Epoch 49 Step 1202/1563 Loss: 2.062 | Acc: 32.577% (12541/38496)\n",
            "Epoch 49 Step 1203/1563 Loss: 2.062 | Acc: 32.579% (12552/38528)\n",
            "Epoch 49 Step 1204/1563 Loss: 2.062 | Acc: 32.562% (12556/38560)\n",
            "Epoch 49 Step 1205/1563 Loss: 2.062 | Acc: 32.574% (12571/38592)\n",
            "Epoch 49 Step 1206/1563 Loss: 2.062 | Acc: 32.573% (12581/38624)\n",
            "Epoch 49 Step 1207/1563 Loss: 2.062 | Acc: 32.572% (12591/38656)\n",
            "Epoch 49 Step 1208/1563 Loss: 2.062 | Acc: 32.589% (12608/38688)\n",
            "Epoch 49 Step 1209/1563 Loss: 2.062 | Acc: 32.585% (12617/38720)\n",
            "Epoch 49 Step 1210/1563 Loss: 2.061 | Acc: 32.605% (12635/38752)\n",
            "Epoch 49 Step 1211/1563 Loss: 2.061 | Acc: 32.604% (12645/38784)\n",
            "Epoch 49 Step 1212/1563 Loss: 2.061 | Acc: 32.600% (12654/38816)\n",
            "Epoch 49 Step 1213/1563 Loss: 2.061 | Acc: 32.614% (12670/38848)\n",
            "Epoch 49 Step 1214/1563 Loss: 2.061 | Acc: 32.616% (12681/38880)\n",
            "Epoch 49 Step 1215/1563 Loss: 2.061 | Acc: 32.612% (12690/38912)\n",
            "Epoch 49 Step 1216/1563 Loss: 2.061 | Acc: 32.619% (12703/38944)\n",
            "Epoch 49 Step 1217/1563 Loss: 2.061 | Acc: 32.623% (12715/38976)\n",
            "Epoch 49 Step 1218/1563 Loss: 2.061 | Acc: 32.619% (12724/39008)\n",
            "Epoch 49 Step 1219/1563 Loss: 2.061 | Acc: 32.600% (12727/39040)\n",
            "Epoch 49 Step 1220/1563 Loss: 2.061 | Acc: 32.609% (12741/39072)\n",
            "Epoch 49 Step 1221/1563 Loss: 2.060 | Acc: 32.616% (12754/39104)\n",
            "Epoch 49 Step 1222/1563 Loss: 2.060 | Acc: 32.620% (12766/39136)\n",
            "Epoch 49 Step 1223/1563 Loss: 2.060 | Acc: 32.621% (12777/39168)\n",
            "Epoch 49 Step 1224/1563 Loss: 2.060 | Acc: 32.628% (12790/39200)\n",
            "Epoch 49 Step 1225/1563 Loss: 2.060 | Acc: 32.634% (12803/39232)\n",
            "Epoch 49 Step 1226/1563 Loss: 2.060 | Acc: 32.633% (12813/39264)\n",
            "Epoch 49 Step 1227/1563 Loss: 2.060 | Acc: 32.629% (12822/39296)\n",
            "Epoch 49 Step 1228/1563 Loss: 2.059 | Acc: 32.638% (12836/39328)\n",
            "Epoch 49 Step 1229/1563 Loss: 2.059 | Acc: 32.640% (12847/39360)\n",
            "Epoch 49 Step 1230/1563 Loss: 2.059 | Acc: 32.646% (12860/39392)\n",
            "Epoch 49 Step 1231/1563 Loss: 2.059 | Acc: 32.635% (12866/39424)\n",
            "Epoch 49 Step 1232/1563 Loss: 2.059 | Acc: 32.629% (12874/39456)\n",
            "Epoch 49 Step 1233/1563 Loss: 2.059 | Acc: 32.625% (12883/39488)\n",
            "Epoch 49 Step 1234/1563 Loss: 2.059 | Acc: 32.629% (12895/39520)\n",
            "Epoch 49 Step 1235/1563 Loss: 2.059 | Acc: 32.630% (12906/39552)\n",
            "Epoch 49 Step 1236/1563 Loss: 2.059 | Acc: 32.622% (12913/39584)\n",
            "Epoch 49 Step 1237/1563 Loss: 2.059 | Acc: 32.618% (12922/39616)\n",
            "Epoch 49 Step 1238/1563 Loss: 2.059 | Acc: 32.625% (12935/39648)\n",
            "Epoch 49 Step 1239/1563 Loss: 2.059 | Acc: 32.623% (12945/39680)\n",
            "Epoch 49 Step 1240/1563 Loss: 2.059 | Acc: 32.625% (12956/39712)\n",
            "Epoch 49 Step 1241/1563 Loss: 2.059 | Acc: 32.621% (12965/39744)\n",
            "Epoch 49 Step 1242/1563 Loss: 2.059 | Acc: 32.618% (12974/39776)\n",
            "Epoch 49 Step 1243/1563 Loss: 2.059 | Acc: 32.617% (12984/39808)\n",
            "Epoch 49 Step 1244/1563 Loss: 2.059 | Acc: 32.608% (12991/39840)\n",
            "Epoch 49 Step 1245/1563 Loss: 2.058 | Acc: 32.612% (13003/39872)\n",
            "Epoch 49 Step 1246/1563 Loss: 2.058 | Acc: 32.616% (13015/39904)\n",
            "Epoch 49 Step 1247/1563 Loss: 2.058 | Acc: 32.617% (13026/39936)\n",
            "Epoch 49 Step 1248/1563 Loss: 2.058 | Acc: 32.614% (13035/39968)\n",
            "Epoch 49 Step 1249/1563 Loss: 2.058 | Acc: 32.623% (13049/40000)\n",
            "Epoch 49 Step 1250/1563 Loss: 2.058 | Acc: 32.614% (13056/40032)\n",
            "Epoch 49 Step 1251/1563 Loss: 2.058 | Acc: 32.620% (13069/40064)\n",
            "Epoch 49 Step 1252/1563 Loss: 2.058 | Acc: 32.624% (13081/40096)\n",
            "Epoch 49 Step 1253/1563 Loss: 2.058 | Acc: 32.621% (13090/40128)\n",
            "Epoch 49 Step 1254/1563 Loss: 2.058 | Acc: 32.625% (13102/40160)\n",
            "Epoch 49 Step 1255/1563 Loss: 2.058 | Acc: 32.623% (13112/40192)\n",
            "Epoch 49 Step 1256/1563 Loss: 2.059 | Acc: 32.615% (13119/40224)\n",
            "Epoch 49 Step 1257/1563 Loss: 2.058 | Acc: 32.619% (13131/40256)\n",
            "Epoch 49 Step 1258/1563 Loss: 2.058 | Acc: 32.623% (13143/40288)\n",
            "Epoch 49 Step 1259/1563 Loss: 2.058 | Acc: 32.614% (13150/40320)\n",
            "Epoch 49 Step 1260/1563 Loss: 2.058 | Acc: 32.620% (13163/40352)\n",
            "Epoch 49 Step 1261/1563 Loss: 2.058 | Acc: 32.612% (13170/40384)\n",
            "Epoch 49 Step 1262/1563 Loss: 2.058 | Acc: 32.616% (13182/40416)\n",
            "Epoch 49 Step 1263/1563 Loss: 2.059 | Acc: 32.605% (13188/40448)\n",
            "Epoch 49 Step 1264/1563 Loss: 2.058 | Acc: 32.604% (13198/40480)\n",
            "Epoch 49 Step 1265/1563 Loss: 2.058 | Acc: 32.608% (13210/40512)\n",
            "Epoch 49 Step 1266/1563 Loss: 2.058 | Acc: 32.609% (13221/40544)\n",
            "Epoch 49 Step 1267/1563 Loss: 2.058 | Acc: 32.613% (13233/40576)\n",
            "Epoch 49 Step 1268/1563 Loss: 2.058 | Acc: 32.609% (13242/40608)\n",
            "Epoch 49 Step 1269/1563 Loss: 2.058 | Acc: 32.608% (13252/40640)\n",
            "Epoch 49 Step 1270/1563 Loss: 2.058 | Acc: 32.612% (13264/40672)\n",
            "Epoch 49 Step 1271/1563 Loss: 2.058 | Acc: 32.606% (13272/40704)\n",
            "Epoch 49 Step 1272/1563 Loss: 2.058 | Acc: 32.605% (13282/40736)\n",
            "Epoch 49 Step 1273/1563 Loss: 2.058 | Acc: 32.609% (13294/40768)\n",
            "Epoch 49 Step 1274/1563 Loss: 2.058 | Acc: 32.605% (13303/40800)\n",
            "Epoch 49 Step 1275/1563 Loss: 2.058 | Acc: 32.612% (13316/40832)\n",
            "Epoch 49 Step 1276/1563 Loss: 2.058 | Acc: 32.618% (13329/40864)\n",
            "Epoch 49 Step 1277/1563 Loss: 2.058 | Acc: 32.622% (13341/40896)\n",
            "Epoch 49 Step 1278/1563 Loss: 2.058 | Acc: 32.606% (13345/40928)\n",
            "Epoch 49 Step 1279/1563 Loss: 2.058 | Acc: 32.607% (13356/40960)\n",
            "Epoch 49 Step 1280/1563 Loss: 2.058 | Acc: 32.609% (13367/40992)\n",
            "Epoch 49 Step 1281/1563 Loss: 2.059 | Acc: 32.598% (13373/41024)\n",
            "Epoch 49 Step 1282/1563 Loss: 2.059 | Acc: 32.597% (13383/41056)\n",
            "Epoch 49 Step 1283/1563 Loss: 2.058 | Acc: 32.608% (13398/41088)\n",
            "Epoch 49 Step 1284/1563 Loss: 2.058 | Acc: 32.614% (13411/41120)\n",
            "Epoch 49 Step 1285/1563 Loss: 2.058 | Acc: 32.611% (13420/41152)\n",
            "Epoch 49 Step 1286/1563 Loss: 2.058 | Acc: 32.617% (13433/41184)\n",
            "Epoch 49 Step 1287/1563 Loss: 2.058 | Acc: 32.614% (13442/41216)\n",
            "Epoch 49 Step 1288/1563 Loss: 2.058 | Acc: 32.615% (13453/41248)\n",
            "Epoch 49 Step 1289/1563 Loss: 2.058 | Acc: 32.607% (13460/41280)\n",
            "Epoch 49 Step 1290/1563 Loss: 2.059 | Acc: 32.603% (13469/41312)\n",
            "Epoch 49 Step 1291/1563 Loss: 2.059 | Acc: 32.590% (13474/41344)\n",
            "Epoch 49 Step 1292/1563 Loss: 2.059 | Acc: 32.594% (13486/41376)\n",
            "Epoch 49 Step 1293/1563 Loss: 2.059 | Acc: 32.590% (13495/41408)\n",
            "Epoch 49 Step 1294/1563 Loss: 2.058 | Acc: 32.604% (13511/41440)\n",
            "Epoch 49 Step 1295/1563 Loss: 2.058 | Acc: 32.608% (13523/41472)\n",
            "Epoch 49 Step 1296/1563 Loss: 2.058 | Acc: 32.597% (13529/41504)\n",
            "Epoch 49 Step 1297/1563 Loss: 2.058 | Acc: 32.610% (13545/41536)\n",
            "Epoch 49 Step 1298/1563 Loss: 2.058 | Acc: 32.616% (13558/41568)\n",
            "Epoch 49 Step 1299/1563 Loss: 2.058 | Acc: 32.611% (13566/41600)\n",
            "Epoch 49 Step 1300/1563 Loss: 2.058 | Acc: 32.614% (13578/41632)\n",
            "Epoch 49 Step 1301/1563 Loss: 2.058 | Acc: 32.618% (13590/41664)\n",
            "Epoch 49 Step 1302/1563 Loss: 2.058 | Acc: 32.617% (13600/41696)\n",
            "Epoch 49 Step 1303/1563 Loss: 2.058 | Acc: 32.609% (13607/41728)\n",
            "Epoch 49 Step 1304/1563 Loss: 2.058 | Acc: 32.615% (13620/41760)\n",
            "Epoch 49 Step 1305/1563 Loss: 2.058 | Acc: 32.609% (13628/41792)\n",
            "Epoch 49 Step 1306/1563 Loss: 2.058 | Acc: 32.610% (13639/41824)\n",
            "Epoch 49 Step 1307/1563 Loss: 2.059 | Acc: 32.602% (13646/41856)\n",
            "Epoch 49 Step 1308/1563 Loss: 2.059 | Acc: 32.587% (13650/41888)\n",
            "Epoch 49 Step 1309/1563 Loss: 2.059 | Acc: 32.595% (13664/41920)\n",
            "Epoch 49 Step 1310/1563 Loss: 2.059 | Acc: 32.597% (13675/41952)\n",
            "Epoch 49 Step 1311/1563 Loss: 2.059 | Acc: 32.610% (13691/41984)\n",
            "Epoch 49 Step 1312/1563 Loss: 2.058 | Acc: 32.616% (13704/42016)\n",
            "Epoch 49 Step 1313/1563 Loss: 2.059 | Acc: 32.617% (13715/42048)\n",
            "Epoch 49 Step 1314/1563 Loss: 2.058 | Acc: 32.628% (13730/42080)\n",
            "Epoch 49 Step 1315/1563 Loss: 2.058 | Acc: 32.630% (13741/42112)\n",
            "Epoch 49 Step 1316/1563 Loss: 2.059 | Acc: 32.614% (13745/42144)\n",
            "Epoch 49 Step 1317/1563 Loss: 2.059 | Acc: 32.613% (13755/42176)\n",
            "Epoch 49 Step 1318/1563 Loss: 2.058 | Acc: 32.617% (13767/42208)\n",
            "Epoch 49 Step 1319/1563 Loss: 2.058 | Acc: 32.618% (13778/42240)\n",
            "Epoch 49 Step 1320/1563 Loss: 2.059 | Acc: 32.617% (13788/42272)\n",
            "Epoch 49 Step 1321/1563 Loss: 2.058 | Acc: 32.623% (13801/42304)\n",
            "Epoch 49 Step 1322/1563 Loss: 2.059 | Acc: 32.622% (13811/42336)\n",
            "Epoch 49 Step 1323/1563 Loss: 2.059 | Acc: 32.628% (13824/42368)\n",
            "Epoch 49 Step 1324/1563 Loss: 2.059 | Acc: 32.639% (13839/42400)\n",
            "Epoch 49 Step 1325/1563 Loss: 2.059 | Acc: 32.629% (13845/42432)\n",
            "Epoch 49 Step 1326/1563 Loss: 2.059 | Acc: 32.630% (13856/42464)\n",
            "Epoch 49 Step 1327/1563 Loss: 2.059 | Acc: 32.636% (13869/42496)\n",
            "Epoch 49 Step 1328/1563 Loss: 2.059 | Acc: 32.626% (13875/42528)\n",
            "Epoch 49 Step 1329/1563 Loss: 2.059 | Acc: 32.634% (13889/42560)\n",
            "Epoch 49 Step 1330/1563 Loss: 2.059 | Acc: 32.638% (13901/42592)\n",
            "Epoch 49 Step 1331/1563 Loss: 2.060 | Acc: 32.632% (13909/42624)\n",
            "Epoch 49 Step 1332/1563 Loss: 2.059 | Acc: 32.636% (13921/42656)\n",
            "Epoch 49 Step 1333/1563 Loss: 2.059 | Acc: 32.644% (13935/42688)\n",
            "Epoch 49 Step 1334/1563 Loss: 2.059 | Acc: 32.640% (13944/42720)\n",
            "Epoch 49 Step 1335/1563 Loss: 2.059 | Acc: 32.637% (13953/42752)\n",
            "Epoch 49 Step 1336/1563 Loss: 2.059 | Acc: 32.636% (13963/42784)\n",
            "Epoch 49 Step 1337/1563 Loss: 2.059 | Acc: 32.642% (13976/42816)\n",
            "Epoch 49 Step 1338/1563 Loss: 2.059 | Acc: 32.632% (13982/42848)\n",
            "Epoch 49 Step 1339/1563 Loss: 2.059 | Acc: 32.633% (13993/42880)\n",
            "Epoch 49 Step 1340/1563 Loss: 2.059 | Acc: 32.637% (14005/42912)\n",
            "Epoch 49 Step 1341/1563 Loss: 2.059 | Acc: 32.640% (14017/42944)\n",
            "Epoch 49 Step 1342/1563 Loss: 2.059 | Acc: 32.628% (14022/42976)\n",
            "Epoch 49 Step 1343/1563 Loss: 2.059 | Acc: 32.629% (14033/43008)\n",
            "Epoch 49 Step 1344/1563 Loss: 2.059 | Acc: 32.637% (14047/43040)\n",
            "Epoch 49 Step 1345/1563 Loss: 2.059 | Acc: 32.641% (14059/43072)\n",
            "Epoch 49 Step 1346/1563 Loss: 2.059 | Acc: 32.637% (14068/43104)\n",
            "Epoch 49 Step 1347/1563 Loss: 2.059 | Acc: 32.634% (14077/43136)\n",
            "Epoch 49 Step 1348/1563 Loss: 2.060 | Acc: 32.626% (14084/43168)\n",
            "Epoch 49 Step 1349/1563 Loss: 2.059 | Acc: 32.632% (14097/43200)\n",
            "Epoch 49 Step 1350/1563 Loss: 2.059 | Acc: 32.629% (14106/43232)\n",
            "Epoch 49 Step 1351/1563 Loss: 2.059 | Acc: 32.632% (14118/43264)\n",
            "Epoch 49 Step 1352/1563 Loss: 2.059 | Acc: 32.622% (14124/43296)\n",
            "Epoch 49 Step 1353/1563 Loss: 2.060 | Acc: 32.607% (14128/43328)\n",
            "Epoch 49 Step 1354/1563 Loss: 2.060 | Acc: 32.613% (14141/43360)\n",
            "Epoch 49 Step 1355/1563 Loss: 2.060 | Acc: 32.614% (14152/43392)\n",
            "Epoch 49 Step 1356/1563 Loss: 2.060 | Acc: 32.609% (14160/43424)\n",
            "Epoch 49 Step 1357/1563 Loss: 2.060 | Acc: 32.603% (14168/43456)\n",
            "Epoch 49 Step 1358/1563 Loss: 2.060 | Acc: 32.607% (14180/43488)\n",
            "Epoch 49 Step 1359/1563 Loss: 2.060 | Acc: 32.617% (14195/43520)\n",
            "Epoch 49 Step 1360/1563 Loss: 2.060 | Acc: 32.614% (14204/43552)\n",
            "Epoch 49 Step 1361/1563 Loss: 2.060 | Acc: 32.608% (14212/43584)\n",
            "Epoch 49 Step 1362/1563 Loss: 2.060 | Acc: 32.607% (14222/43616)\n",
            "Epoch 49 Step 1363/1563 Loss: 2.060 | Acc: 32.611% (14234/43648)\n",
            "Epoch 49 Step 1364/1563 Loss: 2.060 | Acc: 32.601% (14240/43680)\n",
            "Epoch 49 Step 1365/1563 Loss: 2.060 | Acc: 32.604% (14252/43712)\n",
            "Epoch 49 Step 1366/1563 Loss: 2.060 | Acc: 32.612% (14266/43744)\n",
            "Epoch 49 Step 1367/1563 Loss: 2.060 | Acc: 32.609% (14275/43776)\n",
            "Epoch 49 Step 1368/1563 Loss: 2.060 | Acc: 32.597% (14280/43808)\n",
            "Epoch 49 Step 1369/1563 Loss: 2.061 | Acc: 32.591% (14288/43840)\n",
            "Epoch 49 Step 1370/1563 Loss: 2.061 | Acc: 32.590% (14298/43872)\n",
            "Epoch 49 Step 1371/1563 Loss: 2.060 | Acc: 32.598% (14312/43904)\n",
            "Epoch 49 Step 1372/1563 Loss: 2.060 | Acc: 32.609% (14327/43936)\n",
            "Epoch 49 Step 1373/1563 Loss: 2.060 | Acc: 32.606% (14336/43968)\n",
            "Epoch 49 Step 1374/1563 Loss: 2.060 | Acc: 32.611% (14349/44000)\n",
            "Epoch 49 Step 1375/1563 Loss: 2.060 | Acc: 32.608% (14358/44032)\n",
            "Epoch 49 Step 1376/1563 Loss: 2.061 | Acc: 32.605% (14367/44064)\n",
            "Epoch 49 Step 1377/1563 Loss: 2.060 | Acc: 32.595% (14373/44096)\n",
            "Epoch 49 Step 1378/1563 Loss: 2.060 | Acc: 32.598% (14385/44128)\n",
            "Epoch 49 Step 1379/1563 Loss: 2.060 | Acc: 32.597% (14395/44160)\n",
            "Epoch 49 Step 1380/1563 Loss: 2.060 | Acc: 32.603% (14408/44192)\n",
            "Epoch 49 Step 1381/1563 Loss: 2.060 | Acc: 32.600% (14417/44224)\n",
            "Epoch 49 Step 1382/1563 Loss: 2.060 | Acc: 32.594% (14425/44256)\n",
            "Epoch 49 Step 1383/1563 Loss: 2.061 | Acc: 32.593% (14435/44288)\n",
            "Epoch 49 Step 1384/1563 Loss: 2.061 | Acc: 32.590% (14444/44320)\n",
            "Epoch 49 Step 1385/1563 Loss: 2.061 | Acc: 32.589% (14454/44352)\n",
            "Epoch 49 Step 1386/1563 Loss: 2.061 | Acc: 32.586% (14463/44384)\n",
            "Epoch 49 Step 1387/1563 Loss: 2.061 | Acc: 32.594% (14477/44416)\n",
            "Epoch 49 Step 1388/1563 Loss: 2.061 | Acc: 32.602% (14491/44448)\n",
            "Epoch 49 Step 1389/1563 Loss: 2.061 | Acc: 32.606% (14503/44480)\n",
            "Epoch 49 Step 1390/1563 Loss: 2.061 | Acc: 32.605% (14513/44512)\n",
            "Epoch 49 Step 1391/1563 Loss: 2.061 | Acc: 32.606% (14524/44544)\n",
            "Epoch 49 Step 1392/1563 Loss: 2.061 | Acc: 32.607% (14535/44576)\n",
            "Epoch 49 Step 1393/1563 Loss: 2.061 | Acc: 32.597% (14541/44608)\n",
            "Epoch 49 Step 1394/1563 Loss: 2.061 | Acc: 32.599% (14552/44640)\n",
            "Epoch 49 Step 1395/1563 Loss: 2.062 | Acc: 32.589% (14558/44672)\n",
            "Epoch 49 Step 1396/1563 Loss: 2.062 | Acc: 32.585% (14567/44704)\n",
            "Epoch 49 Step 1397/1563 Loss: 2.062 | Acc: 32.596% (14582/44736)\n",
            "Epoch 49 Step 1398/1563 Loss: 2.062 | Acc: 32.588% (14589/44768)\n",
            "Epoch 49 Step 1399/1563 Loss: 2.062 | Acc: 32.596% (14603/44800)\n",
            "Epoch 49 Step 1400/1563 Loss: 2.062 | Acc: 32.595% (14613/44832)\n",
            "Epoch 49 Step 1401/1563 Loss: 2.062 | Acc: 32.605% (14628/44864)\n",
            "Epoch 49 Step 1402/1563 Loss: 2.062 | Acc: 32.606% (14639/44896)\n",
            "Epoch 49 Step 1403/1563 Loss: 2.062 | Acc: 32.612% (14652/44928)\n",
            "Epoch 49 Step 1404/1563 Loss: 2.062 | Acc: 32.613% (14663/44960)\n",
            "Epoch 49 Step 1405/1563 Loss: 2.062 | Acc: 32.619% (14676/44992)\n",
            "Epoch 49 Step 1406/1563 Loss: 2.062 | Acc: 32.618% (14686/45024)\n",
            "Epoch 49 Step 1407/1563 Loss: 2.062 | Acc: 32.622% (14698/45056)\n",
            "Epoch 49 Step 1408/1563 Loss: 2.062 | Acc: 32.623% (14709/45088)\n",
            "Epoch 49 Step 1409/1563 Loss: 2.062 | Acc: 32.629% (14722/45120)\n",
            "Epoch 49 Step 1410/1563 Loss: 2.062 | Acc: 32.634% (14735/45152)\n",
            "Epoch 49 Step 1411/1563 Loss: 2.061 | Acc: 32.644% (14750/45184)\n",
            "Epoch 49 Step 1412/1563 Loss: 2.061 | Acc: 32.641% (14759/45216)\n",
            "Epoch 49 Step 1413/1563 Loss: 2.061 | Acc: 32.638% (14768/45248)\n",
            "Epoch 49 Step 1414/1563 Loss: 2.062 | Acc: 32.630% (14775/45280)\n",
            "Epoch 49 Step 1415/1563 Loss: 2.062 | Acc: 32.634% (14787/45312)\n",
            "Epoch 49 Step 1416/1563 Loss: 2.062 | Acc: 32.628% (14795/45344)\n",
            "Epoch 49 Step 1417/1563 Loss: 2.061 | Acc: 32.630% (14806/45376)\n",
            "Epoch 49 Step 1418/1563 Loss: 2.062 | Acc: 32.629% (14816/45408)\n",
            "Epoch 49 Step 1419/1563 Loss: 2.062 | Acc: 32.625% (14825/45440)\n",
            "Epoch 49 Step 1420/1563 Loss: 2.062 | Acc: 32.627% (14836/45472)\n",
            "Epoch 49 Step 1421/1563 Loss: 2.062 | Acc: 32.624% (14845/45504)\n",
            "Epoch 49 Step 1422/1563 Loss: 2.062 | Acc: 32.623% (14855/45536)\n",
            "Epoch 49 Step 1423/1563 Loss: 2.062 | Acc: 32.624% (14866/45568)\n",
            "Epoch 49 Step 1424/1563 Loss: 2.062 | Acc: 32.625% (14877/45600)\n",
            "Epoch 49 Step 1425/1563 Loss: 2.063 | Acc: 32.620% (14885/45632)\n",
            "Epoch 49 Step 1426/1563 Loss: 2.063 | Acc: 32.614% (14893/45664)\n",
            "Epoch 49 Step 1427/1563 Loss: 2.062 | Acc: 32.633% (14912/45696)\n",
            "Epoch 49 Step 1428/1563 Loss: 2.062 | Acc: 32.639% (14925/45728)\n",
            "Epoch 49 Step 1429/1563 Loss: 2.062 | Acc: 32.649% (14940/45760)\n",
            "Epoch 49 Step 1430/1563 Loss: 2.062 | Acc: 32.652% (14952/45792)\n",
            "Epoch 49 Step 1431/1563 Loss: 2.062 | Acc: 32.640% (14957/45824)\n",
            "Epoch 49 Step 1432/1563 Loss: 2.062 | Acc: 32.635% (14965/45856)\n",
            "Epoch 49 Step 1433/1563 Loss: 2.062 | Acc: 32.636% (14976/45888)\n",
            "Epoch 49 Step 1434/1563 Loss: 2.062 | Acc: 32.631% (14984/45920)\n",
            "Epoch 49 Step 1435/1563 Loss: 2.062 | Acc: 32.628% (14993/45952)\n",
            "Epoch 49 Step 1436/1563 Loss: 2.062 | Acc: 32.624% (15002/45984)\n",
            "Epoch 49 Step 1437/1563 Loss: 2.062 | Acc: 32.630% (15015/46016)\n",
            "Epoch 49 Step 1438/1563 Loss: 2.063 | Acc: 32.625% (15023/46048)\n",
            "Epoch 49 Step 1439/1563 Loss: 2.063 | Acc: 32.624% (15033/46080)\n",
            "Epoch 49 Step 1440/1563 Loss: 2.063 | Acc: 32.621% (15042/46112)\n",
            "Epoch 49 Step 1441/1563 Loss: 2.063 | Acc: 32.613% (15049/46144)\n",
            "Epoch 49 Step 1442/1563 Loss: 2.063 | Acc: 32.612% (15059/46176)\n",
            "Epoch 49 Step 1443/1563 Loss: 2.063 | Acc: 32.605% (15066/46208)\n",
            "Epoch 49 Step 1444/1563 Loss: 2.063 | Acc: 32.615% (15081/46240)\n",
            "Epoch 49 Step 1445/1563 Loss: 2.063 | Acc: 32.603% (15086/46272)\n",
            "Epoch 49 Step 1446/1563 Loss: 2.063 | Acc: 32.598% (15094/46304)\n",
            "Epoch 49 Step 1447/1563 Loss: 2.064 | Acc: 32.592% (15102/46336)\n",
            "Epoch 49 Step 1448/1563 Loss: 2.063 | Acc: 32.598% (15115/46368)\n",
            "Epoch 49 Step 1449/1563 Loss: 2.063 | Acc: 32.588% (15121/46400)\n",
            "Epoch 49 Step 1450/1563 Loss: 2.063 | Acc: 32.583% (15129/46432)\n",
            "Epoch 49 Step 1451/1563 Loss: 2.064 | Acc: 32.578% (15137/46464)\n",
            "Epoch 49 Step 1452/1563 Loss: 2.064 | Acc: 32.577% (15147/46496)\n",
            "Epoch 49 Step 1453/1563 Loss: 2.064 | Acc: 32.580% (15159/46528)\n",
            "Epoch 49 Step 1454/1563 Loss: 2.064 | Acc: 32.582% (15170/46560)\n",
            "Epoch 49 Step 1455/1563 Loss: 2.064 | Acc: 32.583% (15181/46592)\n",
            "Epoch 49 Step 1456/1563 Loss: 2.064 | Acc: 32.584% (15192/46624)\n",
            "Epoch 49 Step 1457/1563 Loss: 2.064 | Acc: 32.592% (15206/46656)\n",
            "Epoch 49 Step 1458/1563 Loss: 2.064 | Acc: 32.593% (15217/46688)\n",
            "Epoch 49 Step 1459/1563 Loss: 2.064 | Acc: 32.594% (15228/46720)\n",
            "Epoch 49 Step 1460/1563 Loss: 2.064 | Acc: 32.593% (15238/46752)\n",
            "Epoch 49 Step 1461/1563 Loss: 2.064 | Acc: 32.592% (15248/46784)\n",
            "Epoch 49 Step 1462/1563 Loss: 2.064 | Acc: 32.594% (15259/46816)\n",
            "Epoch 49 Step 1463/1563 Loss: 2.064 | Acc: 32.591% (15268/46848)\n",
            "Epoch 49 Step 1464/1563 Loss: 2.064 | Acc: 32.587% (15277/46880)\n",
            "Epoch 49 Step 1465/1563 Loss: 2.064 | Acc: 32.582% (15285/46912)\n",
            "Epoch 49 Step 1466/1563 Loss: 2.064 | Acc: 32.588% (15298/46944)\n",
            "Epoch 49 Step 1467/1563 Loss: 2.064 | Acc: 32.587% (15308/46976)\n",
            "Epoch 49 Step 1468/1563 Loss: 2.064 | Acc: 32.584% (15317/47008)\n",
            "Epoch 49 Step 1469/1563 Loss: 2.064 | Acc: 32.583% (15327/47040)\n",
            "Epoch 49 Step 1470/1563 Loss: 2.064 | Acc: 32.576% (15334/47072)\n",
            "Epoch 49 Step 1471/1563 Loss: 2.064 | Acc: 32.573% (15343/47104)\n",
            "Epoch 49 Step 1472/1563 Loss: 2.064 | Acc: 32.574% (15354/47136)\n",
            "Epoch 49 Step 1473/1563 Loss: 2.065 | Acc: 32.577% (15366/47168)\n",
            "Epoch 49 Step 1474/1563 Loss: 2.065 | Acc: 32.572% (15374/47200)\n",
            "Epoch 49 Step 1475/1563 Loss: 2.065 | Acc: 32.571% (15384/47232)\n",
            "Epoch 49 Step 1476/1563 Loss: 2.065 | Acc: 32.570% (15394/47264)\n",
            "Epoch 49 Step 1477/1563 Loss: 2.065 | Acc: 32.578% (15408/47296)\n",
            "Epoch 49 Step 1478/1563 Loss: 2.065 | Acc: 32.583% (15421/47328)\n",
            "Epoch 49 Step 1479/1563 Loss: 2.065 | Acc: 32.578% (15429/47360)\n",
            "Epoch 49 Step 1480/1563 Loss: 2.064 | Acc: 32.581% (15441/47392)\n",
            "Epoch 49 Step 1481/1563 Loss: 2.064 | Acc: 32.587% (15454/47424)\n",
            "Epoch 49 Step 1482/1563 Loss: 2.064 | Acc: 32.582% (15462/47456)\n",
            "Epoch 49 Step 1483/1563 Loss: 2.064 | Acc: 32.591% (15477/47488)\n",
            "Epoch 49 Step 1484/1563 Loss: 2.064 | Acc: 32.593% (15488/47520)\n",
            "Epoch 49 Step 1485/1563 Loss: 2.064 | Acc: 32.592% (15498/47552)\n",
            "Epoch 49 Step 1486/1563 Loss: 2.064 | Acc: 32.591% (15508/47584)\n",
            "Epoch 49 Step 1487/1563 Loss: 2.064 | Acc: 32.594% (15520/47616)\n",
            "Epoch 49 Step 1488/1563 Loss: 2.064 | Acc: 32.602% (15534/47648)\n",
            "Epoch 49 Step 1489/1563 Loss: 2.064 | Acc: 32.605% (15546/47680)\n",
            "Epoch 49 Step 1490/1563 Loss: 2.064 | Acc: 32.614% (15561/47712)\n",
            "Epoch 49 Step 1491/1563 Loss: 2.063 | Acc: 32.614% (15571/47744)\n",
            "Epoch 49 Step 1492/1563 Loss: 2.063 | Acc: 32.606% (15578/47776)\n",
            "Epoch 49 Step 1493/1563 Loss: 2.064 | Acc: 32.612% (15591/47808)\n",
            "Epoch 49 Step 1494/1563 Loss: 2.063 | Acc: 32.615% (15603/47840)\n",
            "Epoch 49 Step 1495/1563 Loss: 2.063 | Acc: 32.606% (15609/47872)\n",
            "Epoch 49 Step 1496/1563 Loss: 2.063 | Acc: 32.601% (15617/47904)\n",
            "Epoch 49 Step 1497/1563 Loss: 2.063 | Acc: 32.606% (15630/47936)\n",
            "Epoch 49 Step 1498/1563 Loss: 2.063 | Acc: 32.609% (15642/47968)\n",
            "Epoch 49 Step 1499/1563 Loss: 2.063 | Acc: 32.602% (15649/48000)\n",
            "Epoch 49 Step 1500/1563 Loss: 2.063 | Acc: 32.599% (15658/48032)\n",
            "Epoch 49 Step 1501/1563 Loss: 2.064 | Acc: 32.596% (15667/48064)\n",
            "Epoch 49 Step 1502/1563 Loss: 2.064 | Acc: 32.608% (15683/48096)\n",
            "Epoch 49 Step 1503/1563 Loss: 2.064 | Acc: 32.611% (15695/48128)\n",
            "Epoch 49 Step 1504/1563 Loss: 2.064 | Acc: 32.616% (15708/48160)\n",
            "Epoch 49 Step 1505/1563 Loss: 2.064 | Acc: 32.617% (15719/48192)\n",
            "Epoch 49 Step 1506/1563 Loss: 2.064 | Acc: 32.617% (15729/48224)\n",
            "Epoch 49 Step 1507/1563 Loss: 2.064 | Acc: 32.616% (15739/48256)\n",
            "Epoch 49 Step 1508/1563 Loss: 2.064 | Acc: 32.609% (15746/48288)\n",
            "Epoch 49 Step 1509/1563 Loss: 2.064 | Acc: 32.614% (15759/48320)\n",
            "Epoch 49 Step 1510/1563 Loss: 2.064 | Acc: 32.621% (15773/48352)\n",
            "Epoch 49 Step 1511/1563 Loss: 2.064 | Acc: 32.614% (15780/48384)\n",
            "Epoch 49 Step 1512/1563 Loss: 2.064 | Acc: 32.611% (15789/48416)\n",
            "Epoch 49 Step 1513/1563 Loss: 2.064 | Acc: 32.616% (15802/48448)\n",
            "Epoch 49 Step 1514/1563 Loss: 2.064 | Acc: 32.616% (15812/48480)\n",
            "Epoch 49 Step 1515/1563 Loss: 2.064 | Acc: 32.604% (15817/48512)\n",
            "Epoch 49 Step 1516/1563 Loss: 2.064 | Acc: 32.597% (15824/48544)\n",
            "Epoch 49 Step 1517/1563 Loss: 2.064 | Acc: 32.596% (15834/48576)\n",
            "Epoch 49 Step 1518/1563 Loss: 2.064 | Acc: 32.593% (15843/48608)\n",
            "Epoch 49 Step 1519/1563 Loss: 2.064 | Acc: 32.599% (15856/48640)\n",
            "Epoch 49 Step 1520/1563 Loss: 2.064 | Acc: 32.598% (15866/48672)\n",
            "Epoch 49 Step 1521/1563 Loss: 2.064 | Acc: 32.597% (15876/48704)\n",
            "Epoch 49 Step 1522/1563 Loss: 2.064 | Acc: 32.600% (15888/48736)\n",
            "Epoch 49 Step 1523/1563 Loss: 2.064 | Acc: 32.607% (15902/48768)\n",
            "Epoch 49 Step 1524/1563 Loss: 2.064 | Acc: 32.611% (15914/48800)\n",
            "Epoch 49 Step 1525/1563 Loss: 2.064 | Acc: 32.614% (15926/48832)\n",
            "Epoch 49 Step 1526/1563 Loss: 2.064 | Acc: 32.615% (15937/48864)\n",
            "Epoch 49 Step 1527/1563 Loss: 2.064 | Acc: 32.606% (15943/48896)\n",
            "Epoch 49 Step 1528/1563 Loss: 2.064 | Acc: 32.607% (15954/48928)\n",
            "Epoch 49 Step 1529/1563 Loss: 2.064 | Acc: 32.616% (15969/48960)\n",
            "Epoch 49 Step 1530/1563 Loss: 2.064 | Acc: 32.616% (15979/48992)\n",
            "Epoch 49 Step 1531/1563 Loss: 2.064 | Acc: 32.615% (15989/49024)\n",
            "Epoch 49 Step 1532/1563 Loss: 2.064 | Acc: 32.616% (16000/49056)\n",
            "Epoch 49 Step 1533/1563 Loss: 2.065 | Acc: 32.615% (16010/49088)\n",
            "Epoch 49 Step 1534/1563 Loss: 2.064 | Acc: 32.620% (16023/49120)\n",
            "Epoch 49 Step 1535/1563 Loss: 2.064 | Acc: 32.625% (16036/49152)\n",
            "Epoch 49 Step 1536/1563 Loss: 2.064 | Acc: 32.628% (16048/49184)\n",
            "Epoch 49 Step 1537/1563 Loss: 2.064 | Acc: 32.622% (16055/49216)\n",
            "Epoch 49 Step 1538/1563 Loss: 2.064 | Acc: 32.623% (16066/49248)\n",
            "Epoch 49 Step 1539/1563 Loss: 2.064 | Acc: 32.632% (16081/49280)\n",
            "Epoch 49 Step 1540/1563 Loss: 2.064 | Acc: 32.631% (16091/49312)\n",
            "Epoch 49 Step 1541/1563 Loss: 2.064 | Acc: 32.622% (16097/49344)\n",
            "Epoch 49 Step 1542/1563 Loss: 2.065 | Acc: 32.619% (16106/49376)\n",
            "Epoch 49 Step 1543/1563 Loss: 2.065 | Acc: 32.618% (16116/49408)\n",
            "Epoch 49 Step 1544/1563 Loss: 2.065 | Acc: 32.611% (16123/49440)\n",
            "Epoch 49 Step 1545/1563 Loss: 2.065 | Acc: 32.618% (16137/49472)\n",
            "Epoch 49 Step 1546/1563 Loss: 2.065 | Acc: 32.622% (16149/49504)\n",
            "Epoch 49 Step 1547/1563 Loss: 2.064 | Acc: 32.625% (16161/49536)\n",
            "Epoch 49 Step 1548/1563 Loss: 2.064 | Acc: 32.630% (16174/49568)\n",
            "Epoch 49 Step 1549/1563 Loss: 2.064 | Acc: 32.631% (16185/49600)\n",
            "Epoch 49 Step 1550/1563 Loss: 2.064 | Acc: 32.636% (16198/49632)\n",
            "Epoch 49 Step 1551/1563 Loss: 2.064 | Acc: 32.635% (16208/49664)\n",
            "Epoch 49 Step 1552/1563 Loss: 2.064 | Acc: 32.638% (16220/49696)\n",
            "Epoch 49 Step 1553/1563 Loss: 2.064 | Acc: 32.636% (16229/49728)\n",
            "Epoch 49 Step 1554/1563 Loss: 2.064 | Acc: 32.639% (16241/49760)\n",
            "Epoch 49 Step 1555/1563 Loss: 2.064 | Acc: 32.636% (16250/49792)\n",
            "Epoch 49 Step 1556/1563 Loss: 2.064 | Acc: 32.629% (16257/49824)\n",
            "Epoch 49 Step 1557/1563 Loss: 2.064 | Acc: 32.646% (16276/49856)\n",
            "Epoch 49 Step 1558/1563 Loss: 2.064 | Acc: 32.657% (16292/49888)\n",
            "Epoch 49 Step 1559/1563 Loss: 2.064 | Acc: 32.656% (16302/49920)\n",
            "Epoch 49 Step 1560/1563 Loss: 2.064 | Acc: 32.651% (16310/49952)\n",
            "Epoch 49 Step 1561/1563 Loss: 2.064 | Acc: 32.652% (16321/49984)\n",
            "Epoch 49 Step 1562/1563 Loss: 2.064 | Acc: 32.656% (16328/50000)\n",
            "Epoch 49 Step 0/313 Test Loss: 1.434 | Test Acc: 59.375% (19/32)\n",
            "Epoch 49 Step 1/313 Test Loss: 1.732 | Test Acc: 40.625% (26/64)\n",
            "Epoch 49 Step 2/313 Test Loss: 1.746 | Test Acc: 40.625% (39/96)\n",
            "Epoch 49 Step 3/313 Test Loss: 1.796 | Test Acc: 39.062% (50/128)\n",
            "Epoch 49 Step 4/313 Test Loss: 1.782 | Test Acc: 38.750% (62/160)\n",
            "Epoch 49 Step 5/313 Test Loss: 1.789 | Test Acc: 38.542% (74/192)\n",
            "Epoch 49 Step 6/313 Test Loss: 1.860 | Test Acc: 36.607% (82/224)\n",
            "Epoch 49 Step 7/313 Test Loss: 1.913 | Test Acc: 34.766% (89/256)\n",
            "Epoch 49 Step 8/313 Test Loss: 1.946 | Test Acc: 34.722% (100/288)\n",
            "Epoch 49 Step 9/313 Test Loss: 1.910 | Test Acc: 36.250% (116/320)\n",
            "Epoch 49 Step 10/313 Test Loss: 1.925 | Test Acc: 34.943% (123/352)\n",
            "Epoch 49 Step 11/313 Test Loss: 1.990 | Test Acc: 33.594% (129/384)\n",
            "Epoch 49 Step 12/313 Test Loss: 1.961 | Test Acc: 34.615% (144/416)\n",
            "Epoch 49 Step 13/313 Test Loss: 1.990 | Test Acc: 34.152% (153/448)\n",
            "Epoch 49 Step 14/313 Test Loss: 2.004 | Test Acc: 33.958% (163/480)\n",
            "Epoch 49 Step 15/313 Test Loss: 2.013 | Test Acc: 34.570% (177/512)\n",
            "Epoch 49 Step 16/313 Test Loss: 1.986 | Test Acc: 35.662% (194/544)\n",
            "Epoch 49 Step 17/313 Test Loss: 1.969 | Test Acc: 35.938% (207/576)\n",
            "Epoch 49 Step 18/313 Test Loss: 1.954 | Test Acc: 36.678% (223/608)\n",
            "Epoch 49 Step 19/313 Test Loss: 1.935 | Test Acc: 36.562% (234/640)\n",
            "Epoch 49 Step 20/313 Test Loss: 1.933 | Test Acc: 36.756% (247/672)\n",
            "Epoch 49 Step 21/313 Test Loss: 1.945 | Test Acc: 36.506% (257/704)\n",
            "Epoch 49 Step 22/313 Test Loss: 1.952 | Test Acc: 36.277% (267/736)\n",
            "Epoch 49 Step 23/313 Test Loss: 1.976 | Test Acc: 35.807% (275/768)\n",
            "Epoch 49 Step 24/313 Test Loss: 1.973 | Test Acc: 35.625% (285/800)\n",
            "Epoch 49 Step 25/313 Test Loss: 1.973 | Test Acc: 35.697% (297/832)\n",
            "Epoch 49 Step 26/313 Test Loss: 1.973 | Test Acc: 35.880% (310/864)\n",
            "Epoch 49 Step 27/313 Test Loss: 1.974 | Test Acc: 36.272% (325/896)\n",
            "Epoch 49 Step 28/313 Test Loss: 1.980 | Test Acc: 36.207% (336/928)\n",
            "Epoch 49 Step 29/313 Test Loss: 1.969 | Test Acc: 36.354% (349/960)\n",
            "Epoch 49 Step 30/313 Test Loss: 1.967 | Test Acc: 36.290% (360/992)\n",
            "Epoch 49 Step 31/313 Test Loss: 1.972 | Test Acc: 36.133% (370/1024)\n",
            "Epoch 49 Step 32/313 Test Loss: 1.980 | Test Acc: 35.701% (377/1056)\n",
            "Epoch 49 Step 33/313 Test Loss: 1.972 | Test Acc: 35.938% (391/1088)\n",
            "Epoch 49 Step 34/313 Test Loss: 1.962 | Test Acc: 35.893% (402/1120)\n",
            "Epoch 49 Step 35/313 Test Loss: 1.977 | Test Acc: 35.590% (410/1152)\n",
            "Epoch 49 Step 36/313 Test Loss: 1.975 | Test Acc: 35.726% (423/1184)\n",
            "Epoch 49 Step 37/313 Test Loss: 1.991 | Test Acc: 35.609% (433/1216)\n",
            "Epoch 49 Step 38/313 Test Loss: 1.998 | Test Acc: 35.337% (441/1248)\n",
            "Epoch 49 Step 39/313 Test Loss: 1.997 | Test Acc: 35.547% (455/1280)\n",
            "Epoch 49 Step 40/313 Test Loss: 1.996 | Test Acc: 35.671% (468/1312)\n",
            "Epoch 49 Step 41/313 Test Loss: 2.004 | Test Acc: 35.193% (473/1344)\n",
            "Epoch 49 Step 42/313 Test Loss: 1.996 | Test Acc: 35.174% (484/1376)\n",
            "Epoch 49 Step 43/313 Test Loss: 2.004 | Test Acc: 35.014% (493/1408)\n",
            "Epoch 49 Step 44/313 Test Loss: 2.004 | Test Acc: 35.000% (504/1440)\n",
            "Epoch 49 Step 45/313 Test Loss: 1.997 | Test Acc: 35.054% (516/1472)\n",
            "Epoch 49 Step 46/313 Test Loss: 1.998 | Test Acc: 34.973% (526/1504)\n",
            "Epoch 49 Step 47/313 Test Loss: 1.997 | Test Acc: 34.961% (537/1536)\n",
            "Epoch 49 Step 48/313 Test Loss: 1.994 | Test Acc: 34.949% (548/1568)\n",
            "Epoch 49 Step 49/313 Test Loss: 2.003 | Test Acc: 34.750% (556/1600)\n",
            "Epoch 49 Step 50/313 Test Loss: 2.006 | Test Acc: 34.743% (567/1632)\n",
            "Epoch 49 Step 51/313 Test Loss: 2.002 | Test Acc: 34.856% (580/1664)\n",
            "Epoch 49 Step 52/313 Test Loss: 2.002 | Test Acc: 34.965% (593/1696)\n",
            "Epoch 49 Step 53/313 Test Loss: 2.010 | Test Acc: 34.838% (602/1728)\n",
            "Epoch 49 Step 54/313 Test Loss: 2.014 | Test Acc: 34.773% (612/1760)\n",
            "Epoch 49 Step 55/313 Test Loss: 2.010 | Test Acc: 34.766% (623/1792)\n",
            "Epoch 49 Step 56/313 Test Loss: 2.010 | Test Acc: 34.485% (629/1824)\n",
            "Epoch 49 Step 57/313 Test Loss: 2.007 | Test Acc: 34.591% (642/1856)\n",
            "Epoch 49 Step 58/313 Test Loss: 2.005 | Test Acc: 34.693% (655/1888)\n",
            "Epoch 49 Step 59/313 Test Loss: 2.004 | Test Acc: 34.740% (667/1920)\n",
            "Epoch 49 Step 60/313 Test Loss: 2.006 | Test Acc: 34.682% (677/1952)\n",
            "Epoch 49 Step 61/313 Test Loss: 2.002 | Test Acc: 34.778% (690/1984)\n",
            "Epoch 49 Step 62/313 Test Loss: 2.011 | Test Acc: 34.772% (701/2016)\n",
            "Epoch 49 Step 63/313 Test Loss: 2.006 | Test Acc: 34.912% (715/2048)\n",
            "Epoch 49 Step 64/313 Test Loss: 2.002 | Test Acc: 34.856% (725/2080)\n",
            "Epoch 49 Step 65/313 Test Loss: 2.004 | Test Acc: 34.801% (735/2112)\n",
            "Epoch 49 Step 66/313 Test Loss: 2.002 | Test Acc: 34.795% (746/2144)\n",
            "Epoch 49 Step 67/313 Test Loss: 2.003 | Test Acc: 34.926% (760/2176)\n",
            "Epoch 49 Step 68/313 Test Loss: 2.004 | Test Acc: 34.918% (771/2208)\n",
            "Epoch 49 Step 69/313 Test Loss: 2.006 | Test Acc: 34.732% (778/2240)\n",
            "Epoch 49 Step 70/313 Test Loss: 2.007 | Test Acc: 34.771% (790/2272)\n",
            "Epoch 49 Step 71/313 Test Loss: 2.010 | Test Acc: 34.635% (798/2304)\n",
            "Epoch 49 Step 72/313 Test Loss: 2.014 | Test Acc: 34.461% (805/2336)\n",
            "Epoch 49 Step 73/313 Test Loss: 2.010 | Test Acc: 34.586% (819/2368)\n",
            "Epoch 49 Step 74/313 Test Loss: 2.010 | Test Acc: 34.667% (832/2400)\n",
            "Epoch 49 Step 75/313 Test Loss: 2.012 | Test Acc: 34.622% (842/2432)\n",
            "Epoch 49 Step 76/313 Test Loss: 2.015 | Test Acc: 34.700% (855/2464)\n",
            "Epoch 49 Step 77/313 Test Loss: 2.008 | Test Acc: 35.016% (874/2496)\n",
            "Epoch 49 Step 78/313 Test Loss: 2.007 | Test Acc: 35.047% (886/2528)\n",
            "Epoch 49 Step 79/313 Test Loss: 2.005 | Test Acc: 35.039% (897/2560)\n",
            "Epoch 49 Step 80/313 Test Loss: 2.006 | Test Acc: 34.954% (906/2592)\n",
            "Epoch 49 Step 81/313 Test Loss: 2.005 | Test Acc: 35.023% (919/2624)\n",
            "Epoch 49 Step 82/313 Test Loss: 2.012 | Test Acc: 34.940% (928/2656)\n",
            "Epoch 49 Step 83/313 Test Loss: 2.008 | Test Acc: 35.045% (942/2688)\n",
            "Epoch 49 Step 84/313 Test Loss: 2.005 | Test Acc: 35.074% (954/2720)\n",
            "Epoch 49 Step 85/313 Test Loss: 2.007 | Test Acc: 34.811% (958/2752)\n",
            "Epoch 49 Step 86/313 Test Loss: 2.011 | Test Acc: 34.806% (969/2784)\n",
            "Epoch 49 Step 87/313 Test Loss: 2.011 | Test Acc: 34.766% (979/2816)\n",
            "Epoch 49 Step 88/313 Test Loss: 2.012 | Test Acc: 34.761% (990/2848)\n",
            "Epoch 49 Step 89/313 Test Loss: 2.011 | Test Acc: 34.618% (997/2880)\n",
            "Epoch 49 Step 90/313 Test Loss: 2.014 | Test Acc: 34.581% (1007/2912)\n",
            "Epoch 49 Step 91/313 Test Loss: 2.012 | Test Acc: 34.647% (1020/2944)\n",
            "Epoch 49 Step 92/313 Test Loss: 2.012 | Test Acc: 34.677% (1032/2976)\n",
            "Epoch 49 Step 93/313 Test Loss: 2.011 | Test Acc: 34.641% (1042/3008)\n",
            "Epoch 49 Step 94/313 Test Loss: 2.008 | Test Acc: 34.836% (1059/3040)\n",
            "Epoch 49 Step 95/313 Test Loss: 2.006 | Test Acc: 34.896% (1072/3072)\n",
            "Epoch 49 Step 96/313 Test Loss: 2.003 | Test Acc: 34.826% (1081/3104)\n",
            "Epoch 49 Step 97/313 Test Loss: 2.005 | Test Acc: 34.694% (1088/3136)\n",
            "Epoch 49 Step 98/313 Test Loss: 2.003 | Test Acc: 34.754% (1101/3168)\n",
            "Epoch 49 Step 99/313 Test Loss: 2.000 | Test Acc: 34.750% (1112/3200)\n",
            "Epoch 49 Step 100/313 Test Loss: 2.006 | Test Acc: 34.623% (1119/3232)\n",
            "Epoch 49 Step 101/313 Test Loss: 2.003 | Test Acc: 34.681% (1132/3264)\n",
            "Epoch 49 Step 102/313 Test Loss: 1.997 | Test Acc: 34.830% (1148/3296)\n",
            "Epoch 49 Step 103/313 Test Loss: 2.000 | Test Acc: 34.766% (1157/3328)\n",
            "Epoch 49 Step 104/313 Test Loss: 2.000 | Test Acc: 34.732% (1167/3360)\n",
            "Epoch 49 Step 105/313 Test Loss: 1.996 | Test Acc: 34.847% (1182/3392)\n",
            "Epoch 49 Step 106/313 Test Loss: 1.998 | Test Acc: 34.813% (1192/3424)\n",
            "Epoch 49 Step 107/313 Test Loss: 2.000 | Test Acc: 34.751% (1201/3456)\n",
            "Epoch 49 Step 108/313 Test Loss: 1.998 | Test Acc: 34.719% (1211/3488)\n",
            "Epoch 49 Step 109/313 Test Loss: 1.999 | Test Acc: 34.773% (1224/3520)\n",
            "Epoch 49 Step 110/313 Test Loss: 1.998 | Test Acc: 34.769% (1235/3552)\n",
            "Epoch 49 Step 111/313 Test Loss: 1.995 | Test Acc: 34.849% (1249/3584)\n",
            "Epoch 49 Step 112/313 Test Loss: 1.997 | Test Acc: 34.790% (1258/3616)\n",
            "Epoch 49 Step 113/313 Test Loss: 1.994 | Test Acc: 34.868% (1272/3648)\n",
            "Epoch 49 Step 114/313 Test Loss: 1.990 | Test Acc: 34.973% (1287/3680)\n",
            "Epoch 49 Step 115/313 Test Loss: 1.989 | Test Acc: 34.995% (1299/3712)\n",
            "Epoch 49 Step 116/313 Test Loss: 1.988 | Test Acc: 35.016% (1311/3744)\n",
            "Epoch 49 Step 117/313 Test Loss: 1.990 | Test Acc: 35.064% (1324/3776)\n",
            "Epoch 49 Step 118/313 Test Loss: 1.988 | Test Acc: 35.189% (1340/3808)\n",
            "Epoch 49 Step 119/313 Test Loss: 1.983 | Test Acc: 35.312% (1356/3840)\n",
            "Epoch 49 Step 120/313 Test Loss: 1.983 | Test Acc: 35.279% (1366/3872)\n",
            "Epoch 49 Step 121/313 Test Loss: 1.984 | Test Acc: 35.220% (1375/3904)\n",
            "Epoch 49 Step 122/313 Test Loss: 1.985 | Test Acc: 35.112% (1382/3936)\n",
            "Epoch 49 Step 123/313 Test Loss: 1.981 | Test Acc: 35.282% (1400/3968)\n",
            "Epoch 49 Step 124/313 Test Loss: 1.983 | Test Acc: 35.200% (1408/4000)\n",
            "Epoch 49 Step 125/313 Test Loss: 1.983 | Test Acc: 35.218% (1420/4032)\n",
            "Epoch 49 Step 126/313 Test Loss: 1.986 | Test Acc: 35.113% (1427/4064)\n",
            "Epoch 49 Step 127/313 Test Loss: 1.986 | Test Acc: 35.181% (1441/4096)\n",
            "Epoch 49 Step 128/313 Test Loss: 1.987 | Test Acc: 35.199% (1453/4128)\n",
            "Epoch 49 Step 129/313 Test Loss: 1.988 | Test Acc: 35.144% (1462/4160)\n",
            "Epoch 49 Step 130/313 Test Loss: 1.986 | Test Acc: 35.162% (1474/4192)\n",
            "Epoch 49 Step 131/313 Test Loss: 1.987 | Test Acc: 35.156% (1485/4224)\n",
            "Epoch 49 Step 132/313 Test Loss: 1.989 | Test Acc: 35.033% (1491/4256)\n",
            "Epoch 49 Step 133/313 Test Loss: 1.987 | Test Acc: 35.028% (1502/4288)\n",
            "Epoch 49 Step 134/313 Test Loss: 1.989 | Test Acc: 34.977% (1511/4320)\n",
            "Epoch 49 Step 135/313 Test Loss: 1.991 | Test Acc: 34.972% (1522/4352)\n",
            "Epoch 49 Step 136/313 Test Loss: 1.991 | Test Acc: 34.945% (1532/4384)\n",
            "Epoch 49 Step 137/313 Test Loss: 1.989 | Test Acc: 34.986% (1545/4416)\n",
            "Epoch 49 Step 138/313 Test Loss: 1.990 | Test Acc: 35.027% (1558/4448)\n",
            "Epoch 49 Step 139/313 Test Loss: 1.989 | Test Acc: 35.067% (1571/4480)\n",
            "Epoch 49 Step 140/313 Test Loss: 1.987 | Test Acc: 35.106% (1584/4512)\n",
            "Epoch 49 Step 141/313 Test Loss: 1.987 | Test Acc: 35.101% (1595/4544)\n",
            "Epoch 49 Step 142/313 Test Loss: 1.988 | Test Acc: 35.052% (1604/4576)\n",
            "Epoch 49 Step 143/313 Test Loss: 1.987 | Test Acc: 35.113% (1618/4608)\n",
            "Epoch 49 Step 144/313 Test Loss: 1.987 | Test Acc: 35.108% (1629/4640)\n",
            "Epoch 49 Step 145/313 Test Loss: 1.985 | Test Acc: 35.188% (1644/4672)\n",
            "Epoch 49 Step 146/313 Test Loss: 1.985 | Test Acc: 35.119% (1652/4704)\n",
            "Epoch 49 Step 147/313 Test Loss: 1.985 | Test Acc: 35.072% (1661/4736)\n",
            "Epoch 49 Step 148/313 Test Loss: 1.987 | Test Acc: 35.088% (1673/4768)\n",
            "Epoch 49 Step 149/313 Test Loss: 1.989 | Test Acc: 35.042% (1682/4800)\n",
            "Epoch 49 Step 150/313 Test Loss: 1.987 | Test Acc: 35.017% (1692/4832)\n",
            "Epoch 49 Step 151/313 Test Loss: 1.985 | Test Acc: 35.095% (1707/4864)\n",
            "Epoch 49 Step 152/313 Test Loss: 1.982 | Test Acc: 35.131% (1720/4896)\n",
            "Epoch 49 Step 153/313 Test Loss: 1.979 | Test Acc: 35.126% (1731/4928)\n",
            "Epoch 49 Step 154/313 Test Loss: 1.978 | Test Acc: 35.202% (1746/4960)\n",
            "Epoch 49 Step 155/313 Test Loss: 1.981 | Test Acc: 35.076% (1751/4992)\n",
            "Epoch 49 Step 156/313 Test Loss: 1.981 | Test Acc: 35.072% (1762/5024)\n",
            "Epoch 49 Step 157/313 Test Loss: 1.985 | Test Acc: 35.008% (1770/5056)\n",
            "Epoch 49 Step 158/313 Test Loss: 1.984 | Test Acc: 35.083% (1785/5088)\n",
            "Epoch 49 Step 159/313 Test Loss: 1.984 | Test Acc: 35.098% (1797/5120)\n",
            "Epoch 49 Step 160/313 Test Loss: 1.981 | Test Acc: 35.113% (1809/5152)\n",
            "Epoch 49 Step 161/313 Test Loss: 1.981 | Test Acc: 35.204% (1825/5184)\n",
            "Epoch 49 Step 162/313 Test Loss: 1.983 | Test Acc: 35.123% (1832/5216)\n",
            "Epoch 49 Step 163/313 Test Loss: 1.987 | Test Acc: 35.061% (1840/5248)\n",
            "Epoch 49 Step 164/313 Test Loss: 1.988 | Test Acc: 35.057% (1851/5280)\n",
            "Epoch 49 Step 165/313 Test Loss: 1.988 | Test Acc: 35.072% (1863/5312)\n",
            "Epoch 49 Step 166/313 Test Loss: 1.992 | Test Acc: 34.993% (1870/5344)\n",
            "Epoch 49 Step 167/313 Test Loss: 1.994 | Test Acc: 34.970% (1880/5376)\n",
            "Epoch 49 Step 168/313 Test Loss: 1.999 | Test Acc: 34.967% (1891/5408)\n",
            "Epoch 49 Step 169/313 Test Loss: 1.998 | Test Acc: 35.018% (1905/5440)\n",
            "Epoch 49 Step 170/313 Test Loss: 1.999 | Test Acc: 35.033% (1917/5472)\n",
            "Epoch 49 Step 171/313 Test Loss: 2.000 | Test Acc: 34.956% (1924/5504)\n",
            "Epoch 49 Step 172/313 Test Loss: 2.000 | Test Acc: 34.953% (1935/5536)\n",
            "Epoch 49 Step 173/313 Test Loss: 2.002 | Test Acc: 34.914% (1944/5568)\n",
            "Epoch 49 Step 174/313 Test Loss: 2.004 | Test Acc: 34.875% (1953/5600)\n",
            "Epoch 49 Step 175/313 Test Loss: 2.005 | Test Acc: 34.872% (1964/5632)\n",
            "Epoch 49 Step 176/313 Test Loss: 2.005 | Test Acc: 34.816% (1972/5664)\n",
            "Epoch 49 Step 177/313 Test Loss: 2.005 | Test Acc: 34.849% (1985/5696)\n",
            "Epoch 49 Step 178/313 Test Loss: 2.002 | Test Acc: 34.899% (1999/5728)\n",
            "Epoch 49 Step 179/313 Test Loss: 2.002 | Test Acc: 34.844% (2007/5760)\n",
            "Epoch 49 Step 180/313 Test Loss: 2.001 | Test Acc: 34.927% (2023/5792)\n",
            "Epoch 49 Step 181/313 Test Loss: 2.005 | Test Acc: 34.787% (2026/5824)\n",
            "Epoch 49 Step 182/313 Test Loss: 2.006 | Test Acc: 34.768% (2036/5856)\n",
            "Epoch 49 Step 183/313 Test Loss: 2.005 | Test Acc: 34.766% (2047/5888)\n",
            "Epoch 49 Step 184/313 Test Loss: 2.005 | Test Acc: 34.780% (2059/5920)\n",
            "Epoch 49 Step 185/313 Test Loss: 2.006 | Test Acc: 34.745% (2068/5952)\n",
            "Epoch 49 Step 186/313 Test Loss: 2.004 | Test Acc: 34.843% (2085/5984)\n",
            "Epoch 49 Step 187/313 Test Loss: 2.002 | Test Acc: 34.857% (2097/6016)\n",
            "Epoch 49 Step 188/313 Test Loss: 2.001 | Test Acc: 34.838% (2107/6048)\n",
            "Epoch 49 Step 189/313 Test Loss: 2.001 | Test Acc: 34.819% (2117/6080)\n",
            "Epoch 49 Step 190/313 Test Loss: 2.000 | Test Acc: 34.849% (2130/6112)\n",
            "Epoch 49 Step 191/313 Test Loss: 1.999 | Test Acc: 34.831% (2140/6144)\n",
            "Epoch 49 Step 192/313 Test Loss: 2.000 | Test Acc: 34.796% (2149/6176)\n",
            "Epoch 49 Step 193/313 Test Loss: 2.001 | Test Acc: 34.745% (2157/6208)\n",
            "Epoch 49 Step 194/313 Test Loss: 2.001 | Test Acc: 34.728% (2167/6240)\n",
            "Epoch 49 Step 195/313 Test Loss: 2.001 | Test Acc: 34.726% (2178/6272)\n",
            "Epoch 49 Step 196/313 Test Loss: 1.999 | Test Acc: 34.819% (2195/6304)\n",
            "Epoch 49 Step 197/313 Test Loss: 1.999 | Test Acc: 34.817% (2206/6336)\n",
            "Epoch 49 Step 198/313 Test Loss: 1.996 | Test Acc: 34.893% (2222/6368)\n",
            "Epoch 49 Step 199/313 Test Loss: 1.995 | Test Acc: 34.891% (2233/6400)\n",
            "Epoch 49 Step 200/313 Test Loss: 1.996 | Test Acc: 34.810% (2239/6432)\n",
            "Epoch 49 Step 201/313 Test Loss: 1.998 | Test Acc: 34.746% (2246/6464)\n",
            "Epoch 49 Step 202/313 Test Loss: 1.997 | Test Acc: 34.791% (2260/6496)\n",
            "Epoch 49 Step 203/313 Test Loss: 1.997 | Test Acc: 34.743% (2268/6528)\n",
            "Epoch 49 Step 204/313 Test Loss: 1.999 | Test Acc: 34.695% (2276/6560)\n",
            "Epoch 49 Step 205/313 Test Loss: 2.001 | Test Acc: 34.663% (2285/6592)\n",
            "Epoch 49 Step 206/313 Test Loss: 2.002 | Test Acc: 34.617% (2293/6624)\n",
            "Epoch 49 Step 207/313 Test Loss: 2.002 | Test Acc: 34.585% (2302/6656)\n",
            "Epoch 49 Step 208/313 Test Loss: 2.003 | Test Acc: 34.584% (2313/6688)\n",
            "Epoch 49 Step 209/313 Test Loss: 2.004 | Test Acc: 34.554% (2322/6720)\n",
            "Epoch 49 Step 210/313 Test Loss: 2.003 | Test Acc: 34.553% (2333/6752)\n",
            "Epoch 49 Step 211/313 Test Loss: 2.003 | Test Acc: 34.552% (2344/6784)\n",
            "Epoch 49 Step 212/313 Test Loss: 2.000 | Test Acc: 34.668% (2363/6816)\n",
            "Epoch 49 Step 213/313 Test Loss: 2.000 | Test Acc: 34.696% (2376/6848)\n",
            "Epoch 49 Step 214/313 Test Loss: 2.002 | Test Acc: 34.608% (2381/6880)\n",
            "Epoch 49 Step 215/313 Test Loss: 2.000 | Test Acc: 34.679% (2397/6912)\n",
            "Epoch 49 Step 216/313 Test Loss: 2.002 | Test Acc: 34.634% (2405/6944)\n",
            "Epoch 49 Step 217/313 Test Loss: 2.003 | Test Acc: 34.590% (2413/6976)\n",
            "Epoch 49 Step 218/313 Test Loss: 2.004 | Test Acc: 34.603% (2425/7008)\n",
            "Epoch 49 Step 219/313 Test Loss: 2.005 | Test Acc: 34.560% (2433/7040)\n",
            "Epoch 49 Step 220/313 Test Loss: 2.008 | Test Acc: 34.531% (2442/7072)\n",
            "Epoch 49 Step 221/313 Test Loss: 2.007 | Test Acc: 34.558% (2455/7104)\n",
            "Epoch 49 Step 222/313 Test Loss: 2.010 | Test Acc: 34.501% (2462/7136)\n",
            "Epoch 49 Step 223/313 Test Loss: 2.008 | Test Acc: 34.515% (2474/7168)\n",
            "Epoch 49 Step 224/313 Test Loss: 2.009 | Test Acc: 34.528% (2486/7200)\n",
            "Epoch 49 Step 225/313 Test Loss: 2.011 | Test Acc: 34.444% (2491/7232)\n",
            "Epoch 49 Step 226/313 Test Loss: 2.010 | Test Acc: 34.471% (2504/7264)\n",
            "Epoch 49 Step 227/313 Test Loss: 2.008 | Test Acc: 34.471% (2515/7296)\n",
            "Epoch 49 Step 228/313 Test Loss: 2.008 | Test Acc: 34.484% (2527/7328)\n",
            "Epoch 49 Step 229/313 Test Loss: 2.008 | Test Acc: 34.457% (2536/7360)\n",
            "Epoch 49 Step 230/313 Test Loss: 2.007 | Test Acc: 34.470% (2548/7392)\n",
            "Epoch 49 Step 231/313 Test Loss: 2.006 | Test Acc: 34.456% (2558/7424)\n",
            "Epoch 49 Step 232/313 Test Loss: 2.006 | Test Acc: 34.455% (2569/7456)\n",
            "Epoch 49 Step 233/313 Test Loss: 2.006 | Test Acc: 34.482% (2582/7488)\n",
            "Epoch 49 Step 234/313 Test Loss: 2.006 | Test Acc: 34.495% (2594/7520)\n",
            "Epoch 49 Step 235/313 Test Loss: 2.005 | Test Acc: 34.494% (2605/7552)\n",
            "Epoch 49 Step 236/313 Test Loss: 2.007 | Test Acc: 34.480% (2615/7584)\n",
            "Epoch 49 Step 237/313 Test Loss: 2.006 | Test Acc: 34.506% (2628/7616)\n",
            "Epoch 49 Step 238/313 Test Loss: 2.006 | Test Acc: 34.480% (2637/7648)\n",
            "Epoch 49 Step 239/313 Test Loss: 2.005 | Test Acc: 34.479% (2648/7680)\n",
            "Epoch 49 Step 240/313 Test Loss: 2.003 | Test Acc: 34.595% (2668/7712)\n",
            "Epoch 49 Step 241/313 Test Loss: 2.003 | Test Acc: 34.595% (2679/7744)\n",
            "Epoch 49 Step 242/313 Test Loss: 2.002 | Test Acc: 34.581% (2689/7776)\n",
            "Epoch 49 Step 243/313 Test Loss: 2.000 | Test Acc: 34.644% (2705/7808)\n",
            "Epoch 49 Step 244/313 Test Loss: 2.001 | Test Acc: 34.630% (2715/7840)\n",
            "Epoch 49 Step 245/313 Test Loss: 2.002 | Test Acc: 34.642% (2727/7872)\n",
            "Epoch 49 Step 246/313 Test Loss: 2.002 | Test Acc: 34.641% (2738/7904)\n",
            "Epoch 49 Step 247/313 Test Loss: 2.003 | Test Acc: 34.577% (2744/7936)\n",
            "Epoch 49 Step 248/313 Test Loss: 2.004 | Test Acc: 34.526% (2751/7968)\n",
            "Epoch 49 Step 249/313 Test Loss: 2.003 | Test Acc: 34.587% (2767/8000)\n",
            "Epoch 49 Step 250/313 Test Loss: 2.002 | Test Acc: 34.599% (2779/8032)\n",
            "Epoch 49 Step 251/313 Test Loss: 2.003 | Test Acc: 34.586% (2789/8064)\n",
            "Epoch 49 Step 252/313 Test Loss: 2.002 | Test Acc: 34.610% (2802/8096)\n",
            "Epoch 49 Step 253/313 Test Loss: 2.003 | Test Acc: 34.596% (2812/8128)\n",
            "Epoch 49 Step 254/313 Test Loss: 2.003 | Test Acc: 34.632% (2826/8160)\n",
            "Epoch 49 Step 255/313 Test Loss: 2.002 | Test Acc: 34.607% (2835/8192)\n",
            "Epoch 49 Step 256/313 Test Loss: 2.002 | Test Acc: 34.606% (2846/8224)\n",
            "Epoch 49 Step 257/313 Test Loss: 2.003 | Test Acc: 34.569% (2854/8256)\n",
            "Epoch 49 Step 258/313 Test Loss: 2.003 | Test Acc: 34.544% (2863/8288)\n",
            "Epoch 49 Step 259/313 Test Loss: 2.004 | Test Acc: 34.507% (2871/8320)\n",
            "Epoch 49 Step 260/313 Test Loss: 2.004 | Test Acc: 34.495% (2881/8352)\n",
            "Epoch 49 Step 261/313 Test Loss: 2.004 | Test Acc: 34.506% (2893/8384)\n",
            "Epoch 49 Step 262/313 Test Loss: 2.002 | Test Acc: 34.529% (2906/8416)\n",
            "Epoch 49 Step 263/313 Test Loss: 2.002 | Test Acc: 34.517% (2916/8448)\n",
            "Epoch 49 Step 264/313 Test Loss: 2.002 | Test Acc: 34.458% (2922/8480)\n",
            "Epoch 49 Step 265/313 Test Loss: 2.003 | Test Acc: 34.434% (2931/8512)\n",
            "Epoch 49 Step 266/313 Test Loss: 2.005 | Test Acc: 34.434% (2942/8544)\n",
            "Epoch 49 Step 267/313 Test Loss: 2.005 | Test Acc: 34.410% (2951/8576)\n",
            "Epoch 49 Step 268/313 Test Loss: 2.005 | Test Acc: 34.445% (2965/8608)\n",
            "Epoch 49 Step 269/313 Test Loss: 2.004 | Test Acc: 34.491% (2980/8640)\n",
            "Epoch 49 Step 270/313 Test Loss: 2.005 | Test Acc: 34.433% (2986/8672)\n",
            "Epoch 49 Step 271/313 Test Loss: 2.006 | Test Acc: 34.421% (2996/8704)\n",
            "Epoch 49 Step 272/313 Test Loss: 2.007 | Test Acc: 34.386% (3004/8736)\n",
            "Epoch 49 Step 273/313 Test Loss: 2.006 | Test Acc: 34.443% (3020/8768)\n",
            "Epoch 49 Step 274/313 Test Loss: 2.005 | Test Acc: 34.432% (3030/8800)\n",
            "Epoch 49 Step 275/313 Test Loss: 2.006 | Test Acc: 34.443% (3042/8832)\n",
            "Epoch 49 Step 276/313 Test Loss: 2.007 | Test Acc: 34.409% (3050/8864)\n",
            "Epoch 49 Step 277/313 Test Loss: 2.006 | Test Acc: 34.431% (3063/8896)\n",
            "Epoch 49 Step 278/313 Test Loss: 2.004 | Test Acc: 34.532% (3083/8928)\n",
            "Epoch 49 Step 279/313 Test Loss: 2.004 | Test Acc: 34.509% (3092/8960)\n",
            "Epoch 49 Step 280/313 Test Loss: 2.004 | Test Acc: 34.464% (3099/8992)\n",
            "Epoch 49 Step 281/313 Test Loss: 2.005 | Test Acc: 34.464% (3110/9024)\n",
            "Epoch 49 Step 282/313 Test Loss: 2.005 | Test Acc: 34.441% (3119/9056)\n",
            "Epoch 49 Step 283/313 Test Loss: 2.004 | Test Acc: 34.463% (3132/9088)\n",
            "Epoch 49 Step 284/313 Test Loss: 2.003 | Test Acc: 34.485% (3145/9120)\n",
            "Epoch 49 Step 285/313 Test Loss: 2.004 | Test Acc: 34.473% (3155/9152)\n",
            "Epoch 49 Step 286/313 Test Loss: 2.003 | Test Acc: 34.495% (3168/9184)\n",
            "Epoch 49 Step 287/313 Test Loss: 2.002 | Test Acc: 34.538% (3183/9216)\n",
            "Epoch 49 Step 288/313 Test Loss: 2.000 | Test Acc: 34.537% (3194/9248)\n",
            "Epoch 49 Step 289/313 Test Loss: 2.000 | Test Acc: 34.537% (3205/9280)\n",
            "Epoch 49 Step 290/313 Test Loss: 2.001 | Test Acc: 34.504% (3213/9312)\n",
            "Epoch 49 Step 291/313 Test Loss: 2.001 | Test Acc: 34.471% (3221/9344)\n",
            "Epoch 49 Step 292/313 Test Loss: 2.000 | Test Acc: 34.514% (3236/9376)\n",
            "Epoch 49 Step 293/313 Test Loss: 2.000 | Test Acc: 34.524% (3248/9408)\n",
            "Epoch 49 Step 294/313 Test Loss: 2.000 | Test Acc: 34.502% (3257/9440)\n",
            "Epoch 49 Step 295/313 Test Loss: 2.001 | Test Acc: 34.491% (3267/9472)\n",
            "Epoch 49 Step 296/313 Test Loss: 2.001 | Test Acc: 34.501% (3279/9504)\n",
            "Epoch 49 Step 297/313 Test Loss: 2.000 | Test Acc: 34.532% (3293/9536)\n",
            "Epoch 49 Step 298/313 Test Loss: 2.001 | Test Acc: 34.500% (3301/9568)\n",
            "Epoch 49 Step 299/313 Test Loss: 2.000 | Test Acc: 34.552% (3317/9600)\n",
            "Epoch 49 Step 300/313 Test Loss: 2.000 | Test Acc: 34.551% (3328/9632)\n",
            "Epoch 49 Step 301/313 Test Loss: 2.000 | Test Acc: 34.510% (3335/9664)\n",
            "Epoch 49 Step 302/313 Test Loss: 2.001 | Test Acc: 34.478% (3343/9696)\n",
            "Epoch 49 Step 303/313 Test Loss: 2.001 | Test Acc: 34.519% (3358/9728)\n",
            "Epoch 49 Step 304/313 Test Loss: 2.002 | Test Acc: 34.498% (3367/9760)\n",
            "Epoch 49 Step 305/313 Test Loss: 2.003 | Test Acc: 34.477% (3376/9792)\n",
            "Epoch 49 Step 306/313 Test Loss: 2.004 | Test Acc: 34.446% (3384/9824)\n",
            "Epoch 49 Step 307/313 Test Loss: 2.004 | Test Acc: 34.426% (3393/9856)\n",
            "Epoch 49 Step 308/313 Test Loss: 2.006 | Test Acc: 34.405% (3402/9888)\n",
            "Epoch 49 Step 309/313 Test Loss: 2.007 | Test Acc: 34.355% (3408/9920)\n",
            "Epoch 49 Step 310/313 Test Loss: 2.007 | Test Acc: 34.325% (3416/9952)\n",
            "Epoch 49 Step 311/313 Test Loss: 2.007 | Test Acc: 34.315% (3426/9984)\n",
            "Epoch 49 Step 312/313 Test Loss: 2.006 | Test Acc: 34.310% (3431/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHGlidpu2gpJ",
        "outputId": "4597cc47-ae34-44aa-972e-f3fc6225e84e"
      },
      "source": [
        "# Training Wide_ResNet16\n",
        "best_train_acc = best_test_acc = 0\n",
        "privacy_engine = PrivacyEngine(\n",
        "    Linear_WideLeNet5,\n",
        "    batch_size,\n",
        "    sample_size,\n",
        "    alphas=[10, 100],\n",
        "    noise_multiplier=1.3,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "privacy_engine.attach(optimizer1)\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+50):\n",
        "  train_wide(Linear_WideLeNet5, epoch)\n",
        "  test(Linear_WideLeNet5, epoch)\n",
        "\n",
        "best_acc.append([best_train_acc, best_test_acc])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/opacus/privacy_engine.py:104: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 0/1563 Loss: 2.276 | Acc: 12.500% (4/32)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1/1563 Loss: 2.285 | Acc: 9.375% (6/64)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 2/1563 Loss: 2.292 | Acc: 8.333% (8/96)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 3/1563 Loss: 2.296 | Acc: 9.375% (12/128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 4/1563 Loss: 2.298 | Acc: 8.125% (13/160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 5/1563 Loss: 2.299 | Acc: 7.812% (15/192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 6/1563 Loss: 2.297 | Acc: 8.929% (20/224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 7/1563 Loss: 2.296 | Acc: 9.766% (25/256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 8/1563 Loss: 2.297 | Acc: 9.375% (27/288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 9/1563 Loss: 2.297 | Acc: 10.000% (32/320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 10/1563 Loss: 2.298 | Acc: 9.943% (35/352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 11/1563 Loss: 2.298 | Acc: 10.156% (39/384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 12/1563 Loss: 2.297 | Acc: 10.577% (44/416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 13/1563 Loss: 2.298 | Acc: 10.714% (48/448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 14/1563 Loss: 2.298 | Acc: 10.833% (52/480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 15/1563 Loss: 2.298 | Acc: 10.742% (55/512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 16/1563 Loss: 2.297 | Acc: 11.213% (61/544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 17/1563 Loss: 2.297 | Acc: 11.458% (66/576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 18/1563 Loss: 2.297 | Acc: 11.349% (69/608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 19/1563 Loss: 2.297 | Acc: 11.406% (73/640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 20/1563 Loss: 2.297 | Acc: 11.310% (76/672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 21/1563 Loss: 2.298 | Acc: 11.222% (79/704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 22/1563 Loss: 2.298 | Acc: 10.870% (80/736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 23/1563 Loss: 2.297 | Acc: 10.677% (82/768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 24/1563 Loss: 2.297 | Acc: 10.750% (86/800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 25/1563 Loss: 2.297 | Acc: 11.058% (92/832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 26/1563 Loss: 2.297 | Acc: 11.111% (96/864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 27/1563 Loss: 2.297 | Acc: 11.272% (101/896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 28/1563 Loss: 2.298 | Acc: 11.099% (103/928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 29/1563 Loss: 2.298 | Acc: 10.938% (105/960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 30/1563 Loss: 2.298 | Acc: 10.685% (106/992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 31/1563 Loss: 2.298 | Acc: 10.449% (107/1024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 32/1563 Loss: 2.299 | Acc: 10.227% (108/1056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 33/1563 Loss: 2.299 | Acc: 9.926% (108/1088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 34/1563 Loss: 2.299 | Acc: 9.821% (110/1120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 35/1563 Loss: 2.300 | Acc: 9.722% (112/1152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 36/1563 Loss: 2.300 | Acc: 9.882% (117/1184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 37/1563 Loss: 2.300 | Acc: 9.786% (119/1216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 38/1563 Loss: 2.300 | Acc: 9.776% (122/1248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 39/1563 Loss: 2.300 | Acc: 10.078% (129/1280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 40/1563 Loss: 2.300 | Acc: 9.985% (131/1312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 41/1563 Loss: 2.300 | Acc: 9.970% (134/1344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 42/1563 Loss: 2.301 | Acc: 10.029% (138/1376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 43/1563 Loss: 2.300 | Acc: 10.014% (141/1408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 44/1563 Loss: 2.301 | Acc: 9.861% (142/1440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 45/1563 Loss: 2.301 | Acc: 9.783% (144/1472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 46/1563 Loss: 2.302 | Acc: 9.774% (147/1504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 47/1563 Loss: 2.301 | Acc: 9.831% (151/1536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 48/1563 Loss: 2.301 | Acc: 9.758% (153/1568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 49/1563 Loss: 2.301 | Acc: 9.938% (159/1600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 50/1563 Loss: 2.301 | Acc: 10.049% (164/1632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 51/1563 Loss: 2.301 | Acc: 10.156% (169/1664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 52/1563 Loss: 2.301 | Acc: 10.436% (177/1696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 53/1563 Loss: 2.301 | Acc: 10.532% (182/1728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 54/1563 Loss: 2.301 | Acc: 10.511% (185/1760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 55/1563 Loss: 2.301 | Acc: 10.435% (187/1792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 56/1563 Loss: 2.301 | Acc: 10.526% (192/1824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 57/1563 Loss: 2.301 | Acc: 10.560% (196/1856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 58/1563 Loss: 2.301 | Acc: 10.487% (198/1888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 59/1563 Loss: 2.301 | Acc: 10.833% (208/1920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 60/1563 Loss: 2.301 | Acc: 10.758% (210/1952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 61/1563 Loss: 2.301 | Acc: 10.685% (212/1984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 62/1563 Loss: 2.302 | Acc: 10.565% (213/2016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 63/1563 Loss: 2.302 | Acc: 10.498% (215/2048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 64/1563 Loss: 2.302 | Acc: 10.577% (220/2080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 65/1563 Loss: 2.302 | Acc: 10.511% (222/2112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 66/1563 Loss: 2.302 | Acc: 10.494% (225/2144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 67/1563 Loss: 2.302 | Acc: 10.478% (228/2176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 68/1563 Loss: 2.302 | Acc: 10.462% (231/2208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 69/1563 Loss: 2.302 | Acc: 10.357% (232/2240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 70/1563 Loss: 2.302 | Acc: 10.343% (235/2272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 71/1563 Loss: 2.302 | Acc: 10.547% (243/2304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 72/1563 Loss: 2.302 | Acc: 10.574% (247/2336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 73/1563 Loss: 2.302 | Acc: 10.473% (248/2368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 74/1563 Loss: 2.302 | Acc: 10.542% (253/2400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 75/1563 Loss: 2.302 | Acc: 10.567% (257/2432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 76/1563 Loss: 2.302 | Acc: 10.552% (260/2464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 77/1563 Loss: 2.302 | Acc: 10.577% (264/2496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 78/1563 Loss: 2.302 | Acc: 10.522% (266/2528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 79/1563 Loss: 2.302 | Acc: 10.547% (270/2560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 80/1563 Loss: 2.302 | Acc: 10.532% (273/2592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 81/1563 Loss: 2.302 | Acc: 10.556% (277/2624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 82/1563 Loss: 2.302 | Acc: 10.693% (284/2656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 83/1563 Loss: 2.302 | Acc: 10.826% (291/2688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 84/1563 Loss: 2.302 | Acc: 10.809% (294/2720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 85/1563 Loss: 2.302 | Acc: 10.683% (294/2752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 86/1563 Loss: 2.302 | Acc: 10.704% (298/2784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 87/1563 Loss: 2.302 | Acc: 10.724% (302/2816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 88/1563 Loss: 2.302 | Acc: 10.674% (304/2848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 89/1563 Loss: 2.302 | Acc: 10.660% (307/2880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 90/1563 Loss: 2.302 | Acc: 10.611% (309/2912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 91/1563 Loss: 2.302 | Acc: 10.632% (313/2944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 92/1563 Loss: 2.302 | Acc: 10.618% (316/2976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 93/1563 Loss: 2.302 | Acc: 10.572% (318/3008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 94/1563 Loss: 2.302 | Acc: 10.592% (322/3040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 95/1563 Loss: 2.302 | Acc: 10.547% (324/3072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 96/1563 Loss: 2.302 | Acc: 10.503% (326/3104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 97/1563 Loss: 2.302 | Acc: 10.491% (329/3136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 98/1563 Loss: 2.302 | Acc: 10.606% (336/3168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 99/1563 Loss: 2.302 | Acc: 10.750% (344/3200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 100/1563 Loss: 2.302 | Acc: 10.705% (346/3232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 101/1563 Loss: 2.302 | Acc: 10.662% (348/3264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 102/1563 Loss: 2.302 | Acc: 10.619% (350/3296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 103/1563 Loss: 2.302 | Acc: 10.607% (353/3328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 104/1563 Loss: 2.302 | Acc: 10.565% (355/3360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 105/1563 Loss: 2.302 | Acc: 10.672% (362/3392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 106/1563 Loss: 2.302 | Acc: 10.777% (369/3424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 107/1563 Loss: 2.302 | Acc: 10.706% (370/3456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 108/1563 Loss: 2.302 | Acc: 10.694% (373/3488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 109/1563 Loss: 2.302 | Acc: 10.625% (374/3520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 110/1563 Loss: 2.302 | Acc: 10.642% (378/3552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 111/1563 Loss: 2.302 | Acc: 10.575% (379/3584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 112/1563 Loss: 2.302 | Acc: 10.537% (381/3616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 113/1563 Loss: 2.302 | Acc: 10.526% (384/3648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 114/1563 Loss: 2.302 | Acc: 10.489% (386/3680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 115/1563 Loss: 2.302 | Acc: 10.506% (390/3712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 116/1563 Loss: 2.302 | Acc: 10.470% (392/3744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 117/1563 Loss: 2.302 | Acc: 10.434% (394/3776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 118/1563 Loss: 2.302 | Acc: 10.399% (396/3808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 119/1563 Loss: 2.302 | Acc: 10.469% (402/3840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 120/1563 Loss: 2.302 | Acc: 10.460% (405/3872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 121/1563 Loss: 2.302 | Acc: 10.476% (409/3904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 122/1563 Loss: 2.302 | Acc: 10.493% (413/3936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 123/1563 Loss: 2.302 | Acc: 10.484% (416/3968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 124/1563 Loss: 2.301 | Acc: 10.500% (420/4000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 125/1563 Loss: 2.301 | Acc: 10.466% (422/4032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 126/1563 Loss: 2.301 | Acc: 10.458% (425/4064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 127/1563 Loss: 2.301 | Acc: 10.449% (428/4096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 128/1563 Loss: 2.301 | Acc: 10.417% (430/4128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 129/1563 Loss: 2.301 | Acc: 10.385% (432/4160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 130/1563 Loss: 2.301 | Acc: 10.448% (438/4192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 131/1563 Loss: 2.301 | Acc: 10.440% (441/4224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 132/1563 Loss: 2.302 | Acc: 10.409% (443/4256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 133/1563 Loss: 2.301 | Acc: 10.448% (448/4288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 134/1563 Loss: 2.301 | Acc: 10.463% (452/4320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 135/1563 Loss: 2.301 | Acc: 10.455% (455/4352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 136/1563 Loss: 2.301 | Acc: 10.516% (461/4384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 137/1563 Loss: 2.302 | Acc: 10.553% (466/4416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 138/1563 Loss: 2.302 | Acc: 10.567% (470/4448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 139/1563 Loss: 2.302 | Acc: 10.580% (474/4480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 140/1563 Loss: 2.302 | Acc: 10.616% (479/4512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 141/1563 Loss: 2.302 | Acc: 10.651% (484/4544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 142/1563 Loss: 2.302 | Acc: 10.621% (486/4576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 143/1563 Loss: 2.302 | Acc: 10.612% (489/4608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 144/1563 Loss: 2.302 | Acc: 10.647% (494/4640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 145/1563 Loss: 2.302 | Acc: 10.574% (494/4672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 146/1563 Loss: 2.302 | Acc: 10.502% (494/4704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 147/1563 Loss: 2.302 | Acc: 10.452% (495/4736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 148/1563 Loss: 2.302 | Acc: 10.466% (499/4768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 149/1563 Loss: 2.302 | Acc: 10.458% (502/4800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 150/1563 Loss: 2.302 | Acc: 10.493% (507/4832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 151/1563 Loss: 2.302 | Acc: 10.465% (509/4864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 152/1563 Loss: 2.302 | Acc: 10.458% (512/4896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 153/1563 Loss: 2.302 | Acc: 10.450% (515/4928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 154/1563 Loss: 2.302 | Acc: 10.423% (517/4960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 155/1563 Loss: 2.302 | Acc: 10.377% (518/4992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 156/1563 Loss: 2.302 | Acc: 10.330% (519/5024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 157/1563 Loss: 2.302 | Acc: 10.324% (522/5056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 158/1563 Loss: 2.302 | Acc: 10.338% (526/5088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 159/1563 Loss: 2.302 | Acc: 10.312% (528/5120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 160/1563 Loss: 2.302 | Acc: 10.307% (531/5152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 161/1563 Loss: 2.302 | Acc: 10.359% (537/5184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 162/1563 Loss: 2.302 | Acc: 10.353% (540/5216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 163/1563 Loss: 2.302 | Acc: 10.328% (542/5248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 164/1563 Loss: 2.302 | Acc: 10.265% (542/5280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 165/1563 Loss: 2.302 | Acc: 10.222% (543/5312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 166/1563 Loss: 2.302 | Acc: 10.254% (548/5344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 167/1563 Loss: 2.302 | Acc: 10.249% (551/5376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 168/1563 Loss: 2.302 | Acc: 10.263% (555/5408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 169/1563 Loss: 2.302 | Acc: 10.257% (558/5440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 170/1563 Loss: 2.302 | Acc: 10.252% (561/5472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 171/1563 Loss: 2.302 | Acc: 10.229% (563/5504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 172/1563 Loss: 2.302 | Acc: 10.170% (563/5536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 173/1563 Loss: 2.302 | Acc: 10.201% (568/5568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 174/1563 Loss: 2.302 | Acc: 10.179% (570/5600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 175/1563 Loss: 2.302 | Acc: 10.227% (576/5632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 176/1563 Loss: 2.302 | Acc: 10.258% (581/5664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 177/1563 Loss: 2.302 | Acc: 10.270% (585/5696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 178/1563 Loss: 2.302 | Acc: 10.248% (587/5728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 179/1563 Loss: 2.302 | Acc: 10.260% (591/5760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 180/1563 Loss: 2.302 | Acc: 10.238% (593/5792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 181/1563 Loss: 2.302 | Acc: 10.251% (597/5824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 182/1563 Loss: 2.302 | Acc: 10.246% (600/5856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 183/1563 Loss: 2.302 | Acc: 10.258% (604/5888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 184/1563 Loss: 2.302 | Acc: 10.287% (609/5920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 185/1563 Loss: 2.302 | Acc: 10.232% (609/5952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 186/1563 Loss: 2.302 | Acc: 10.244% (613/5984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 187/1563 Loss: 2.302 | Acc: 10.239% (616/6016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 188/1563 Loss: 2.302 | Acc: 10.235% (619/6048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 189/1563 Loss: 2.302 | Acc: 10.181% (619/6080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 190/1563 Loss: 2.302 | Acc: 10.209% (624/6112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 191/1563 Loss: 2.302 | Acc: 10.205% (627/6144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 192/1563 Loss: 2.302 | Acc: 10.152% (627/6176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 193/1563 Loss: 2.302 | Acc: 10.132% (629/6208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 194/1563 Loss: 2.302 | Acc: 10.128% (632/6240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 195/1563 Loss: 2.302 | Acc: 10.124% (635/6272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 196/1563 Loss: 2.302 | Acc: 10.121% (638/6304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 197/1563 Loss: 2.302 | Acc: 10.164% (644/6336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 198/1563 Loss: 2.302 | Acc: 10.192% (649/6368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 199/1563 Loss: 2.302 | Acc: 10.250% (656/6400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 200/1563 Loss: 2.302 | Acc: 10.199% (656/6432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 201/1563 Loss: 2.302 | Acc: 10.179% (658/6464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 202/1563 Loss: 2.302 | Acc: 10.252% (666/6496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 203/1563 Loss: 2.302 | Acc: 10.325% (674/6528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 204/1563 Loss: 2.302 | Acc: 10.366% (680/6560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 205/1563 Loss: 2.302 | Acc: 10.361% (683/6592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 206/1563 Loss: 2.302 | Acc: 10.402% (689/6624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 207/1563 Loss: 2.302 | Acc: 10.412% (693/6656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 208/1563 Loss: 2.302 | Acc: 10.452% (699/6688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 209/1563 Loss: 2.302 | Acc: 10.432% (701/6720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 210/1563 Loss: 2.302 | Acc: 10.427% (704/6752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 211/1563 Loss: 2.302 | Acc: 10.451% (709/6784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 212/1563 Loss: 2.302 | Acc: 10.475% (714/6816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 213/1563 Loss: 2.302 | Acc: 10.456% (716/6848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 214/1563 Loss: 2.302 | Acc: 10.436% (718/6880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 215/1563 Loss: 2.302 | Acc: 10.446% (722/6912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 216/1563 Loss: 2.302 | Acc: 10.412% (723/6944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 217/1563 Loss: 2.302 | Acc: 10.393% (725/6976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 218/1563 Loss: 2.302 | Acc: 10.360% (726/7008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 219/1563 Loss: 2.302 | Acc: 10.384% (731/7040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 220/1563 Loss: 2.302 | Acc: 10.421% (737/7072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 221/1563 Loss: 2.302 | Acc: 10.417% (740/7104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 222/1563 Loss: 2.302 | Acc: 10.440% (745/7136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 223/1563 Loss: 2.302 | Acc: 10.449% (749/7168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 224/1563 Loss: 2.302 | Acc: 10.486% (755/7200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 225/1563 Loss: 2.302 | Acc: 10.467% (757/7232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 226/1563 Loss: 2.302 | Acc: 10.463% (760/7264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 227/1563 Loss: 2.302 | Acc: 10.471% (764/7296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 228/1563 Loss: 2.302 | Acc: 10.467% (767/7328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 229/1563 Loss: 2.302 | Acc: 10.476% (771/7360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 230/1563 Loss: 2.302 | Acc: 10.444% (772/7392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 231/1563 Loss: 2.302 | Acc: 10.439% (775/7424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 232/1563 Loss: 2.302 | Acc: 10.448% (779/7456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 233/1563 Loss: 2.302 | Acc: 10.483% (785/7488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 234/1563 Loss: 2.302 | Acc: 10.479% (788/7520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 235/1563 Loss: 2.302 | Acc: 10.487% (792/7552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 236/1563 Loss: 2.302 | Acc: 10.483% (795/7584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 237/1563 Loss: 2.302 | Acc: 10.478% (798/7616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 238/1563 Loss: 2.302 | Acc: 10.486% (802/7648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 239/1563 Loss: 2.302 | Acc: 10.508% (807/7680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 240/1563 Loss: 2.302 | Acc: 10.607% (818/7712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 241/1563 Loss: 2.302 | Acc: 10.615% (822/7744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 242/1563 Loss: 2.302 | Acc: 10.622% (826/7776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 243/1563 Loss: 2.302 | Acc: 10.579% (826/7808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 244/1563 Loss: 2.302 | Acc: 10.561% (828/7840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 245/1563 Loss: 2.302 | Acc: 10.531% (829/7872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 246/1563 Loss: 2.302 | Acc: 10.539% (833/7904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 247/1563 Loss: 2.302 | Acc: 10.534% (836/7936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 248/1563 Loss: 2.302 | Acc: 10.530% (839/7968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 249/1563 Loss: 2.302 | Acc: 10.525% (842/8000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 250/1563 Loss: 2.302 | Acc: 10.508% (844/8032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 251/1563 Loss: 2.302 | Acc: 10.479% (845/8064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 252/1563 Loss: 2.302 | Acc: 10.511% (851/8096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 253/1563 Loss: 2.302 | Acc: 10.470% (851/8128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 254/1563 Loss: 2.302 | Acc: 10.466% (854/8160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 255/1563 Loss: 2.302 | Acc: 10.474% (858/8192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 256/1563 Loss: 2.302 | Acc: 10.433% (858/8224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 257/1563 Loss: 2.302 | Acc: 10.441% (862/8256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 258/1563 Loss: 2.302 | Acc: 10.425% (864/8288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 259/1563 Loss: 2.302 | Acc: 10.433% (868/8320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 260/1563 Loss: 2.302 | Acc: 10.465% (874/8352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 261/1563 Loss: 2.302 | Acc: 10.472% (878/8384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 262/1563 Loss: 2.302 | Acc: 10.444% (879/8416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 263/1563 Loss: 2.302 | Acc: 10.464% (884/8448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 264/1563 Loss: 2.302 | Acc: 10.448% (886/8480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 265/1563 Loss: 2.302 | Acc: 10.432% (888/8512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 266/1563 Loss: 2.302 | Acc: 10.440% (892/8544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 267/1563 Loss: 2.302 | Acc: 10.436% (895/8576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 268/1563 Loss: 2.302 | Acc: 10.455% (900/8608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 269/1563 Loss: 2.302 | Acc: 10.486% (906/8640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 270/1563 Loss: 2.302 | Acc: 10.459% (907/8672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 271/1563 Loss: 2.302 | Acc: 10.432% (908/8704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 272/1563 Loss: 2.302 | Acc: 10.428% (911/8736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 273/1563 Loss: 2.302 | Acc: 10.424% (914/8768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 274/1563 Loss: 2.302 | Acc: 10.443% (919/8800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 275/1563 Loss: 2.302 | Acc: 10.462% (924/8832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 276/1563 Loss: 2.302 | Acc: 10.469% (928/8864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 277/1563 Loss: 2.302 | Acc: 10.465% (931/8896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 278/1563 Loss: 2.302 | Acc: 10.473% (935/8928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 279/1563 Loss: 2.302 | Acc: 10.446% (936/8960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 280/1563 Loss: 2.302 | Acc: 10.487% (943/8992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 281/1563 Loss: 2.301 | Acc: 10.461% (944/9024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 282/1563 Loss: 2.301 | Acc: 10.468% (948/9056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 283/1563 Loss: 2.301 | Acc: 10.442% (949/9088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 284/1563 Loss: 2.301 | Acc: 10.439% (952/9120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 285/1563 Loss: 2.301 | Acc: 10.424% (954/9152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 286/1563 Loss: 2.301 | Acc: 10.442% (959/9184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 287/1563 Loss: 2.301 | Acc: 10.417% (960/9216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 288/1563 Loss: 2.301 | Acc: 10.435% (965/9248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 289/1563 Loss: 2.301 | Acc: 10.442% (969/9280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 290/1563 Loss: 2.301 | Acc: 10.427% (971/9312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 291/1563 Loss: 2.301 | Acc: 10.413% (973/9344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 292/1563 Loss: 2.301 | Acc: 10.420% (977/9376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 293/1563 Loss: 2.301 | Acc: 10.427% (981/9408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 294/1563 Loss: 2.301 | Acc: 10.424% (984/9440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 295/1563 Loss: 2.301 | Acc: 10.441% (989/9472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 296/1563 Loss: 2.301 | Acc: 10.427% (991/9504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 297/1563 Loss: 2.301 | Acc: 10.434% (995/9536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 298/1563 Loss: 2.301 | Acc: 10.399% (995/9568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 299/1563 Loss: 2.301 | Acc: 10.427% (1001/9600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 300/1563 Loss: 2.301 | Acc: 10.455% (1007/9632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 301/1563 Loss: 2.301 | Acc: 10.451% (1010/9664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 302/1563 Loss: 2.301 | Acc: 10.448% (1013/9696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 303/1563 Loss: 2.301 | Acc: 10.475% (1019/9728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 304/1563 Loss: 2.301 | Acc: 10.482% (1023/9760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 305/1563 Loss: 2.301 | Acc: 10.488% (1027/9792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 306/1563 Loss: 2.301 | Acc: 10.515% (1033/9824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 307/1563 Loss: 2.301 | Acc: 10.522% (1037/9856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 308/1563 Loss: 2.301 | Acc: 10.568% (1045/9888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 309/1563 Loss: 2.301 | Acc: 10.534% (1045/9920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 310/1563 Loss: 2.301 | Acc: 10.531% (1048/9952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 311/1563 Loss: 2.301 | Acc: 10.537% (1052/9984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 312/1563 Loss: 2.301 | Acc: 10.503% (1052/10016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 313/1563 Loss: 2.301 | Acc: 10.500% (1055/10048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 314/1563 Loss: 2.301 | Acc: 10.536% (1062/10080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 315/1563 Loss: 2.301 | Acc: 10.542% (1066/10112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 316/1563 Loss: 2.301 | Acc: 10.509% (1066/10144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 317/1563 Loss: 2.301 | Acc: 10.485% (1067/10176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 318/1563 Loss: 2.301 | Acc: 10.502% (1072/10208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 319/1563 Loss: 2.301 | Acc: 10.498% (1075/10240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 320/1563 Loss: 2.301 | Acc: 10.543% (1083/10272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 321/1563 Loss: 2.301 | Acc: 10.549% (1087/10304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 322/1563 Loss: 2.301 | Acc: 10.555% (1091/10336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 323/1563 Loss: 2.301 | Acc: 10.571% (1096/10368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 324/1563 Loss: 2.301 | Acc: 10.567% (1099/10400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 325/1563 Loss: 2.301 | Acc: 10.583% (1104/10432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 326/1563 Loss: 2.301 | Acc: 10.608% (1110/10464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 327/1563 Loss: 2.301 | Acc: 10.595% (1112/10496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 328/1563 Loss: 2.301 | Acc: 10.610% (1117/10528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 329/1563 Loss: 2.301 | Acc: 10.606% (1120/10560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 330/1563 Loss: 2.301 | Acc: 10.631% (1126/10592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 331/1563 Loss: 2.301 | Acc: 10.617% (1128/10624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 332/1563 Loss: 2.301 | Acc: 10.633% (1133/10656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 333/1563 Loss: 2.301 | Acc: 10.619% (1135/10688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 334/1563 Loss: 2.301 | Acc: 10.644% (1141/10720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 335/1563 Loss: 2.301 | Acc: 10.658% (1146/10752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 336/1563 Loss: 2.301 | Acc: 10.673% (1151/10784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 337/1563 Loss: 2.301 | Acc: 10.706% (1158/10816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 338/1563 Loss: 2.301 | Acc: 10.721% (1163/10848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 339/1563 Loss: 2.301 | Acc: 10.744% (1169/10880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 340/1563 Loss: 2.301 | Acc: 10.740% (1172/10912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 341/1563 Loss: 2.301 | Acc: 10.746% (1176/10944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 342/1563 Loss: 2.301 | Acc: 10.760% (1181/10976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 343/1563 Loss: 2.301 | Acc: 10.765% (1185/11008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 344/1563 Loss: 2.301 | Acc: 10.770% (1189/11040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 345/1563 Loss: 2.301 | Acc: 10.766% (1192/11072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 346/1563 Loss: 2.301 | Acc: 10.762% (1195/11104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 347/1563 Loss: 2.301 | Acc: 10.767% (1199/11136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 348/1563 Loss: 2.301 | Acc: 10.808% (1207/11168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 349/1563 Loss: 2.301 | Acc: 10.830% (1213/11200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 350/1563 Loss: 2.301 | Acc: 10.853% (1219/11232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 351/1563 Loss: 2.301 | Acc: 10.866% (1224/11264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 352/1563 Loss: 2.301 | Acc: 10.853% (1226/11296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 353/1563 Loss: 2.301 | Acc: 10.876% (1232/11328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 354/1563 Loss: 2.300 | Acc: 10.889% (1237/11360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 355/1563 Loss: 2.300 | Acc: 10.929% (1245/11392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 356/1563 Loss: 2.300 | Acc: 10.924% (1248/11424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 357/1563 Loss: 2.300 | Acc: 10.946% (1254/11456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 358/1563 Loss: 2.300 | Acc: 10.924% (1255/11488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 359/1563 Loss: 2.300 | Acc: 10.911% (1257/11520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 360/1563 Loss: 2.300 | Acc: 10.916% (1261/11552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 361/1563 Loss: 2.300 | Acc: 10.912% (1264/11584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 362/1563 Loss: 2.300 | Acc: 10.890% (1265/11616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 363/1563 Loss: 2.300 | Acc: 10.895% (1269/11648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 364/1563 Loss: 2.300 | Acc: 10.882% (1271/11680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 365/1563 Loss: 2.300 | Acc: 10.895% (1276/11712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 366/1563 Loss: 2.300 | Acc: 10.899% (1280/11744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 367/1563 Loss: 2.300 | Acc: 10.929% (1287/11776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 368/1563 Loss: 2.300 | Acc: 10.925% (1290/11808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 369/1563 Loss: 2.300 | Acc: 10.938% (1295/11840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 370/1563 Loss: 2.300 | Acc: 10.959% (1301/11872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 371/1563 Loss: 2.300 | Acc: 10.963% (1305/11904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 372/1563 Loss: 2.300 | Acc: 11.000% (1313/11936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 373/1563 Loss: 2.300 | Acc: 10.996% (1316/11968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 374/1563 Loss: 2.300 | Acc: 11.008% (1321/12000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 375/1563 Loss: 2.300 | Acc: 11.037% (1328/12032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 376/1563 Loss: 2.300 | Acc: 11.049% (1333/12064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 377/1563 Loss: 2.300 | Acc: 11.053% (1337/12096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 378/1563 Loss: 2.300 | Acc: 11.074% (1343/12128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 379/1563 Loss: 2.300 | Acc: 11.094% (1349/12160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 380/1563 Loss: 2.300 | Acc: 11.106% (1354/12192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 381/1563 Loss: 2.300 | Acc: 11.109% (1358/12224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 382/1563 Loss: 2.300 | Acc: 11.105% (1361/12256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 383/1563 Loss: 2.299 | Acc: 11.092% (1363/12288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 384/1563 Loss: 2.299 | Acc: 11.104% (1368/12320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 385/1563 Loss: 2.299 | Acc: 11.124% (1374/12352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 386/1563 Loss: 2.299 | Acc: 11.119% (1377/12384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 387/1563 Loss: 2.299 | Acc: 11.163% (1386/12416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 388/1563 Loss: 2.299 | Acc: 11.150% (1388/12448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 389/1563 Loss: 2.299 | Acc: 11.154% (1392/12480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 390/1563 Loss: 2.299 | Acc: 11.141% (1394/12512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 391/1563 Loss: 2.299 | Acc: 11.121% (1395/12544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 392/1563 Loss: 2.299 | Acc: 11.148% (1402/12576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 393/1563 Loss: 2.299 | Acc: 11.160% (1407/12608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 394/1563 Loss: 2.299 | Acc: 11.171% (1412/12640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 395/1563 Loss: 2.299 | Acc: 11.190% (1418/12672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 396/1563 Loss: 2.299 | Acc: 11.209% (1424/12704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 397/1563 Loss: 2.299 | Acc: 11.244% (1432/12736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 398/1563 Loss: 2.299 | Acc: 11.247% (1436/12768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 399/1563 Loss: 2.299 | Acc: 11.281% (1444/12800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 400/1563 Loss: 2.299 | Acc: 11.308% (1451/12832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 401/1563 Loss: 2.299 | Acc: 11.342% (1459/12864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 402/1563 Loss: 2.299 | Acc: 11.337% (1462/12896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 403/1563 Loss: 2.299 | Acc: 11.371% (1470/12928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 404/1563 Loss: 2.299 | Acc: 11.389% (1476/12960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 405/1563 Loss: 2.299 | Acc: 11.361% (1476/12992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 406/1563 Loss: 2.299 | Acc: 11.371% (1481/13024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 407/1563 Loss: 2.298 | Acc: 11.397% (1488/13056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 408/1563 Loss: 2.298 | Acc: 11.392% (1491/13088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 409/1563 Loss: 2.298 | Acc: 11.395% (1495/13120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 410/1563 Loss: 2.298 | Acc: 11.390% (1498/13152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 411/1563 Loss: 2.298 | Acc: 11.400% (1503/13184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 412/1563 Loss: 2.298 | Acc: 11.395% (1506/13216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 413/1563 Loss: 2.298 | Acc: 11.398% (1510/13248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 414/1563 Loss: 2.298 | Acc: 11.401% (1514/13280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 415/1563 Loss: 2.298 | Acc: 11.433% (1522/13312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 416/1563 Loss: 2.298 | Acc: 11.421% (1524/13344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 417/1563 Loss: 2.298 | Acc: 11.431% (1529/13376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 418/1563 Loss: 2.298 | Acc: 11.478% (1539/13408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 419/1563 Loss: 2.298 | Acc: 11.503% (1546/13440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 420/1563 Loss: 2.298 | Acc: 11.498% (1549/13472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 421/1563 Loss: 2.298 | Acc: 11.508% (1554/13504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 422/1563 Loss: 2.298 | Acc: 11.510% (1558/13536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 423/1563 Loss: 2.298 | Acc: 11.520% (1563/13568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 424/1563 Loss: 2.298 | Acc: 11.522% (1567/13600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 425/1563 Loss: 2.298 | Acc: 11.539% (1573/13632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 426/1563 Loss: 2.297 | Acc: 11.549% (1578/13664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 427/1563 Loss: 2.297 | Acc: 11.558% (1583/13696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 428/1563 Loss: 2.297 | Acc: 11.611% (1594/13728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 429/1563 Loss: 2.297 | Acc: 11.621% (1599/13760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 430/1563 Loss: 2.297 | Acc: 11.630% (1604/13792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 431/1563 Loss: 2.297 | Acc: 11.617% (1606/13824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 432/1563 Loss: 2.297 | Acc: 11.620% (1610/13856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 433/1563 Loss: 2.297 | Acc: 11.643% (1617/13888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 434/1563 Loss: 2.297 | Acc: 11.638% (1620/13920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 435/1563 Loss: 2.297 | Acc: 11.654% (1626/13952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 436/1563 Loss: 2.297 | Acc: 11.656% (1630/13984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 437/1563 Loss: 2.297 | Acc: 11.687% (1638/14016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 438/1563 Loss: 2.297 | Acc: 11.681% (1641/14048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 439/1563 Loss: 2.297 | Acc: 11.676% (1644/14080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 440/1563 Loss: 2.297 | Acc: 11.692% (1650/14112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 441/1563 Loss: 2.296 | Acc: 11.694% (1654/14144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 442/1563 Loss: 2.296 | Acc: 11.717% (1661/14176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 443/1563 Loss: 2.296 | Acc: 11.726% (1666/14208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 444/1563 Loss: 2.296 | Acc: 11.749% (1673/14240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 445/1563 Loss: 2.296 | Acc: 11.778% (1681/14272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 446/1563 Loss: 2.296 | Acc: 11.829% (1692/14304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 447/1563 Loss: 2.296 | Acc: 11.865% (1701/14336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 448/1563 Loss: 2.296 | Acc: 11.874% (1706/14368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 449/1563 Loss: 2.296 | Acc: 11.868% (1709/14400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 450/1563 Loss: 2.296 | Acc: 11.863% (1712/14432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 451/1563 Loss: 2.296 | Acc: 11.892% (1720/14464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 452/1563 Loss: 2.295 | Acc: 11.893% (1724/14496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 453/1563 Loss: 2.295 | Acc: 11.908% (1730/14528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 454/1563 Loss: 2.295 | Acc: 11.930% (1737/14560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 455/1563 Loss: 2.295 | Acc: 11.917% (1739/14592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 456/1563 Loss: 2.295 | Acc: 11.926% (1744/14624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 457/1563 Loss: 2.295 | Acc: 11.934% (1749/14656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 458/1563 Loss: 2.295 | Acc: 11.955% (1756/14688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 459/1563 Loss: 2.295 | Acc: 11.970% (1762/14720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 460/1563 Loss: 2.294 | Acc: 11.998% (1770/14752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 461/1563 Loss: 2.294 | Acc: 12.027% (1778/14784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 462/1563 Loss: 2.294 | Acc: 12.048% (1785/14816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 463/1563 Loss: 2.294 | Acc: 12.062% (1791/14848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 464/1563 Loss: 2.294 | Acc: 12.083% (1798/14880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 465/1563 Loss: 2.294 | Acc: 12.098% (1804/14912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 466/1563 Loss: 2.294 | Acc: 12.099% (1808/14944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 467/1563 Loss: 2.294 | Acc: 12.119% (1815/14976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 468/1563 Loss: 2.293 | Acc: 12.107% (1817/15008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 469/1563 Loss: 2.293 | Acc: 12.121% (1823/15040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 470/1563 Loss: 2.293 | Acc: 12.115% (1826/15072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 471/1563 Loss: 2.293 | Acc: 12.123% (1831/15104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 472/1563 Loss: 2.293 | Acc: 12.143% (1838/15136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 473/1563 Loss: 2.293 | Acc: 12.151% (1843/15168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 474/1563 Loss: 2.293 | Acc: 12.164% (1849/15200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 475/1563 Loss: 2.293 | Acc: 12.172% (1854/15232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 476/1563 Loss: 2.293 | Acc: 12.192% (1861/15264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 477/1563 Loss: 2.292 | Acc: 12.219% (1869/15296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 478/1563 Loss: 2.292 | Acc: 12.252% (1878/15328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 479/1563 Loss: 2.292 | Acc: 12.259% (1883/15360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 480/1563 Loss: 2.292 | Acc: 12.286% (1891/15392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 481/1563 Loss: 2.292 | Acc: 12.293% (1896/15424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 482/1563 Loss: 2.292 | Acc: 12.293% (1900/15456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 483/1563 Loss: 2.292 | Acc: 12.332% (1910/15488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 484/1563 Loss: 2.292 | Acc: 12.358% (1918/15520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 485/1563 Loss: 2.291 | Acc: 12.365% (1923/15552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 486/1563 Loss: 2.291 | Acc: 12.384% (1930/15584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 487/1563 Loss: 2.291 | Acc: 12.391% (1935/15616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 488/1563 Loss: 2.291 | Acc: 12.404% (1941/15648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 489/1563 Loss: 2.291 | Acc: 12.423% (1948/15680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 490/1563 Loss: 2.291 | Acc: 12.449% (1956/15712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 491/1563 Loss: 2.291 | Acc: 12.449% (1960/15744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 492/1563 Loss: 2.290 | Acc: 12.462% (1966/15776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 493/1563 Loss: 2.290 | Acc: 12.487% (1974/15808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 494/1563 Loss: 2.290 | Acc: 12.500% (1980/15840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 495/1563 Loss: 2.290 | Acc: 12.525% (1988/15872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 496/1563 Loss: 2.290 | Acc: 12.544% (1995/15904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 497/1563 Loss: 2.290 | Acc: 12.575% (2004/15936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 498/1563 Loss: 2.289 | Acc: 12.569% (2007/15968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 499/1563 Loss: 2.289 | Acc: 12.594% (2015/16000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 500/1563 Loss: 2.289 | Acc: 12.625% (2024/16032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 501/1563 Loss: 2.289 | Acc: 12.649% (2032/16064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 502/1563 Loss: 2.289 | Acc: 12.649% (2036/16096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 503/1563 Loss: 2.288 | Acc: 12.680% (2045/16128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 504/1563 Loss: 2.288 | Acc: 12.686% (2050/16160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 505/1563 Loss: 2.288 | Acc: 12.704% (2057/16192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 506/1563 Loss: 2.288 | Acc: 12.703% (2061/16224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 507/1563 Loss: 2.288 | Acc: 12.715% (2067/16256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 508/1563 Loss: 2.288 | Acc: 12.733% (2074/16288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 509/1563 Loss: 2.287 | Acc: 12.751% (2081/16320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 510/1563 Loss: 2.287 | Acc: 12.739% (2083/16352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 511/1563 Loss: 2.287 | Acc: 12.744% (2088/16384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 512/1563 Loss: 2.287 | Acc: 12.744% (2092/16416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 513/1563 Loss: 2.287 | Acc: 12.743% (2096/16448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 514/1563 Loss: 2.287 | Acc: 12.749% (2101/16480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 515/1563 Loss: 2.286 | Acc: 12.748% (2105/16512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 516/1563 Loss: 2.286 | Acc: 12.754% (2110/16544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 517/1563 Loss: 2.286 | Acc: 12.765% (2116/16576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 518/1563 Loss: 2.286 | Acc: 12.783% (2123/16608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 519/1563 Loss: 2.286 | Acc: 12.782% (2127/16640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 520/1563 Loss: 2.286 | Acc: 12.788% (2132/16672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 521/1563 Loss: 2.285 | Acc: 12.793% (2137/16704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 522/1563 Loss: 2.285 | Acc: 12.817% (2145/16736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 523/1563 Loss: 2.285 | Acc: 12.804% (2147/16768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 524/1563 Loss: 2.285 | Acc: 12.798% (2150/16800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 525/1563 Loss: 2.285 | Acc: 12.821% (2158/16832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 526/1563 Loss: 2.284 | Acc: 12.838% (2165/16864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 527/1563 Loss: 2.284 | Acc: 12.861% (2173/16896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 528/1563 Loss: 2.284 | Acc: 12.860% (2177/16928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 529/1563 Loss: 2.284 | Acc: 12.871% (2183/16960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 530/1563 Loss: 2.284 | Acc: 12.894% (2191/16992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 531/1563 Loss: 2.284 | Acc: 12.905% (2197/17024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 532/1563 Loss: 2.283 | Acc: 12.905% (2201/17056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 533/1563 Loss: 2.283 | Acc: 12.904% (2205/17088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 534/1563 Loss: 2.283 | Acc: 12.903% (2209/17120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 535/1563 Loss: 2.283 | Acc: 12.908% (2214/17152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 536/1563 Loss: 2.283 | Acc: 12.919% (2220/17184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 537/1563 Loss: 2.283 | Acc: 12.941% (2228/17216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 538/1563 Loss: 2.282 | Acc: 12.964% (2236/17248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 539/1563 Loss: 2.282 | Acc: 12.975% (2242/17280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 540/1563 Loss: 2.282 | Acc: 12.974% (2246/17312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 541/1563 Loss: 2.282 | Acc: 12.973% (2250/17344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 542/1563 Loss: 2.282 | Acc: 12.978% (2255/17376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 543/1563 Loss: 2.282 | Acc: 12.988% (2261/17408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 544/1563 Loss: 2.282 | Acc: 12.976% (2263/17440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 545/1563 Loss: 2.281 | Acc: 12.998% (2271/17472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 546/1563 Loss: 2.281 | Acc: 12.997% (2275/17504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 547/1563 Loss: 2.281 | Acc: 12.996% (2279/17536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 548/1563 Loss: 2.281 | Acc: 12.984% (2281/17568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 549/1563 Loss: 2.281 | Acc: 12.989% (2286/17600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 550/1563 Loss: 2.281 | Acc: 13.005% (2293/17632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 551/1563 Loss: 2.280 | Acc: 13.021% (2300/17664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 552/1563 Loss: 2.280 | Acc: 13.026% (2305/17696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 553/1563 Loss: 2.280 | Acc: 13.030% (2310/17728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 554/1563 Loss: 2.280 | Acc: 13.024% (2313/17760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 555/1563 Loss: 2.280 | Acc: 13.040% (2320/17792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 556/1563 Loss: 2.280 | Acc: 13.039% (2324/17824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 557/1563 Loss: 2.280 | Acc: 13.043% (2329/17856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 558/1563 Loss: 2.280 | Acc: 13.059% (2336/17888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 559/1563 Loss: 2.279 | Acc: 13.069% (2342/17920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 560/1563 Loss: 2.279 | Acc: 13.079% (2348/17952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 561/1563 Loss: 2.279 | Acc: 13.078% (2352/17984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 562/1563 Loss: 2.279 | Acc: 13.105% (2361/18016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 563/1563 Loss: 2.278 | Acc: 13.126% (2369/18048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 564/1563 Loss: 2.278 | Acc: 13.136% (2375/18080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 565/1563 Loss: 2.278 | Acc: 13.152% (2382/18112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 566/1563 Loss: 2.278 | Acc: 13.183% (2392/18144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 567/1563 Loss: 2.277 | Acc: 13.221% (2403/18176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 568/1563 Loss: 2.277 | Acc: 13.219% (2407/18208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 569/1563 Loss: 2.277 | Acc: 13.224% (2412/18240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 570/1563 Loss: 2.277 | Acc: 13.250% (2421/18272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 571/1563 Loss: 2.276 | Acc: 13.254% (2426/18304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 572/1563 Loss: 2.276 | Acc: 13.253% (2430/18336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 573/1563 Loss: 2.276 | Acc: 13.257% (2435/18368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 574/1563 Loss: 2.276 | Acc: 13.283% (2444/18400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 575/1563 Loss: 2.275 | Acc: 13.308% (2453/18432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 576/1563 Loss: 2.275 | Acc: 13.329% (2461/18464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 577/1563 Loss: 2.275 | Acc: 13.333% (2466/18496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 578/1563 Loss: 2.275 | Acc: 13.326% (2469/18528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 579/1563 Loss: 2.275 | Acc: 13.330% (2474/18560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 580/1563 Loss: 2.275 | Acc: 13.312% (2475/18592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 581/1563 Loss: 2.275 | Acc: 13.305% (2478/18624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 582/1563 Loss: 2.275 | Acc: 13.315% (2484/18656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 583/1563 Loss: 2.274 | Acc: 13.351% (2495/18688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 584/1563 Loss: 2.274 | Acc: 13.371% (2503/18720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 585/1563 Loss: 2.274 | Acc: 13.359% (2505/18752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 586/1563 Loss: 2.274 | Acc: 13.346% (2507/18784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 587/1563 Loss: 2.274 | Acc: 13.345% (2511/18816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 588/1563 Loss: 2.273 | Acc: 13.375% (2521/18848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 589/1563 Loss: 2.273 | Acc: 13.374% (2525/18880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 590/1563 Loss: 2.273 | Acc: 13.404% (2535/18912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 591/1563 Loss: 2.273 | Acc: 13.397% (2538/18944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 592/1563 Loss: 2.273 | Acc: 13.401% (2543/18976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 593/1563 Loss: 2.273 | Acc: 13.410% (2549/19008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 594/1563 Loss: 2.272 | Acc: 13.403% (2552/19040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 595/1563 Loss: 2.272 | Acc: 13.428% (2561/19072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 596/1563 Loss: 2.272 | Acc: 13.468% (2573/19104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 597/1563 Loss: 2.271 | Acc: 13.456% (2575/19136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 598/1563 Loss: 2.271 | Acc: 13.476% (2583/19168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 599/1563 Loss: 2.271 | Acc: 13.495% (2591/19200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 600/1563 Loss: 2.271 | Acc: 13.509% (2598/19232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 601/1563 Loss: 2.270 | Acc: 13.517% (2604/19264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 602/1563 Loss: 2.270 | Acc: 13.521% (2609/19296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 603/1563 Loss: 2.270 | Acc: 13.540% (2617/19328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 604/1563 Loss: 2.270 | Acc: 13.543% (2622/19360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 605/1563 Loss: 2.270 | Acc: 13.562% (2630/19392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 606/1563 Loss: 2.269 | Acc: 13.561% (2634/19424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 607/1563 Loss: 2.269 | Acc: 13.559% (2638/19456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 608/1563 Loss: 2.269 | Acc: 13.562% (2643/19488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 609/1563 Loss: 2.269 | Acc: 13.566% (2648/19520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 610/1563 Loss: 2.269 | Acc: 13.579% (2655/19552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 611/1563 Loss: 2.269 | Acc: 13.572% (2658/19584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 612/1563 Loss: 2.269 | Acc: 13.581% (2664/19616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 613/1563 Loss: 2.268 | Acc: 13.615% (2675/19648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 614/1563 Loss: 2.268 | Acc: 13.618% (2680/19680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 615/1563 Loss: 2.268 | Acc: 13.641% (2689/19712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 616/1563 Loss: 2.267 | Acc: 13.660% (2697/19744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 617/1563 Loss: 2.267 | Acc: 13.683% (2706/19776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 618/1563 Loss: 2.267 | Acc: 13.696% (2713/19808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 619/1563 Loss: 2.266 | Acc: 13.690% (2716/19840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 620/1563 Loss: 2.266 | Acc: 13.688% (2720/19872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 621/1563 Loss: 2.267 | Acc: 13.691% (2725/19904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 622/1563 Loss: 2.266 | Acc: 13.704% (2732/19936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 623/1563 Loss: 2.266 | Acc: 13.702% (2736/19968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 624/1563 Loss: 2.266 | Acc: 13.710% (2742/20000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 625/1563 Loss: 2.266 | Acc: 13.723% (2749/20032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 626/1563 Loss: 2.266 | Acc: 13.706% (2750/20064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 627/1563 Loss: 2.265 | Acc: 13.734% (2760/20096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 628/1563 Loss: 2.265 | Acc: 13.762% (2770/20128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 629/1563 Loss: 2.265 | Acc: 13.780% (2778/20160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 630/1563 Loss: 2.264 | Acc: 13.812% (2789/20192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 631/1563 Loss: 2.264 | Acc: 13.830% (2797/20224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 632/1563 Loss: 2.264 | Acc: 13.838% (2803/20256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 633/1563 Loss: 2.264 | Acc: 13.865% (2813/20288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 634/1563 Loss: 2.264 | Acc: 13.878% (2820/20320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 635/1563 Loss: 2.263 | Acc: 13.886% (2826/20352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 636/1563 Loss: 2.263 | Acc: 13.879% (2829/20384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 637/1563 Loss: 2.263 | Acc: 13.896% (2837/20416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 638/1563 Loss: 2.263 | Acc: 13.899% (2842/20448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 639/1563 Loss: 2.263 | Acc: 13.896% (2846/20480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 640/1563 Loss: 2.263 | Acc: 13.894% (2850/20512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 641/1563 Loss: 2.263 | Acc: 13.916% (2859/20544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 642/1563 Loss: 2.263 | Acc: 13.914% (2863/20576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 643/1563 Loss: 2.262 | Acc: 13.927% (2870/20608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 644/1563 Loss: 2.262 | Acc: 13.934% (2876/20640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 645/1563 Loss: 2.262 | Acc: 13.917% (2877/20672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 646/1563 Loss: 2.262 | Acc: 13.935% (2885/20704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 647/1563 Loss: 2.262 | Acc: 13.961% (2895/20736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 648/1563 Loss: 2.262 | Acc: 13.954% (2898/20768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 649/1563 Loss: 2.262 | Acc: 13.971% (2906/20800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 650/1563 Loss: 2.262 | Acc: 13.974% (2911/20832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 651/1563 Loss: 2.261 | Acc: 13.991% (2919/20864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 652/1563 Loss: 2.261 | Acc: 14.012% (2928/20896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 653/1563 Loss: 2.261 | Acc: 14.039% (2938/20928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 654/1563 Loss: 2.261 | Acc: 14.051% (2945/20960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 655/1563 Loss: 2.260 | Acc: 14.058% (2951/20992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 656/1563 Loss: 2.260 | Acc: 14.079% (2960/21024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 657/1563 Loss: 2.260 | Acc: 14.086% (2966/21056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 658/1563 Loss: 2.259 | Acc: 14.098% (2973/21088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 659/1563 Loss: 2.259 | Acc: 14.119% (2982/21120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 660/1563 Loss: 2.259 | Acc: 14.131% (2989/21152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 661/1563 Loss: 2.259 | Acc: 14.152% (2998/21184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 662/1563 Loss: 2.258 | Acc: 14.150% (3002/21216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 663/1563 Loss: 2.258 | Acc: 14.166% (3010/21248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 664/1563 Loss: 2.258 | Acc: 14.164% (3014/21280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 665/1563 Loss: 2.258 | Acc: 14.189% (3024/21312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 666/1563 Loss: 2.258 | Acc: 14.229% (3037/21344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 667/1563 Loss: 2.258 | Acc: 14.231% (3042/21376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 668/1563 Loss: 2.258 | Acc: 14.228% (3046/21408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 669/1563 Loss: 2.257 | Acc: 14.240% (3053/21440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 670/1563 Loss: 2.257 | Acc: 14.256% (3061/21472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 671/1563 Loss: 2.257 | Acc: 14.262% (3067/21504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 672/1563 Loss: 2.257 | Acc: 14.292% (3078/21536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 673/1563 Loss: 2.257 | Acc: 14.304% (3085/21568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 674/1563 Loss: 2.257 | Acc: 14.292% (3087/21600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 675/1563 Loss: 2.256 | Acc: 14.317% (3097/21632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 676/1563 Loss: 2.256 | Acc: 14.323% (3103/21664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 677/1563 Loss: 2.256 | Acc: 14.316% (3106/21696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 678/1563 Loss: 2.256 | Acc: 14.313% (3110/21728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 679/1563 Loss: 2.256 | Acc: 14.306% (3113/21760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 680/1563 Loss: 2.256 | Acc: 14.322% (3121/21792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 681/1563 Loss: 2.256 | Acc: 14.324% (3126/21824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 682/1563 Loss: 2.256 | Acc: 14.321% (3130/21856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 683/1563 Loss: 2.256 | Acc: 14.350% (3141/21888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 684/1563 Loss: 2.255 | Acc: 14.348% (3145/21920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 685/1563 Loss: 2.255 | Acc: 14.359% (3152/21952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 686/1563 Loss: 2.255 | Acc: 14.379% (3161/21984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 687/1563 Loss: 2.255 | Acc: 14.385% (3167/22016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 688/1563 Loss: 2.255 | Acc: 14.378% (3170/22048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 689/1563 Loss: 2.255 | Acc: 14.389% (3177/22080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 690/1563 Loss: 2.255 | Acc: 14.395% (3183/22112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 691/1563 Loss: 2.255 | Acc: 14.401% (3189/22144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 692/1563 Loss: 2.255 | Acc: 14.394% (3192/22176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 693/1563 Loss: 2.254 | Acc: 14.396% (3197/22208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 694/1563 Loss: 2.254 | Acc: 14.411% (3205/22240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 695/1563 Loss: 2.254 | Acc: 14.404% (3208/22272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 696/1563 Loss: 2.254 | Acc: 14.392% (3210/22304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 697/1563 Loss: 2.254 | Acc: 14.394% (3215/22336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 698/1563 Loss: 2.253 | Acc: 14.391% (3219/22368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 699/1563 Loss: 2.253 | Acc: 14.402% (3226/22400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 700/1563 Loss: 2.253 | Acc: 14.399% (3230/22432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 701/1563 Loss: 2.253 | Acc: 14.401% (3235/22464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 702/1563 Loss: 2.253 | Acc: 14.411% (3242/22496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 703/1563 Loss: 2.253 | Acc: 14.409% (3246/22528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 704/1563 Loss: 2.253 | Acc: 14.415% (3252/22560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 705/1563 Loss: 2.253 | Acc: 14.412% (3256/22592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 706/1563 Loss: 2.253 | Acc: 14.409% (3260/22624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 707/1563 Loss: 2.252 | Acc: 14.411% (3265/22656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 708/1563 Loss: 2.252 | Acc: 14.413% (3270/22688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 709/1563 Loss: 2.252 | Acc: 14.410% (3274/22720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 710/1563 Loss: 2.252 | Acc: 14.421% (3281/22752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 711/1563 Loss: 2.252 | Acc: 14.436% (3289/22784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 712/1563 Loss: 2.252 | Acc: 14.442% (3295/22816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 713/1563 Loss: 2.251 | Acc: 14.461% (3304/22848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 714/1563 Loss: 2.251 | Acc: 14.454% (3307/22880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 715/1563 Loss: 2.251 | Acc: 14.486% (3319/22912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 716/1563 Loss: 2.251 | Acc: 14.496% (3326/22944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 717/1563 Loss: 2.251 | Acc: 14.515% (3335/22976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 718/1563 Loss: 2.250 | Acc: 14.525% (3342/23008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 719/1563 Loss: 2.250 | Acc: 14.531% (3348/23040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 720/1563 Loss: 2.250 | Acc: 14.541% (3355/23072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 721/1563 Loss: 2.250 | Acc: 14.552% (3362/23104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 722/1563 Loss: 2.250 | Acc: 14.553% (3367/23136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 723/1563 Loss: 2.249 | Acc: 14.546% (3370/23168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 724/1563 Loss: 2.249 | Acc: 14.547% (3375/23200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 725/1563 Loss: 2.249 | Acc: 14.549% (3380/23232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 726/1563 Loss: 2.249 | Acc: 14.546% (3384/23264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 727/1563 Loss: 2.249 | Acc: 14.535% (3386/23296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 728/1563 Loss: 2.248 | Acc: 14.528% (3389/23328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 729/1563 Loss: 2.248 | Acc: 14.512% (3390/23360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 730/1563 Loss: 2.248 | Acc: 14.514% (3395/23392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 731/1563 Loss: 2.248 | Acc: 14.528% (3403/23424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 732/1563 Loss: 2.248 | Acc: 14.525% (3407/23456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 733/1563 Loss: 2.248 | Acc: 14.535% (3414/23488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 734/1563 Loss: 2.247 | Acc: 14.545% (3421/23520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 735/1563 Loss: 2.247 | Acc: 14.547% (3426/23552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 736/1563 Loss: 2.247 | Acc: 14.565% (3435/23584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 737/1563 Loss: 2.247 | Acc: 14.571% (3441/23616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 738/1563 Loss: 2.247 | Acc: 14.568% (3445/23648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 739/1563 Loss: 2.246 | Acc: 14.573% (3451/23680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 740/1563 Loss: 2.246 | Acc: 14.579% (3457/23712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 741/1563 Loss: 2.246 | Acc: 14.602% (3467/23744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 742/1563 Loss: 2.246 | Acc: 14.620% (3476/23776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 743/1563 Loss: 2.245 | Acc: 14.625% (3482/23808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 744/1563 Loss: 2.245 | Acc: 14.635% (3489/23840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 745/1563 Loss: 2.245 | Acc: 14.653% (3498/23872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 746/1563 Loss: 2.244 | Acc: 14.663% (3505/23904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 747/1563 Loss: 2.244 | Acc: 14.647% (3506/23936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 748/1563 Loss: 2.244 | Acc: 14.645% (3510/23968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 749/1563 Loss: 2.244 | Acc: 14.654% (3517/24000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 750/1563 Loss: 2.244 | Acc: 14.655% (3522/24032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 751/1563 Loss: 2.244 | Acc: 14.657% (3527/24064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 752/1563 Loss: 2.244 | Acc: 14.670% (3535/24096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 753/1563 Loss: 2.243 | Acc: 14.680% (3542/24128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 754/1563 Loss: 2.243 | Acc: 14.694% (3550/24160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 755/1563 Loss: 2.243 | Acc: 14.703% (3557/24192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 756/1563 Loss: 2.243 | Acc: 14.684% (3557/24224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 757/1563 Loss: 2.243 | Acc: 14.689% (3563/24256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 758/1563 Loss: 2.243 | Acc: 14.707% (3572/24288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 759/1563 Loss: 2.243 | Acc: 14.716% (3579/24320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 760/1563 Loss: 2.243 | Acc: 14.713% (3583/24352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 761/1563 Loss: 2.243 | Acc: 14.710% (3587/24384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 762/1563 Loss: 2.242 | Acc: 14.716% (3593/24416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 763/1563 Loss: 2.242 | Acc: 14.725% (3600/24448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 764/1563 Loss: 2.242 | Acc: 14.722% (3604/24480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 765/1563 Loss: 2.242 | Acc: 14.723% (3609/24512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 766/1563 Loss: 2.242 | Acc: 14.741% (3618/24544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 767/1563 Loss: 2.242 | Acc: 14.746% (3624/24576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 768/1563 Loss: 2.242 | Acc: 14.747% (3629/24608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 769/1563 Loss: 2.241 | Acc: 14.761% (3637/24640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 770/1563 Loss: 2.241 | Acc: 14.770% (3644/24672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 771/1563 Loss: 2.241 | Acc: 14.775% (3650/24704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 772/1563 Loss: 2.241 | Acc: 14.788% (3658/24736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 773/1563 Loss: 2.241 | Acc: 14.793% (3664/24768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 774/1563 Loss: 2.241 | Acc: 14.798% (3670/24800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 775/1563 Loss: 2.241 | Acc: 14.799% (3675/24832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 776/1563 Loss: 2.241 | Acc: 14.809% (3682/24864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 777/1563 Loss: 2.241 | Acc: 14.834% (3693/24896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 778/1563 Loss: 2.240 | Acc: 14.843% (3700/24928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 779/1563 Loss: 2.240 | Acc: 14.856% (3708/24960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 780/1563 Loss: 2.240 | Acc: 14.853% (3712/24992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 781/1563 Loss: 2.240 | Acc: 14.862% (3719/25024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 782/1563 Loss: 2.240 | Acc: 14.855% (3722/25056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 783/1563 Loss: 2.240 | Acc: 14.856% (3727/25088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 784/1563 Loss: 2.240 | Acc: 14.857% (3732/25120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 785/1563 Loss: 2.240 | Acc: 14.862% (3738/25152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 786/1563 Loss: 2.240 | Acc: 14.859% (3742/25184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 787/1563 Loss: 2.240 | Acc: 14.860% (3747/25216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 788/1563 Loss: 2.239 | Acc: 14.865% (3753/25248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 789/1563 Loss: 2.239 | Acc: 14.873% (3760/25280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 790/1563 Loss: 2.239 | Acc: 14.870% (3764/25312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 791/1563 Loss: 2.239 | Acc: 14.879% (3771/25344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 792/1563 Loss: 2.238 | Acc: 14.900% (3781/25376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 793/1563 Loss: 2.238 | Acc: 14.901% (3786/25408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 794/1563 Loss: 2.238 | Acc: 14.906% (3792/25440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 795/1563 Loss: 2.238 | Acc: 14.899% (3795/25472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 796/1563 Loss: 2.238 | Acc: 14.896% (3799/25504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 797/1563 Loss: 2.237 | Acc: 14.916% (3809/25536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 798/1563 Loss: 2.237 | Acc: 14.917% (3814/25568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 799/1563 Loss: 2.237 | Acc: 14.918% (3819/25600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 800/1563 Loss: 2.237 | Acc: 14.919% (3824/25632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 801/1563 Loss: 2.237 | Acc: 14.912% (3827/25664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 802/1563 Loss: 2.237 | Acc: 14.925% (3835/25696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 803/1563 Loss: 2.236 | Acc: 14.925% (3840/25728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 804/1563 Loss: 2.236 | Acc: 14.918% (3843/25760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 805/1563 Loss: 2.236 | Acc: 14.935% (3852/25792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 806/1563 Loss: 2.236 | Acc: 14.932% (3856/25824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 807/1563 Loss: 2.236 | Acc: 14.937% (3862/25856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 808/1563 Loss: 2.236 | Acc: 14.945% (3869/25888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 809/1563 Loss: 2.236 | Acc: 14.946% (3874/25920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 810/1563 Loss: 2.236 | Acc: 14.951% (3880/25952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 811/1563 Loss: 2.236 | Acc: 14.963% (3888/25984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 812/1563 Loss: 2.236 | Acc: 14.972% (3895/26016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 813/1563 Loss: 2.236 | Acc: 14.969% (3899/26048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 814/1563 Loss: 2.235 | Acc: 14.988% (3909/26080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 815/1563 Loss: 2.235 | Acc: 14.997% (3916/26112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 816/1563 Loss: 2.235 | Acc: 15.013% (3925/26144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 817/1563 Loss: 2.235 | Acc: 15.029% (3934/26176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 818/1563 Loss: 2.234 | Acc: 15.034% (3940/26208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 819/1563 Loss: 2.234 | Acc: 15.034% (3945/26240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 820/1563 Loss: 2.234 | Acc: 15.035% (3950/26272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 821/1563 Loss: 2.234 | Acc: 15.036% (3955/26304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 822/1563 Loss: 2.234 | Acc: 15.033% (3959/26336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 823/1563 Loss: 2.234 | Acc: 15.041% (3966/26368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 824/1563 Loss: 2.234 | Acc: 15.042% (3971/26400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 825/1563 Loss: 2.234 | Acc: 15.050% (3978/26432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 826/1563 Loss: 2.233 | Acc: 15.066% (3987/26464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 827/1563 Loss: 2.233 | Acc: 15.082% (3996/26496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 828/1563 Loss: 2.233 | Acc: 15.082% (4001/26528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 829/1563 Loss: 2.233 | Acc: 15.083% (4006/26560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 830/1563 Loss: 2.233 | Acc: 15.091% (4013/26592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 831/1563 Loss: 2.232 | Acc: 15.095% (4019/26624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 832/1563 Loss: 2.232 | Acc: 15.100% (4025/26656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 833/1563 Loss: 2.232 | Acc: 15.104% (4031/26688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 834/1563 Loss: 2.232 | Acc: 15.109% (4037/26720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 835/1563 Loss: 2.232 | Acc: 15.117% (4044/26752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 836/1563 Loss: 2.232 | Acc: 15.114% (4048/26784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 837/1563 Loss: 2.231 | Acc: 15.110% (4052/26816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 838/1563 Loss: 2.231 | Acc: 15.130% (4062/26848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 839/1563 Loss: 2.231 | Acc: 15.138% (4069/26880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 840/1563 Loss: 2.231 | Acc: 15.149% (4077/26912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 841/1563 Loss: 2.231 | Acc: 15.165% (4086/26944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 842/1563 Loss: 2.231 | Acc: 15.173% (4093/26976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 843/1563 Loss: 2.231 | Acc: 15.181% (4100/27008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 844/1563 Loss: 2.231 | Acc: 15.181% (4105/27040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 845/1563 Loss: 2.230 | Acc: 15.185% (4111/27072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 846/1563 Loss: 2.230 | Acc: 15.201% (4120/27104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 847/1563 Loss: 2.230 | Acc: 15.216% (4129/27136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 848/1563 Loss: 2.230 | Acc: 15.227% (4137/27168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 849/1563 Loss: 2.230 | Acc: 15.232% (4143/27200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 850/1563 Loss: 2.230 | Acc: 15.236% (4149/27232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 851/1563 Loss: 2.229 | Acc: 15.258% (4160/27264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 852/1563 Loss: 2.229 | Acc: 15.270% (4168/27296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 853/1563 Loss: 2.229 | Acc: 15.292% (4179/27328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 854/1563 Loss: 2.229 | Acc: 15.289% (4183/27360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 855/1563 Loss: 2.229 | Acc: 15.275% (4184/27392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 856/1563 Loss: 2.229 | Acc: 15.286% (4192/27424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 857/1563 Loss: 2.229 | Acc: 15.294% (4199/27456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 858/1563 Loss: 2.229 | Acc: 15.298% (4205/27488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 859/1563 Loss: 2.229 | Acc: 15.298% (4210/27520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 860/1563 Loss: 2.229 | Acc: 15.313% (4219/27552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 861/1563 Loss: 2.229 | Acc: 15.317% (4225/27584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 862/1563 Loss: 2.228 | Acc: 15.335% (4235/27616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 863/1563 Loss: 2.228 | Acc: 15.332% (4239/27648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 864/1563 Loss: 2.228 | Acc: 15.340% (4246/27680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 865/1563 Loss: 2.228 | Acc: 15.354% (4255/27712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 866/1563 Loss: 2.228 | Acc: 15.365% (4263/27744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 867/1563 Loss: 2.228 | Acc: 15.380% (4272/27776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 868/1563 Loss: 2.228 | Acc: 15.373% (4275/27808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 869/1563 Loss: 2.227 | Acc: 15.366% (4278/27840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 870/1563 Loss: 2.227 | Acc: 15.388% (4289/27872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 871/1563 Loss: 2.227 | Acc: 15.388% (4294/27904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 872/1563 Loss: 2.227 | Acc: 15.389% (4299/27936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 873/1563 Loss: 2.227 | Acc: 15.396% (4306/27968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 874/1563 Loss: 2.226 | Acc: 15.418% (4317/28000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 875/1563 Loss: 2.226 | Acc: 15.418% (4322/28032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 876/1563 Loss: 2.226 | Acc: 15.436% (4332/28064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 877/1563 Loss: 2.226 | Acc: 15.454% (4342/28096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 878/1563 Loss: 2.226 | Acc: 15.472% (4352/28128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 879/1563 Loss: 2.226 | Acc: 15.476% (4358/28160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 880/1563 Loss: 2.226 | Acc: 15.487% (4366/28192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 881/1563 Loss: 2.226 | Acc: 15.480% (4369/28224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 882/1563 Loss: 2.225 | Acc: 15.498% (4379/28256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 883/1563 Loss: 2.225 | Acc: 15.491% (4382/28288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 884/1563 Loss: 2.225 | Acc: 15.491% (4387/28320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 885/1563 Loss: 2.225 | Acc: 15.487% (4391/28352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 886/1563 Loss: 2.225 | Acc: 15.498% (4399/28384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 887/1563 Loss: 2.225 | Acc: 15.502% (4405/28416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 888/1563 Loss: 2.225 | Acc: 15.509% (4412/28448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 889/1563 Loss: 2.225 | Acc: 15.523% (4421/28480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 890/1563 Loss: 2.224 | Acc: 15.534% (4429/28512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 891/1563 Loss: 2.224 | Acc: 15.548% (4438/28544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 892/1563 Loss: 2.224 | Acc: 15.562% (4447/28576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 893/1563 Loss: 2.224 | Acc: 15.562% (4452/28608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 894/1563 Loss: 2.224 | Acc: 15.566% (4458/28640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 895/1563 Loss: 2.224 | Acc: 15.562% (4462/28672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 896/1563 Loss: 2.224 | Acc: 15.562% (4467/28704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 897/1563 Loss: 2.224 | Acc: 15.576% (4476/28736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 898/1563 Loss: 2.224 | Acc: 15.580% (4482/28768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 899/1563 Loss: 2.224 | Acc: 15.580% (4487/28800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 900/1563 Loss: 2.224 | Acc: 15.587% (4494/28832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 901/1563 Loss: 2.223 | Acc: 15.597% (4502/28864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 902/1563 Loss: 2.223 | Acc: 15.608% (4510/28896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 903/1563 Loss: 2.223 | Acc: 15.611% (4516/28928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 904/1563 Loss: 2.223 | Acc: 15.625% (4525/28960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 905/1563 Loss: 2.223 | Acc: 15.622% (4529/28992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 906/1563 Loss: 2.222 | Acc: 15.622% (4534/29024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 907/1563 Loss: 2.222 | Acc: 15.618% (4538/29056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 908/1563 Loss: 2.222 | Acc: 15.618% (4543/29088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 909/1563 Loss: 2.222 | Acc: 15.625% (4550/29120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 910/1563 Loss: 2.222 | Acc: 15.625% (4555/29152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 911/1563 Loss: 2.222 | Acc: 15.635% (4563/29184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 912/1563 Loss: 2.222 | Acc: 15.635% (4568/29216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 913/1563 Loss: 2.222 | Acc: 15.639% (4574/29248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 914/1563 Loss: 2.221 | Acc: 15.632% (4577/29280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 915/1563 Loss: 2.221 | Acc: 15.632% (4582/29312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 916/1563 Loss: 2.221 | Acc: 15.622% (4584/29344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 917/1563 Loss: 2.221 | Acc: 15.632% (4592/29376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 918/1563 Loss: 2.221 | Acc: 15.632% (4597/29408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 919/1563 Loss: 2.221 | Acc: 15.642% (4605/29440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 920/1563 Loss: 2.221 | Acc: 15.659% (4615/29472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 921/1563 Loss: 2.221 | Acc: 15.662% (4621/29504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 922/1563 Loss: 2.221 | Acc: 15.676% (4630/29536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 923/1563 Loss: 2.221 | Acc: 15.676% (4635/29568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 924/1563 Loss: 2.220 | Acc: 15.682% (4642/29600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 925/1563 Loss: 2.220 | Acc: 15.689% (4649/29632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 926/1563 Loss: 2.220 | Acc: 15.689% (4654/29664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 927/1563 Loss: 2.220 | Acc: 15.702% (4663/29696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 928/1563 Loss: 2.220 | Acc: 15.702% (4668/29728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 929/1563 Loss: 2.220 | Acc: 15.716% (4677/29760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 930/1563 Loss: 2.220 | Acc: 15.719% (4683/29792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 931/1563 Loss: 2.220 | Acc: 15.729% (4691/29824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 932/1563 Loss: 2.220 | Acc: 15.725% (4695/29856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 933/1563 Loss: 2.220 | Acc: 15.719% (4698/29888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 934/1563 Loss: 2.219 | Acc: 15.725% (4705/29920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 935/1563 Loss: 2.219 | Acc: 15.735% (4713/29952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 936/1563 Loss: 2.219 | Acc: 15.748% (4722/29984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 937/1563 Loss: 2.219 | Acc: 15.745% (4726/30016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 938/1563 Loss: 2.219 | Acc: 15.738% (4729/30048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 939/1563 Loss: 2.219 | Acc: 15.755% (4739/30080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 940/1563 Loss: 2.219 | Acc: 15.768% (4748/30112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 941/1563 Loss: 2.219 | Acc: 15.778% (4756/30144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 942/1563 Loss: 2.219 | Acc: 15.791% (4765/30176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 943/1563 Loss: 2.219 | Acc: 15.797% (4772/30208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 944/1563 Loss: 2.219 | Acc: 15.794% (4776/30240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 945/1563 Loss: 2.219 | Acc: 15.800% (4783/30272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 946/1563 Loss: 2.219 | Acc: 15.797% (4787/30304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 947/1563 Loss: 2.219 | Acc: 15.813% (4797/30336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 948/1563 Loss: 2.219 | Acc: 15.819% (4804/30368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 949/1563 Loss: 2.218 | Acc: 15.832% (4813/30400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 950/1563 Loss: 2.218 | Acc: 15.825% (4816/30432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 951/1563 Loss: 2.218 | Acc: 15.822% (4820/30464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 952/1563 Loss: 2.218 | Acc: 15.825% (4826/30496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 953/1563 Loss: 2.218 | Acc: 15.831% (4833/30528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 954/1563 Loss: 2.218 | Acc: 15.838% (4840/30560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 955/1563 Loss: 2.218 | Acc: 15.847% (4848/30592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 956/1563 Loss: 2.217 | Acc: 15.847% (4853/30624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 957/1563 Loss: 2.217 | Acc: 15.847% (4858/30656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 958/1563 Loss: 2.217 | Acc: 15.840% (4861/30688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 959/1563 Loss: 2.217 | Acc: 15.856% (4871/30720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 960/1563 Loss: 2.217 | Acc: 15.866% (4879/30752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 961/1563 Loss: 2.217 | Acc: 15.869% (4885/30784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 962/1563 Loss: 2.217 | Acc: 15.878% (4893/30816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 963/1563 Loss: 2.217 | Acc: 15.871% (4896/30848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 964/1563 Loss: 2.217 | Acc: 15.874% (4902/30880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 965/1563 Loss: 2.217 | Acc: 15.871% (4906/30912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 966/1563 Loss: 2.217 | Acc: 15.874% (4912/30944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 967/1563 Loss: 2.217 | Acc: 15.877% (4918/30976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 968/1563 Loss: 2.216 | Acc: 15.886% (4926/31008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 969/1563 Loss: 2.216 | Acc: 15.883% (4930/31040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 970/1563 Loss: 2.216 | Acc: 15.886% (4936/31072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 971/1563 Loss: 2.216 | Acc: 15.892% (4943/31104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 972/1563 Loss: 2.216 | Acc: 15.892% (4948/31136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 973/1563 Loss: 2.216 | Acc: 15.885% (4951/31168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 974/1563 Loss: 2.216 | Acc: 15.913% (4965/31200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 975/1563 Loss: 2.216 | Acc: 15.913% (4970/31232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 976/1563 Loss: 2.215 | Acc: 15.913% (4975/31264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 977/1563 Loss: 2.215 | Acc: 15.925% (4984/31296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 978/1563 Loss: 2.215 | Acc: 15.922% (4988/31328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 979/1563 Loss: 2.215 | Acc: 15.925% (4994/31360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 980/1563 Loss: 2.215 | Acc: 15.918% (4997/31392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 981/1563 Loss: 2.215 | Acc: 15.921% (5003/31424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 982/1563 Loss: 2.215 | Acc: 15.927% (5010/31456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 983/1563 Loss: 2.215 | Acc: 15.936% (5018/31488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 984/1563 Loss: 2.215 | Acc: 15.949% (5027/31520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 985/1563 Loss: 2.215 | Acc: 15.951% (5033/31552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 986/1563 Loss: 2.215 | Acc: 15.970% (5044/31584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 987/1563 Loss: 2.215 | Acc: 15.973% (5050/31616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 988/1563 Loss: 2.214 | Acc: 15.985% (5059/31648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 989/1563 Loss: 2.214 | Acc: 15.994% (5067/31680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 990/1563 Loss: 2.214 | Acc: 15.991% (5071/31712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 991/1563 Loss: 2.214 | Acc: 15.990% (5076/31744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 992/1563 Loss: 2.214 | Acc: 15.996% (5083/31776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 993/1563 Loss: 2.214 | Acc: 15.999% (5089/31808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 994/1563 Loss: 2.214 | Acc: 16.018% (5100/31840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 995/1563 Loss: 2.214 | Acc: 16.033% (5110/31872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 996/1563 Loss: 2.214 | Acc: 16.042% (5118/31904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 997/1563 Loss: 2.214 | Acc: 16.041% (5123/31936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 998/1563 Loss: 2.214 | Acc: 16.038% (5127/31968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 999/1563 Loss: 2.213 | Acc: 16.075% (5144/32000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1000/1563 Loss: 2.213 | Acc: 16.093% (5155/32032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1001/1563 Loss: 2.213 | Acc: 16.102% (5163/32064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1002/1563 Loss: 2.213 | Acc: 16.111% (5171/32096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1003/1563 Loss: 2.213 | Acc: 16.114% (5177/32128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1004/1563 Loss: 2.212 | Acc: 16.135% (5189/32160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1005/1563 Loss: 2.212 | Acc: 16.141% (5196/32192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1006/1563 Loss: 2.212 | Acc: 16.149% (5204/32224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1007/1563 Loss: 2.212 | Acc: 16.146% (5208/32256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1008/1563 Loss: 2.212 | Acc: 16.142% (5212/32288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1009/1563 Loss: 2.212 | Acc: 16.154% (5221/32320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1010/1563 Loss: 2.212 | Acc: 16.154% (5226/32352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1011/1563 Loss: 2.212 | Acc: 16.159% (5233/32384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1012/1563 Loss: 2.212 | Acc: 16.177% (5244/32416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1013/1563 Loss: 2.212 | Acc: 16.189% (5253/32448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1014/1563 Loss: 2.212 | Acc: 16.192% (5259/32480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1015/1563 Loss: 2.211 | Acc: 16.194% (5265/32512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1016/1563 Loss: 2.211 | Acc: 16.197% (5271/32544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1017/1563 Loss: 2.211 | Acc: 16.199% (5277/32576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1018/1563 Loss: 2.211 | Acc: 16.198% (5282/32608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1019/1563 Loss: 2.211 | Acc: 16.201% (5288/32640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1020/1563 Loss: 2.211 | Acc: 16.216% (5298/32672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1021/1563 Loss: 2.211 | Acc: 16.209% (5301/32704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1022/1563 Loss: 2.211 | Acc: 16.215% (5308/32736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1023/1563 Loss: 2.210 | Acc: 16.229% (5318/32768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1024/1563 Loss: 2.210 | Acc: 16.232% (5324/32800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1025/1563 Loss: 2.210 | Acc: 16.243% (5333/32832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1026/1563 Loss: 2.210 | Acc: 16.237% (5336/32864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1027/1563 Loss: 2.210 | Acc: 16.251% (5346/32896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1028/1563 Loss: 2.210 | Acc: 16.248% (5350/32928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1029/1563 Loss: 2.210 | Acc: 16.250% (5356/32960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1030/1563 Loss: 2.210 | Acc: 16.262% (5365/32992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1031/1563 Loss: 2.209 | Acc: 16.279% (5376/33024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1032/1563 Loss: 2.209 | Acc: 16.275% (5380/33056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1033/1563 Loss: 2.209 | Acc: 16.278% (5386/33088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1034/1563 Loss: 2.209 | Acc: 16.277% (5391/33120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1035/1563 Loss: 2.209 | Acc: 16.271% (5394/33152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1036/1563 Loss: 2.209 | Acc: 16.276% (5401/33184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1037/1563 Loss: 2.209 | Acc: 16.281% (5408/33216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1038/1563 Loss: 2.209 | Acc: 16.290% (5416/33248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1039/1563 Loss: 2.209 | Acc: 16.292% (5422/33280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1040/1563 Loss: 2.209 | Acc: 16.309% (5433/33312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1041/1563 Loss: 2.209 | Acc: 16.306% (5437/33344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1042/1563 Loss: 2.209 | Acc: 16.314% (5445/33376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1043/1563 Loss: 2.208 | Acc: 16.325% (5454/33408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1044/1563 Loss: 2.209 | Acc: 16.331% (5461/33440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1045/1563 Loss: 2.209 | Acc: 16.336% (5468/33472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1046/1563 Loss: 2.209 | Acc: 16.338% (5474/33504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1047/1563 Loss: 2.208 | Acc: 16.344% (5481/33536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1048/1563 Loss: 2.208 | Acc: 16.349% (5488/33568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1049/1563 Loss: 2.208 | Acc: 16.357% (5496/33600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1050/1563 Loss: 2.208 | Acc: 16.350% (5499/33632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1051/1563 Loss: 2.208 | Acc: 16.344% (5502/33664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1052/1563 Loss: 2.208 | Acc: 16.343% (5507/33696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1053/1563 Loss: 2.208 | Acc: 16.348% (5514/33728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1054/1563 Loss: 2.208 | Acc: 16.351% (5520/33760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1055/1563 Loss: 2.207 | Acc: 16.371% (5532/33792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1056/1563 Loss: 2.207 | Acc: 16.385% (5542/33824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1057/1563 Loss: 2.207 | Acc: 16.393% (5550/33856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1058/1563 Loss: 2.207 | Acc: 16.389% (5554/33888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1059/1563 Loss: 2.207 | Acc: 16.400% (5563/33920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1060/1563 Loss: 2.206 | Acc: 16.403% (5569/33952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1061/1563 Loss: 2.206 | Acc: 16.414% (5578/33984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1062/1563 Loss: 2.206 | Acc: 16.425% (5587/34016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1063/1563 Loss: 2.206 | Acc: 16.421% (5591/34048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1064/1563 Loss: 2.206 | Acc: 16.426% (5598/34080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1065/1563 Loss: 2.206 | Acc: 16.434% (5606/34112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1066/1563 Loss: 2.206 | Acc: 16.448% (5616/34144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1067/1563 Loss: 2.206 | Acc: 16.438% (5618/34176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1068/1563 Loss: 2.206 | Acc: 16.449% (5627/34208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1069/1563 Loss: 2.206 | Acc: 16.463% (5637/34240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1070/1563 Loss: 2.206 | Acc: 16.465% (5643/34272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1071/1563 Loss: 2.206 | Acc: 16.470% (5650/34304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1072/1563 Loss: 2.206 | Acc: 16.484% (5660/34336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1073/1563 Loss: 2.206 | Acc: 16.475% (5662/34368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1074/1563 Loss: 2.205 | Acc: 16.480% (5669/34400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1075/1563 Loss: 2.205 | Acc: 16.493% (5679/34432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1076/1563 Loss: 2.205 | Acc: 16.510% (5690/34464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1077/1563 Loss: 2.205 | Acc: 16.527% (5701/34496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1078/1563 Loss: 2.205 | Acc: 16.520% (5704/34528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1079/1563 Loss: 2.205 | Acc: 16.519% (5709/34560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1080/1563 Loss: 2.205 | Acc: 16.518% (5714/34592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1081/1563 Loss: 2.205 | Acc: 16.515% (5718/34624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1082/1563 Loss: 2.205 | Acc: 16.517% (5724/34656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1083/1563 Loss: 2.205 | Acc: 16.522% (5731/34688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1084/1563 Loss: 2.204 | Acc: 16.518% (5735/34720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1085/1563 Loss: 2.204 | Acc: 16.520% (5741/34752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1086/1563 Loss: 2.204 | Acc: 16.531% (5750/34784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1087/1563 Loss: 2.204 | Acc: 16.541% (5759/34816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1088/1563 Loss: 2.204 | Acc: 16.543% (5765/34848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1089/1563 Loss: 2.204 | Acc: 16.545% (5771/34880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1090/1563 Loss: 2.203 | Acc: 16.550% (5778/34912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1091/1563 Loss: 2.203 | Acc: 16.558% (5786/34944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1092/1563 Loss: 2.203 | Acc: 16.569% (5795/34976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1093/1563 Loss: 2.203 | Acc: 16.596% (5810/35008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1094/1563 Loss: 2.203 | Acc: 16.598% (5816/35040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1095/1563 Loss: 2.203 | Acc: 16.600% (5822/35072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1096/1563 Loss: 2.203 | Acc: 16.611% (5831/35104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1097/1563 Loss: 2.203 | Acc: 16.621% (5840/35136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1098/1563 Loss: 2.202 | Acc: 16.632% (5849/35168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1099/1563 Loss: 2.202 | Acc: 16.639% (5857/35200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1100/1563 Loss: 2.202 | Acc: 16.647% (5865/35232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1101/1563 Loss: 2.202 | Acc: 16.674% (5880/35264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1102/1563 Loss: 2.202 | Acc: 16.673% (5885/35296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1103/1563 Loss: 2.202 | Acc: 16.675% (5891/35328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1104/1563 Loss: 2.202 | Acc: 16.674% (5896/35360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1105/1563 Loss: 2.202 | Acc: 16.676% (5902/35392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1106/1563 Loss: 2.202 | Acc: 16.669% (5905/35424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1107/1563 Loss: 2.202 | Acc: 16.666% (5909/35456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1108/1563 Loss: 2.201 | Acc: 16.670% (5916/35488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1109/1563 Loss: 2.201 | Acc: 16.667% (5920/35520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1110/1563 Loss: 2.201 | Acc: 16.669% (5926/35552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1111/1563 Loss: 2.201 | Acc: 16.676% (5934/35584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1112/1563 Loss: 2.201 | Acc: 16.698% (5947/35616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1113/1563 Loss: 2.201 | Acc: 16.694% (5951/35648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1114/1563 Loss: 2.201 | Acc: 16.704% (5960/35680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1115/1563 Loss: 2.200 | Acc: 16.717% (5970/35712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1116/1563 Loss: 2.200 | Acc: 16.738% (5983/35744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1117/1563 Loss: 2.200 | Acc: 16.743% (5990/35776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1118/1563 Loss: 2.200 | Acc: 16.742% (5995/35808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1119/1563 Loss: 2.200 | Acc: 16.741% (6000/35840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1120/1563 Loss: 2.200 | Acc: 16.746% (6007/35872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1121/1563 Loss: 2.200 | Acc: 16.764% (6019/35904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1122/1563 Loss: 2.200 | Acc: 16.763% (6024/35936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1123/1563 Loss: 2.200 | Acc: 16.776% (6034/35968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1124/1563 Loss: 2.200 | Acc: 16.775% (6039/36000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1125/1563 Loss: 2.200 | Acc: 16.774% (6044/36032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1126/1563 Loss: 2.200 | Acc: 16.773% (6049/36064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1127/1563 Loss: 2.200 | Acc: 16.775% (6055/36096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1128/1563 Loss: 2.200 | Acc: 16.788% (6065/36128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1129/1563 Loss: 2.199 | Acc: 16.792% (6072/36160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1130/1563 Loss: 2.199 | Acc: 16.794% (6078/36192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1131/1563 Loss: 2.199 | Acc: 16.804% (6087/36224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1132/1563 Loss: 2.199 | Acc: 16.805% (6093/36256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1133/1563 Loss: 2.199 | Acc: 16.810% (6100/36288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1134/1563 Loss: 2.199 | Acc: 16.806% (6104/36320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1135/1563 Loss: 2.199 | Acc: 16.811% (6111/36352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1136/1563 Loss: 2.198 | Acc: 16.815% (6118/36384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1137/1563 Loss: 2.198 | Acc: 16.820% (6125/36416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1138/1563 Loss: 2.198 | Acc: 16.827% (6133/36448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1139/1563 Loss: 2.198 | Acc: 16.831% (6140/36480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1140/1563 Loss: 2.198 | Acc: 16.836% (6147/36512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1141/1563 Loss: 2.198 | Acc: 16.840% (6154/36544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1142/1563 Loss: 2.198 | Acc: 16.839% (6159/36576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1143/1563 Loss: 2.198 | Acc: 16.838% (6164/36608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1144/1563 Loss: 2.198 | Acc: 16.848% (6173/36640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1145/1563 Loss: 2.198 | Acc: 16.847% (6178/36672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1146/1563 Loss: 2.197 | Acc: 16.848% (6184/36704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1147/1563 Loss: 2.197 | Acc: 16.845% (6188/36736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1148/1563 Loss: 2.197 | Acc: 16.846% (6194/36768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1149/1563 Loss: 2.197 | Acc: 16.856% (6203/36800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1150/1563 Loss: 2.197 | Acc: 16.863% (6211/36832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1151/1563 Loss: 2.197 | Acc: 16.873% (6220/36864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1152/1563 Loss: 2.197 | Acc: 16.885% (6230/36896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1153/1563 Loss: 2.196 | Acc: 16.895% (6239/36928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1154/1563 Loss: 2.196 | Acc: 16.899% (6246/36960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1155/1563 Loss: 2.196 | Acc: 16.906% (6254/36992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1156/1563 Loss: 2.196 | Acc: 16.913% (6262/37024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1157/1563 Loss: 2.196 | Acc: 16.920% (6270/37056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1158/1563 Loss: 2.196 | Acc: 16.930% (6279/37088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1159/1563 Loss: 2.196 | Acc: 16.934% (6286/37120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1160/1563 Loss: 2.196 | Acc: 16.928% (6289/37152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1161/1563 Loss: 2.196 | Acc: 16.932% (6296/37184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1162/1563 Loss: 2.196 | Acc: 16.939% (6304/37216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1163/1563 Loss: 2.195 | Acc: 16.946% (6312/37248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1164/1563 Loss: 2.195 | Acc: 16.947% (6318/37280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1165/1563 Loss: 2.195 | Acc: 16.957% (6327/37312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1166/1563 Loss: 2.195 | Acc: 16.959% (6333/37344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1167/1563 Loss: 2.195 | Acc: 16.965% (6341/37376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1168/1563 Loss: 2.195 | Acc: 16.964% (6346/37408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1169/1563 Loss: 2.195 | Acc: 16.968% (6353/37440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1170/1563 Loss: 2.194 | Acc: 16.973% (6360/37472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1171/1563 Loss: 2.194 | Acc: 16.982% (6369/37504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1172/1563 Loss: 2.194 | Acc: 16.989% (6377/37536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1173/1563 Loss: 2.194 | Acc: 16.993% (6384/37568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1174/1563 Loss: 2.194 | Acc: 16.997% (6391/37600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1175/1563 Loss: 2.194 | Acc: 17.004% (6399/37632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1176/1563 Loss: 2.194 | Acc: 17.006% (6405/37664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1177/1563 Loss: 2.194 | Acc: 17.018% (6415/37696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1178/1563 Loss: 2.194 | Acc: 17.019% (6421/37728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1179/1563 Loss: 2.194 | Acc: 17.029% (6430/37760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1180/1563 Loss: 2.194 | Acc: 17.025% (6434/37792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1181/1563 Loss: 2.193 | Acc: 17.032% (6442/37824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1182/1563 Loss: 2.193 | Acc: 17.041% (6451/37856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1183/1563 Loss: 2.193 | Acc: 17.040% (6456/37888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1184/1563 Loss: 2.193 | Acc: 17.041% (6462/37920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1185/1563 Loss: 2.193 | Acc: 17.048% (6470/37952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1186/1563 Loss: 2.193 | Acc: 17.055% (6478/37984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1187/1563 Loss: 2.193 | Acc: 17.059% (6485/38016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1188/1563 Loss: 2.193 | Acc: 17.047% (6486/38048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1189/1563 Loss: 2.193 | Acc: 17.051% (6493/38080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1190/1563 Loss: 2.193 | Acc: 17.052% (6499/38112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1191/1563 Loss: 2.193 | Acc: 17.056% (6506/38144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1192/1563 Loss: 2.193 | Acc: 17.066% (6515/38176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1193/1563 Loss: 2.192 | Acc: 17.072% (6523/38208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1194/1563 Loss: 2.192 | Acc: 17.071% (6528/38240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1195/1563 Loss: 2.192 | Acc: 17.073% (6534/38272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1196/1563 Loss: 2.192 | Acc: 17.074% (6540/38304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1197/1563 Loss: 2.192 | Acc: 17.070% (6544/38336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1198/1563 Loss: 2.192 | Acc: 17.079% (6553/38368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1199/1563 Loss: 2.192 | Acc: 17.083% (6560/38400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1200/1563 Loss: 2.192 | Acc: 17.090% (6568/38432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1201/1563 Loss: 2.192 | Acc: 17.094% (6575/38464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1202/1563 Loss: 2.192 | Acc: 17.095% (6581/38496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1203/1563 Loss: 2.192 | Acc: 17.089% (6584/38528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1204/1563 Loss: 2.191 | Acc: 17.093% (6591/38560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1205/1563 Loss: 2.191 | Acc: 17.105% (6601/38592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1206/1563 Loss: 2.191 | Acc: 17.106% (6607/38624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1207/1563 Loss: 2.192 | Acc: 17.102% (6611/38656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1208/1563 Loss: 2.191 | Acc: 17.122% (6624/38688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1209/1563 Loss: 2.191 | Acc: 17.133% (6634/38720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1210/1563 Loss: 2.191 | Acc: 17.145% (6644/38752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1211/1563 Loss: 2.191 | Acc: 17.146% (6650/38784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1212/1563 Loss: 2.191 | Acc: 17.150% (6657/38816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1213/1563 Loss: 2.191 | Acc: 17.159% (6666/38848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1214/1563 Loss: 2.191 | Acc: 17.163% (6673/38880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1215/1563 Loss: 2.191 | Acc: 17.157% (6676/38912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1216/1563 Loss: 2.191 | Acc: 17.168% (6686/38944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1217/1563 Loss: 2.191 | Acc: 17.182% (6697/38976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1218/1563 Loss: 2.191 | Acc: 17.196% (6708/39008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1219/1563 Loss: 2.191 | Acc: 17.193% (6712/39040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1220/1563 Loss: 2.191 | Acc: 17.194% (6718/39072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1221/1563 Loss: 2.190 | Acc: 17.193% (6723/39104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1222/1563 Loss: 2.190 | Acc: 17.194% (6729/39136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1223/1563 Loss: 2.190 | Acc: 17.210% (6741/39168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1224/1563 Loss: 2.190 | Acc: 17.209% (6746/39200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1225/1563 Loss: 2.190 | Acc: 17.208% (6751/39232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1226/1563 Loss: 2.190 | Acc: 17.217% (6760/39264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1227/1563 Loss: 2.190 | Acc: 17.223% (6768/39296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1228/1563 Loss: 2.190 | Acc: 17.224% (6774/39328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1229/1563 Loss: 2.190 | Acc: 17.226% (6780/39360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1230/1563 Loss: 2.190 | Acc: 17.240% (6791/39392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1231/1563 Loss: 2.189 | Acc: 17.246% (6799/39424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1232/1563 Loss: 2.189 | Acc: 17.245% (6804/39456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1233/1563 Loss: 2.189 | Acc: 17.248% (6811/39488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1234/1563 Loss: 2.189 | Acc: 17.260% (6821/39520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1235/1563 Loss: 2.189 | Acc: 17.268% (6830/39552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1236/1563 Loss: 2.189 | Acc: 17.272% (6837/39584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1237/1563 Loss: 2.189 | Acc: 17.276% (6844/39616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1238/1563 Loss: 2.189 | Acc: 17.287% (6854/39648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1239/1563 Loss: 2.188 | Acc: 17.298% (6864/39680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1240/1563 Loss: 2.188 | Acc: 17.295% (6868/39712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1241/1563 Loss: 2.188 | Acc: 17.293% (6873/39744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1242/1563 Loss: 2.188 | Acc: 17.299% (6881/39776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1243/1563 Loss: 2.188 | Acc: 17.311% (6891/39808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1244/1563 Loss: 2.188 | Acc: 17.324% (6902/39840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1245/1563 Loss: 2.188 | Acc: 17.325% (6908/39872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1246/1563 Loss: 2.188 | Acc: 17.324% (6913/39904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1247/1563 Loss: 2.188 | Acc: 17.323% (6918/39936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1248/1563 Loss: 2.188 | Acc: 17.321% (6923/39968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1249/1563 Loss: 2.187 | Acc: 17.323% (6929/40000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1250/1563 Loss: 2.187 | Acc: 17.331% (6938/40032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1251/1563 Loss: 2.187 | Acc: 17.337% (6946/40064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1252/1563 Loss: 2.187 | Acc: 17.348% (6956/40096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1253/1563 Loss: 2.187 | Acc: 17.349% (6962/40128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1254/1563 Loss: 2.186 | Acc: 17.358% (6971/40160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1255/1563 Loss: 2.186 | Acc: 17.357% (6976/40192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1256/1563 Loss: 2.186 | Acc: 17.358% (6982/40224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1257/1563 Loss: 2.186 | Acc: 17.366% (6991/40256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1258/1563 Loss: 2.186 | Acc: 17.367% (6997/40288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1259/1563 Loss: 2.186 | Acc: 17.374% (7005/40320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1260/1563 Loss: 2.186 | Acc: 17.375% (7011/40352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1261/1563 Loss: 2.186 | Acc: 17.386% (7021/40384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1262/1563 Loss: 2.186 | Acc: 17.389% (7028/40416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1263/1563 Loss: 2.186 | Acc: 17.388% (7033/40448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1264/1563 Loss: 2.186 | Acc: 17.386% (7038/40480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1265/1563 Loss: 2.186 | Acc: 17.397% (7048/40512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1266/1563 Loss: 2.186 | Acc: 17.403% (7056/40544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1267/1563 Loss: 2.185 | Acc: 17.404% (7062/40576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1268/1563 Loss: 2.186 | Acc: 17.408% (7069/40608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1269/1563 Loss: 2.186 | Acc: 17.411% (7076/40640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1270/1563 Loss: 2.186 | Acc: 17.415% (7083/40672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1271/1563 Loss: 2.185 | Acc: 17.416% (7089/40704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1272/1563 Loss: 2.185 | Acc: 17.424% (7098/40736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1273/1563 Loss: 2.185 | Acc: 17.433% (7107/40768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1274/1563 Loss: 2.185 | Acc: 17.444% (7117/40800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1275/1563 Loss: 2.185 | Acc: 17.447% (7124/40832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1276/1563 Loss: 2.185 | Acc: 17.455% (7133/40864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1277/1563 Loss: 2.185 | Acc: 17.454% (7138/40896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1278/1563 Loss: 2.185 | Acc: 17.460% (7146/40928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1279/1563 Loss: 2.185 | Acc: 17.471% (7156/40960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1280/1563 Loss: 2.185 | Acc: 17.469% (7161/40992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1281/1563 Loss: 2.184 | Acc: 17.478% (7170/41024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1282/1563 Loss: 2.184 | Acc: 17.486% (7179/41056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1283/1563 Loss: 2.184 | Acc: 17.487% (7185/41088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1284/1563 Loss: 2.184 | Acc: 17.483% (7189/41120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1285/1563 Loss: 2.184 | Acc: 17.489% (7197/41152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1286/1563 Loss: 2.184 | Acc: 17.490% (7203/41184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1287/1563 Loss: 2.184 | Acc: 17.488% (7208/41216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1288/1563 Loss: 2.184 | Acc: 17.492% (7215/41248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1289/1563 Loss: 2.184 | Acc: 17.493% (7221/41280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1290/1563 Loss: 2.184 | Acc: 17.491% (7226/41312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1291/1563 Loss: 2.184 | Acc: 17.500% (7235/41344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1292/1563 Loss: 2.183 | Acc: 17.505% (7243/41376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1293/1563 Loss: 2.183 | Acc: 17.514% (7252/41408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1294/1563 Loss: 2.183 | Acc: 17.519% (7260/41440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1295/1563 Loss: 2.183 | Acc: 17.532% (7271/41472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1296/1563 Loss: 2.183 | Acc: 17.536% (7278/41504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1297/1563 Loss: 2.183 | Acc: 17.539% (7285/41536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1298/1563 Loss: 2.183 | Acc: 17.540% (7291/41568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1299/1563 Loss: 2.183 | Acc: 17.548% (7300/41600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1300/1563 Loss: 2.183 | Acc: 17.547% (7305/41632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1301/1563 Loss: 2.183 | Acc: 17.552% (7313/41664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1302/1563 Loss: 2.183 | Acc: 17.553% (7319/41696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1303/1563 Loss: 2.183 | Acc: 17.545% (7321/41728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1304/1563 Loss: 2.183 | Acc: 17.538% (7324/41760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1305/1563 Loss: 2.183 | Acc: 17.537% (7329/41792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1306/1563 Loss: 2.183 | Acc: 17.538% (7335/41824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1307/1563 Loss: 2.182 | Acc: 17.548% (7345/41856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1308/1563 Loss: 2.182 | Acc: 17.547% (7350/41888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1309/1563 Loss: 2.182 | Acc: 17.548% (7356/41920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1310/1563 Loss: 2.182 | Acc: 17.553% (7364/41952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1311/1563 Loss: 2.182 | Acc: 17.566% (7375/41984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1312/1563 Loss: 2.182 | Acc: 17.574% (7384/42016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1313/1563 Loss: 2.182 | Acc: 17.582% (7393/42048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1314/1563 Loss: 2.182 | Acc: 17.586% (7400/42080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1315/1563 Loss: 2.182 | Acc: 17.603% (7413/42112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1316/1563 Loss: 2.181 | Acc: 17.613% (7423/42144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1317/1563 Loss: 2.181 | Acc: 17.619% (7431/42176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1318/1563 Loss: 2.181 | Acc: 17.620% (7437/42208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1319/1563 Loss: 2.181 | Acc: 17.618% (7442/42240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1320/1563 Loss: 2.181 | Acc: 17.622% (7449/42272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1321/1563 Loss: 2.181 | Acc: 17.630% (7458/42304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1322/1563 Loss: 2.181 | Acc: 17.630% (7464/42336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1323/1563 Loss: 2.181 | Acc: 17.629% (7469/42368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1324/1563 Loss: 2.181 | Acc: 17.642% (7480/42400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1325/1563 Loss: 2.181 | Acc: 17.642% (7486/42432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1326/1563 Loss: 2.181 | Acc: 17.650% (7495/42464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1327/1563 Loss: 2.180 | Acc: 17.665% (7507/42496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1328/1563 Loss: 2.180 | Acc: 17.682% (7520/42528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1329/1563 Loss: 2.180 | Acc: 17.688% (7528/42560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1330/1563 Loss: 2.180 | Acc: 17.689% (7534/42592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1331/1563 Loss: 2.180 | Acc: 17.692% (7541/42624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1332/1563 Loss: 2.180 | Acc: 17.697% (7549/42656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1333/1563 Loss: 2.180 | Acc: 17.710% (7560/42688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1334/1563 Loss: 2.179 | Acc: 17.720% (7570/42720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1335/1563 Loss: 2.179 | Acc: 17.721% (7576/42752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1336/1563 Loss: 2.179 | Acc: 17.722% (7582/42784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1337/1563 Loss: 2.179 | Acc: 17.720% (7587/42816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1338/1563 Loss: 2.179 | Acc: 17.721% (7593/42848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1339/1563 Loss: 2.179 | Acc: 17.733% (7604/42880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1340/1563 Loss: 2.179 | Acc: 17.734% (7610/42912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1341/1563 Loss: 2.179 | Acc: 17.730% (7614/42944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1342/1563 Loss: 2.179 | Acc: 17.740% (7624/42976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1343/1563 Loss: 2.179 | Acc: 17.759% (7638/43008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1344/1563 Loss: 2.179 | Acc: 17.765% (7646/43040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1345/1563 Loss: 2.179 | Acc: 17.773% (7655/43072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1346/1563 Loss: 2.179 | Acc: 17.773% (7661/43104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1347/1563 Loss: 2.179 | Acc: 17.783% (7671/43136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1348/1563 Loss: 2.179 | Acc: 17.786% (7678/43168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1349/1563 Loss: 2.179 | Acc: 17.792% (7686/43200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1350/1563 Loss: 2.178 | Acc: 17.797% (7694/43232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1351/1563 Loss: 2.178 | Acc: 17.805% (7703/43264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1352/1563 Loss: 2.178 | Acc: 17.815% (7713/43296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1353/1563 Loss: 2.178 | Acc: 17.808% (7716/43328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1354/1563 Loss: 2.178 | Acc: 17.809% (7722/43360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1355/1563 Loss: 2.178 | Acc: 17.819% (7732/43392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1356/1563 Loss: 2.178 | Acc: 17.824% (7740/43424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1357/1563 Loss: 2.178 | Acc: 17.825% (7746/43456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1358/1563 Loss: 2.178 | Acc: 17.837% (7757/43488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1359/1563 Loss: 2.178 | Acc: 17.842% (7765/43520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1360/1563 Loss: 2.177 | Acc: 17.841% (7770/43552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1361/1563 Loss: 2.177 | Acc: 17.841% (7776/43584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1362/1563 Loss: 2.177 | Acc: 17.837% (7780/43616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1363/1563 Loss: 2.178 | Acc: 17.831% (7783/43648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1364/1563 Loss: 2.178 | Acc: 17.834% (7790/43680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1365/1563 Loss: 2.177 | Acc: 17.837% (7797/43712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1366/1563 Loss: 2.177 | Acc: 17.845% (7806/43744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1367/1563 Loss: 2.177 | Acc: 17.839% (7809/43776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1368/1563 Loss: 2.177 | Acc: 17.848% (7819/43808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1369/1563 Loss: 2.177 | Acc: 17.847% (7824/43840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1370/1563 Loss: 2.177 | Acc: 17.845% (7829/43872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1371/1563 Loss: 2.177 | Acc: 17.841% (7833/43904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1372/1563 Loss: 2.177 | Acc: 17.842% (7839/43936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1373/1563 Loss: 2.177 | Acc: 17.840% (7844/43968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1374/1563 Loss: 2.177 | Acc: 17.850% (7854/44000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1375/1563 Loss: 2.177 | Acc: 17.848% (7859/44032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1376/1563 Loss: 2.177 | Acc: 17.856% (7868/44064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1377/1563 Loss: 2.177 | Acc: 17.861% (7876/44096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1378/1563 Loss: 2.177 | Acc: 17.857% (7880/44128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1379/1563 Loss: 2.177 | Acc: 17.858% (7886/44160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1380/1563 Loss: 2.176 | Acc: 17.861% (7893/44192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1381/1563 Loss: 2.176 | Acc: 17.868% (7902/44224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1382/1563 Loss: 2.176 | Acc: 17.880% (7913/44256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1383/1563 Loss: 2.176 | Acc: 17.878% (7918/44288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1384/1563 Loss: 2.176 | Acc: 17.881% (7925/44320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1385/1563 Loss: 2.176 | Acc: 17.886% (7933/44352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1386/1563 Loss: 2.176 | Acc: 17.892% (7941/44384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1387/1563 Loss: 2.176 | Acc: 17.903% (7952/44416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1388/1563 Loss: 2.176 | Acc: 17.922% (7966/44448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1389/1563 Loss: 2.176 | Acc: 17.929% (7975/44480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1390/1563 Loss: 2.175 | Acc: 17.941% (7986/44512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1391/1563 Loss: 2.175 | Acc: 17.946% (7994/44544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1392/1563 Loss: 2.175 | Acc: 17.951% (8002/44576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1393/1563 Loss: 2.175 | Acc: 17.963% (8013/44608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1394/1563 Loss: 2.175 | Acc: 17.970% (8022/44640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1395/1563 Loss: 2.175 | Acc: 17.973% (8029/44672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1396/1563 Loss: 2.175 | Acc: 17.976% (8036/44704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1397/1563 Loss: 2.175 | Acc: 17.981% (8044/44736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1398/1563 Loss: 2.175 | Acc: 17.979% (8049/44768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1399/1563 Loss: 2.175 | Acc: 17.978% (8054/44800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1400/1563 Loss: 2.174 | Acc: 17.980% (8061/44832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1401/1563 Loss: 2.174 | Acc: 17.983% (8068/44864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1402/1563 Loss: 2.174 | Acc: 17.977% (8071/44896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1403/1563 Loss: 2.174 | Acc: 17.991% (8083/44928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1404/1563 Loss: 2.174 | Acc: 17.996% (8091/44960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1405/1563 Loss: 2.174 | Acc: 18.008% (8102/44992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1406/1563 Loss: 2.174 | Acc: 18.013% (8110/45024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1407/1563 Loss: 2.174 | Acc: 18.009% (8114/45056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1408/1563 Loss: 2.174 | Acc: 18.014% (8122/45088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1409/1563 Loss: 2.174 | Acc: 18.016% (8129/45120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1410/1563 Loss: 2.173 | Acc: 18.021% (8137/45152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1411/1563 Loss: 2.173 | Acc: 18.031% (8147/45184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1412/1563 Loss: 2.173 | Acc: 18.031% (8153/45216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1413/1563 Loss: 2.173 | Acc: 18.034% (8160/45248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1414/1563 Loss: 2.173 | Acc: 18.043% (8170/45280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1415/1563 Loss: 2.173 | Acc: 18.044% (8176/45312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1416/1563 Loss: 2.173 | Acc: 18.042% (8181/45344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1417/1563 Loss: 2.173 | Acc: 18.047% (8189/45376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1418/1563 Loss: 2.173 | Acc: 18.050% (8196/45408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1419/1563 Loss: 2.173 | Acc: 18.057% (8205/45440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1420/1563 Loss: 2.172 | Acc: 18.051% (8208/45472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1421/1563 Loss: 2.172 | Acc: 18.053% (8215/45504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1422/1563 Loss: 2.172 | Acc: 18.063% (8225/45536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1423/1563 Loss: 2.172 | Acc: 18.065% (8232/45568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1424/1563 Loss: 2.172 | Acc: 18.061% (8236/45600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1425/1563 Loss: 2.172 | Acc: 18.055% (8239/45632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1426/1563 Loss: 2.172 | Acc: 18.067% (8250/45664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1427/1563 Loss: 2.172 | Acc: 18.074% (8259/45696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1428/1563 Loss: 2.172 | Acc: 18.076% (8266/45728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1429/1563 Loss: 2.172 | Acc: 18.075% (8271/45760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1430/1563 Loss: 2.172 | Acc: 18.073% (8276/45792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1431/1563 Loss: 2.172 | Acc: 18.078% (8284/45824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1432/1563 Loss: 2.171 | Acc: 18.076% (8289/45856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1433/1563 Loss: 2.171 | Acc: 18.092% (8302/45888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1434/1563 Loss: 2.171 | Acc: 18.101% (8312/45920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1435/1563 Loss: 2.171 | Acc: 18.112% (8323/45952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1436/1563 Loss: 2.171 | Acc: 18.132% (8338/45984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1437/1563 Loss: 2.171 | Acc: 18.128% (8342/46016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1438/1563 Loss: 2.171 | Acc: 18.129% (8348/46048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1439/1563 Loss: 2.171 | Acc: 18.123% (8351/46080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1440/1563 Loss: 2.171 | Acc: 18.123% (8357/46112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1441/1563 Loss: 2.170 | Acc: 18.135% (8368/46144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1442/1563 Loss: 2.170 | Acc: 18.137% (8375/46176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1443/1563 Loss: 2.170 | Acc: 18.138% (8381/46208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1444/1563 Loss: 2.170 | Acc: 18.140% (8388/46240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1445/1563 Loss: 2.170 | Acc: 18.151% (8399/46272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1446/1563 Loss: 2.170 | Acc: 18.156% (8407/46304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1447/1563 Loss: 2.170 | Acc: 18.154% (8412/46336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1448/1563 Loss: 2.170 | Acc: 18.161% (8421/46368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1449/1563 Loss: 2.170 | Acc: 18.166% (8429/46400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1450/1563 Loss: 2.170 | Acc: 18.169% (8436/46432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1451/1563 Loss: 2.170 | Acc: 18.173% (8444/46464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1452/1563 Loss: 2.170 | Acc: 18.176% (8451/46496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1453/1563 Loss: 2.170 | Acc: 18.176% (8457/46528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1454/1563 Loss: 2.170 | Acc: 18.179% (8464/46560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1455/1563 Loss: 2.170 | Acc: 18.179% (8470/46592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1456/1563 Loss: 2.169 | Acc: 18.184% (8478/46624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1457/1563 Loss: 2.170 | Acc: 18.186% (8485/46656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1458/1563 Loss: 2.170 | Acc: 18.187% (8491/46688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1459/1563 Loss: 2.170 | Acc: 18.185% (8496/46720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1460/1563 Loss: 2.170 | Acc: 18.185% (8502/46752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1461/1563 Loss: 2.169 | Acc: 18.188% (8509/46784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1462/1563 Loss: 2.169 | Acc: 18.197% (8519/46816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1463/1563 Loss: 2.169 | Acc: 18.204% (8528/46848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1464/1563 Loss: 2.169 | Acc: 18.212% (8538/46880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1465/1563 Loss: 2.169 | Acc: 18.209% (8542/46912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1466/1563 Loss: 2.169 | Acc: 18.205% (8546/46944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1467/1563 Loss: 2.169 | Acc: 18.207% (8553/46976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1468/1563 Loss: 2.169 | Acc: 18.210% (8560/47008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1469/1563 Loss: 2.169 | Acc: 18.223% (8572/47040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1470/1563 Loss: 2.169 | Acc: 18.232% (8582/47072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1471/1563 Loss: 2.169 | Acc: 18.230% (8587/47104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1472/1563 Loss: 2.169 | Acc: 18.232% (8594/47136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1473/1563 Loss: 2.169 | Acc: 18.235% (8601/47168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1474/1563 Loss: 2.169 | Acc: 18.239% (8609/47200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1475/1563 Loss: 2.169 | Acc: 18.246% (8618/47232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1476/1563 Loss: 2.169 | Acc: 18.253% (8627/47264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1477/1563 Loss: 2.169 | Acc: 18.255% (8634/47296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1478/1563 Loss: 2.169 | Acc: 18.268% (8646/47328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1479/1563 Loss: 2.169 | Acc: 18.275% (8655/47360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1480/1563 Loss: 2.169 | Acc: 18.277% (8662/47392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1481/1563 Loss: 2.169 | Acc: 18.284% (8671/47424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1482/1563 Loss: 2.169 | Acc: 18.293% (8681/47456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1483/1563 Loss: 2.168 | Acc: 18.306% (8693/47488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1484/1563 Loss: 2.168 | Acc: 18.312% (8702/47520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1485/1563 Loss: 2.168 | Acc: 18.308% (8706/47552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1486/1563 Loss: 2.168 | Acc: 18.309% (8712/47584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1487/1563 Loss: 2.168 | Acc: 18.311% (8719/47616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1488/1563 Loss: 2.168 | Acc: 18.313% (8726/47648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1489/1563 Loss: 2.168 | Acc: 18.312% (8731/47680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1490/1563 Loss: 2.168 | Acc: 18.320% (8741/47712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1491/1563 Loss: 2.168 | Acc: 18.327% (8750/47744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1492/1563 Loss: 2.168 | Acc: 18.323% (8754/47776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1493/1563 Loss: 2.168 | Acc: 18.325% (8761/47808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1494/1563 Loss: 2.168 | Acc: 18.328% (8768/47840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1495/1563 Loss: 2.167 | Acc: 18.328% (8774/47872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1496/1563 Loss: 2.167 | Acc: 18.341% (8786/47904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1497/1563 Loss: 2.167 | Acc: 18.341% (8792/47936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1498/1563 Loss: 2.167 | Acc: 18.343% (8799/47968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1499/1563 Loss: 2.167 | Acc: 18.350% (8808/48000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1500/1563 Loss: 2.167 | Acc: 18.354% (8816/48032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1501/1563 Loss: 2.167 | Acc: 18.355% (8822/48064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1502/1563 Loss: 2.167 | Acc: 18.355% (8828/48096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1503/1563 Loss: 2.167 | Acc: 18.361% (8837/48128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1504/1563 Loss: 2.167 | Acc: 18.368% (8846/48160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1505/1563 Loss: 2.167 | Acc: 18.374% (8855/48192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1506/1563 Loss: 2.167 | Acc: 18.373% (8860/48224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1507/1563 Loss: 2.167 | Acc: 18.379% (8869/48256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1508/1563 Loss: 2.167 | Acc: 18.377% (8874/48288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1509/1563 Loss: 2.166 | Acc: 18.382% (8882/48320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1510/1563 Loss: 2.166 | Acc: 18.384% (8889/48352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1511/1563 Loss: 2.166 | Acc: 18.388% (8897/48384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1512/1563 Loss: 2.166 | Acc: 18.391% (8904/48416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1513/1563 Loss: 2.166 | Acc: 18.395% (8912/48448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1514/1563 Loss: 2.166 | Acc: 18.401% (8921/48480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1515/1563 Loss: 2.166 | Acc: 18.404% (8928/48512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1516/1563 Loss: 2.166 | Acc: 18.404% (8934/48544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1517/1563 Loss: 2.166 | Acc: 18.404% (8940/48576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1518/1563 Loss: 2.166 | Acc: 18.411% (8949/48608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1519/1563 Loss: 2.166 | Acc: 18.415% (8957/48640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1520/1563 Loss: 2.166 | Acc: 18.417% (8964/48672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1521/1563 Loss: 2.166 | Acc: 18.417% (8970/48704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1522/1563 Loss: 2.166 | Acc: 18.420% (8977/48736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1523/1563 Loss: 2.166 | Acc: 18.412% (8979/48768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1524/1563 Loss: 2.166 | Acc: 18.416% (8987/48800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1525/1563 Loss: 2.166 | Acc: 18.412% (8991/48832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1526/1563 Loss: 2.166 | Acc: 18.414% (8998/48864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1527/1563 Loss: 2.166 | Acc: 18.415% (9004/48896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1528/1563 Loss: 2.166 | Acc: 18.413% (9009/48928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1529/1563 Loss: 2.166 | Acc: 18.417% (9017/48960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1530/1563 Loss: 2.166 | Acc: 18.421% (9025/48992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1531/1563 Loss: 2.166 | Acc: 18.418% (9029/49024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1532/1563 Loss: 2.166 | Acc: 18.416% (9034/49056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1533/1563 Loss: 2.166 | Acc: 18.418% (9041/49088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1534/1563 Loss: 2.165 | Acc: 18.424% (9050/49120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1535/1563 Loss: 2.165 | Acc: 18.420% (9054/49152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1536/1563 Loss: 2.165 | Acc: 18.415% (9057/49184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1537/1563 Loss: 2.165 | Acc: 18.421% (9066/49216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1538/1563 Loss: 2.165 | Acc: 18.425% (9074/49248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1539/1563 Loss: 2.165 | Acc: 18.423% (9079/49280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1540/1563 Loss: 2.165 | Acc: 18.428% (9087/49312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1541/1563 Loss: 2.165 | Acc: 18.434% (9096/49344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1542/1563 Loss: 2.165 | Acc: 18.436% (9103/49376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1543/1563 Loss: 2.165 | Acc: 18.434% (9108/49408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1544/1563 Loss: 2.165 | Acc: 18.443% (9118/49440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1545/1563 Loss: 2.165 | Acc: 18.439% (9122/49472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1546/1563 Loss: 2.165 | Acc: 18.441% (9129/49504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1547/1563 Loss: 2.165 | Acc: 18.441% (9135/49536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1548/1563 Loss: 2.165 | Acc: 18.443% (9142/49568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1549/1563 Loss: 2.164 | Acc: 18.446% (9149/49600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1550/1563 Loss: 2.164 | Acc: 18.456% (9160/49632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1551/1563 Loss: 2.164 | Acc: 18.466% (9171/49664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1552/1563 Loss: 2.164 | Acc: 18.476% (9182/49696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1553/1563 Loss: 2.164 | Acc: 18.477% (9188/49728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1554/1563 Loss: 2.164 | Acc: 18.479% (9195/49760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1555/1563 Loss: 2.164 | Acc: 18.479% (9201/49792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1556/1563 Loss: 2.164 | Acc: 18.483% (9209/49824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1557/1563 Loss: 2.164 | Acc: 18.485% (9216/49856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1558/1563 Loss: 2.164 | Acc: 18.489% (9224/49888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1559/1563 Loss: 2.164 | Acc: 18.496% (9233/49920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1560/1563 Loss: 2.164 | Acc: 18.498% (9240/49952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 0 Step 1561/1563 Loss: 2.164 | Acc: 18.502% (9248/49984)\n",
            "torch.Size([16, 120, 1, 1])\n",
            "torch.Size([16, 120])\n",
            "Epoch 0 Step 1562/1563 Loss: 2.164 | Acc: 18.504% (9252/50000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/opacus/privacy_engine.py:283: UserWarning: PrivacyEngine expected a batch of size 32 but the last step received a batch of size 16. This means that the privacy analysis will be a bit more pessimistic. You can set `drop_last = True` in your PyTorch dataloader to avoid this problem completely\n",
            "  f\"PrivacyEngine expected a batch of size {self.batch_size} \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 209/1563 Loss: 2.100 | Acc: 28.051% (1885/6720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 210/1563 Loss: 2.101 | Acc: 28.051% (1894/6752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 211/1563 Loss: 2.100 | Acc: 28.066% (1904/6784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 212/1563 Loss: 2.101 | Acc: 28.052% (1912/6816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 213/1563 Loss: 2.101 | Acc: 28.052% (1921/6848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 214/1563 Loss: 2.101 | Acc: 28.067% (1931/6880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 215/1563 Loss: 2.100 | Acc: 28.154% (1946/6912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 216/1563 Loss: 2.098 | Acc: 28.211% (1959/6944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 217/1563 Loss: 2.099 | Acc: 28.154% (1964/6976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 218/1563 Loss: 2.098 | Acc: 28.211% (1977/7008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 219/1563 Loss: 2.099 | Acc: 28.139% (1981/7040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 220/1563 Loss: 2.100 | Acc: 28.068% (1985/7072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 221/1563 Loss: 2.102 | Acc: 27.970% (1987/7104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 222/1563 Loss: 2.102 | Acc: 27.929% (1993/7136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 223/1563 Loss: 2.100 | Acc: 27.972% (2005/7168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 224/1563 Loss: 2.101 | Acc: 27.972% (2014/7200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 225/1563 Loss: 2.102 | Acc: 27.959% (2022/7232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 226/1563 Loss: 2.102 | Acc: 27.960% (2031/7264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 227/1563 Loss: 2.102 | Acc: 27.974% (2041/7296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 228/1563 Loss: 2.102 | Acc: 27.948% (2048/7328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 229/1563 Loss: 2.102 | Acc: 27.921% (2055/7360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 230/1563 Loss: 2.102 | Acc: 27.909% (2063/7392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 231/1563 Loss: 2.101 | Acc: 27.923% (2073/7424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 232/1563 Loss: 2.101 | Acc: 27.924% (2082/7456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 233/1563 Loss: 2.101 | Acc: 27.978% (2095/7488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 234/1563 Loss: 2.101 | Acc: 27.979% (2104/7520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 235/1563 Loss: 2.101 | Acc: 27.979% (2113/7552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 236/1563 Loss: 2.101 | Acc: 27.954% (2120/7584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 237/1563 Loss: 2.101 | Acc: 27.981% (2131/7616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 238/1563 Loss: 2.102 | Acc: 27.942% (2137/7648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 239/1563 Loss: 2.101 | Acc: 27.956% (2147/7680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 240/1563 Loss: 2.102 | Acc: 27.969% (2157/7712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 241/1563 Loss: 2.105 | Acc: 27.880% (2159/7744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 242/1563 Loss: 2.103 | Acc: 27.894% (2169/7776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 243/1563 Loss: 2.102 | Acc: 27.946% (2182/7808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 244/1563 Loss: 2.101 | Acc: 27.959% (2192/7840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 245/1563 Loss: 2.101 | Acc: 27.973% (2202/7872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 246/1563 Loss: 2.102 | Acc: 27.961% (2210/7904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 247/1563 Loss: 2.102 | Acc: 27.949% (2218/7936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 248/1563 Loss: 2.103 | Acc: 27.912% (2224/7968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 249/1563 Loss: 2.103 | Acc: 27.913% (2233/8000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 250/1563 Loss: 2.103 | Acc: 27.938% (2244/8032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 251/1563 Loss: 2.103 | Acc: 27.889% (2249/8064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 252/1563 Loss: 2.103 | Acc: 27.866% (2256/8096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 253/1563 Loss: 2.102 | Acc: 27.879% (2266/8128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 254/1563 Loss: 2.104 | Acc: 27.819% (2270/8160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 255/1563 Loss: 2.105 | Acc: 27.771% (2275/8192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 256/1563 Loss: 2.105 | Acc: 27.797% (2286/8224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 257/1563 Loss: 2.103 | Acc: 27.822% (2297/8256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 258/1563 Loss: 2.105 | Acc: 27.787% (2303/8288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 259/1563 Loss: 2.103 | Acc: 27.800% (2313/8320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 260/1563 Loss: 2.103 | Acc: 27.778% (2320/8352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 261/1563 Loss: 2.104 | Acc: 27.779% (2329/8384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 262/1563 Loss: 2.102 | Acc: 27.769% (2337/8416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 263/1563 Loss: 2.103 | Acc: 27.734% (2343/8448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 264/1563 Loss: 2.101 | Acc: 27.818% (2359/8480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 265/1563 Loss: 2.101 | Acc: 27.831% (2369/8512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 266/1563 Loss: 2.102 | Acc: 27.797% (2375/8544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 267/1563 Loss: 2.103 | Acc: 27.764% (2381/8576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 268/1563 Loss: 2.102 | Acc: 27.776% (2391/8608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 269/1563 Loss: 2.103 | Acc: 27.789% (2401/8640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 270/1563 Loss: 2.103 | Acc: 27.768% (2408/8672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 271/1563 Loss: 2.102 | Acc: 27.815% (2421/8704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 272/1563 Loss: 2.102 | Acc: 27.804% (2429/8736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 273/1563 Loss: 2.103 | Acc: 27.783% (2436/8768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 274/1563 Loss: 2.104 | Acc: 27.761% (2443/8800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 275/1563 Loss: 2.104 | Acc: 27.706% (2447/8832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 276/1563 Loss: 2.104 | Acc: 27.708% (2456/8864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 277/1563 Loss: 2.105 | Acc: 27.709% (2465/8896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 278/1563 Loss: 2.104 | Acc: 27.722% (2475/8928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 279/1563 Loss: 2.104 | Acc: 27.723% (2484/8960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 280/1563 Loss: 2.104 | Acc: 27.736% (2494/8992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 281/1563 Loss: 2.104 | Acc: 27.715% (2501/9024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 282/1563 Loss: 2.105 | Acc: 27.705% (2509/9056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 283/1563 Loss: 2.105 | Acc: 27.674% (2515/9088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 284/1563 Loss: 2.106 | Acc: 27.654% (2522/9120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 285/1563 Loss: 2.107 | Acc: 27.655% (2531/9152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 286/1563 Loss: 2.106 | Acc: 27.657% (2540/9184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 287/1563 Loss: 2.107 | Acc: 27.615% (2545/9216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 288/1563 Loss: 2.108 | Acc: 27.606% (2553/9248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 289/1563 Loss: 2.108 | Acc: 27.597% (2561/9280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 290/1563 Loss: 2.109 | Acc: 27.599% (2570/9312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 291/1563 Loss: 2.110 | Acc: 27.536% (2573/9344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 292/1563 Loss: 2.110 | Acc: 27.560% (2584/9376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 293/1563 Loss: 2.110 | Acc: 27.551% (2592/9408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 294/1563 Loss: 2.109 | Acc: 27.574% (2603/9440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 295/1563 Loss: 2.108 | Acc: 27.597% (2614/9472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 296/1563 Loss: 2.108 | Acc: 27.588% (2622/9504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 297/1563 Loss: 2.107 | Acc: 27.601% (2632/9536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 298/1563 Loss: 2.107 | Acc: 27.602% (2641/9568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 299/1563 Loss: 2.107 | Acc: 27.604% (2650/9600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 300/1563 Loss: 2.107 | Acc: 27.596% (2658/9632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 301/1563 Loss: 2.107 | Acc: 27.618% (2669/9664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 302/1563 Loss: 2.107 | Acc: 27.599% (2676/9696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 303/1563 Loss: 2.108 | Acc: 27.580% (2683/9728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 304/1563 Loss: 2.109 | Acc: 27.561% (2690/9760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 305/1563 Loss: 2.107 | Acc: 27.614% (2704/9792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 306/1563 Loss: 2.107 | Acc: 27.575% (2709/9824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 307/1563 Loss: 2.107 | Acc: 27.557% (2716/9856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 308/1563 Loss: 2.107 | Acc: 27.538% (2723/9888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 309/1563 Loss: 2.106 | Acc: 27.560% (2734/9920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 310/1563 Loss: 2.107 | Acc: 27.522% (2739/9952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 311/1563 Loss: 2.108 | Acc: 27.544% (2750/9984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 312/1563 Loss: 2.106 | Acc: 27.576% (2762/10016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 313/1563 Loss: 2.105 | Acc: 27.617% (2775/10048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 314/1563 Loss: 2.105 | Acc: 27.619% (2784/10080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 315/1563 Loss: 2.105 | Acc: 27.631% (2794/10112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 316/1563 Loss: 2.104 | Acc: 27.612% (2801/10144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 317/1563 Loss: 2.105 | Acc: 27.624% (2811/10176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 318/1563 Loss: 2.105 | Acc: 27.665% (2824/10208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 319/1563 Loss: 2.105 | Acc: 27.656% (2832/10240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 320/1563 Loss: 2.105 | Acc: 27.638% (2839/10272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 321/1563 Loss: 2.104 | Acc: 27.649% (2849/10304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 322/1563 Loss: 2.104 | Acc: 27.661% (2859/10336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 323/1563 Loss: 2.103 | Acc: 27.710% (2873/10368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 324/1563 Loss: 2.102 | Acc: 27.740% (2885/10400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 325/1563 Loss: 2.104 | Acc: 27.732% (2893/10432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 326/1563 Loss: 2.103 | Acc: 27.791% (2908/10464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 327/1563 Loss: 2.102 | Acc: 27.820% (2920/10496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 328/1563 Loss: 2.102 | Acc: 27.840% (2931/10528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 329/1563 Loss: 2.101 | Acc: 27.812% (2937/10560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 330/1563 Loss: 2.102 | Acc: 27.785% (2943/10592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 331/1563 Loss: 2.102 | Acc: 27.777% (2951/10624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 332/1563 Loss: 2.102 | Acc: 27.768% (2959/10656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 333/1563 Loss: 2.102 | Acc: 27.779% (2969/10688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 334/1563 Loss: 2.102 | Acc: 27.771% (2977/10720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 335/1563 Loss: 2.102 | Acc: 27.734% (2982/10752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 336/1563 Loss: 2.102 | Acc: 27.754% (2993/10784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 337/1563 Loss: 2.101 | Acc: 27.764% (3003/10816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 338/1563 Loss: 2.100 | Acc: 27.775% (3013/10848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 339/1563 Loss: 2.100 | Acc: 27.748% (3019/10880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 340/1563 Loss: 2.101 | Acc: 27.768% (3030/10912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 341/1563 Loss: 2.100 | Acc: 27.787% (3041/10944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 342/1563 Loss: 2.100 | Acc: 27.797% (3051/10976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 343/1563 Loss: 2.101 | Acc: 27.771% (3057/11008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 344/1563 Loss: 2.101 | Acc: 27.754% (3064/11040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 345/1563 Loss: 2.100 | Acc: 27.773% (3075/11072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 346/1563 Loss: 2.100 | Acc: 27.747% (3081/11104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 347/1563 Loss: 2.101 | Acc: 27.712% (3086/11136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 348/1563 Loss: 2.101 | Acc: 27.695% (3093/11168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 349/1563 Loss: 2.100 | Acc: 27.705% (3103/11200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 350/1563 Loss: 2.101 | Acc: 27.689% (3110/11232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 351/1563 Loss: 2.101 | Acc: 27.663% (3116/11264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 352/1563 Loss: 2.101 | Acc: 27.665% (3125/11296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 353/1563 Loss: 2.101 | Acc: 27.692% (3137/11328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 354/1563 Loss: 2.101 | Acc: 27.658% (3142/11360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 355/1563 Loss: 2.101 | Acc: 27.660% (3151/11392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 356/1563 Loss: 2.100 | Acc: 27.687% (3163/11424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 357/1563 Loss: 2.101 | Acc: 27.689% (3172/11456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 358/1563 Loss: 2.101 | Acc: 27.672% (3179/11488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 359/1563 Loss: 2.101 | Acc: 27.682% (3189/11520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 360/1563 Loss: 2.101 | Acc: 27.684% (3198/11552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 361/1563 Loss: 2.101 | Acc: 27.667% (3205/11584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 362/1563 Loss: 2.101 | Acc: 27.660% (3213/11616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 363/1563 Loss: 2.100 | Acc: 27.696% (3226/11648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 364/1563 Loss: 2.100 | Acc: 27.714% (3237/11680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 365/1563 Loss: 2.099 | Acc: 27.715% (3246/11712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 366/1563 Loss: 2.099 | Acc: 27.716% (3255/11744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 367/1563 Loss: 2.099 | Acc: 27.709% (3263/11776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 368/1563 Loss: 2.099 | Acc: 27.735% (3275/11808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 369/1563 Loss: 2.099 | Acc: 27.728% (3283/11840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 370/1563 Loss: 2.098 | Acc: 27.754% (3295/11872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 371/1563 Loss: 2.097 | Acc: 27.764% (3305/11904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 372/1563 Loss: 2.098 | Acc: 27.748% (3312/11936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 373/1563 Loss: 2.098 | Acc: 27.757% (3322/11968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 374/1563 Loss: 2.097 | Acc: 27.767% (3332/12000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 375/1563 Loss: 2.099 | Acc: 27.726% (3336/12032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 376/1563 Loss: 2.098 | Acc: 27.735% (3346/12064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 377/1563 Loss: 2.098 | Acc: 27.753% (3357/12096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 378/1563 Loss: 2.099 | Acc: 27.762% (3367/12128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 379/1563 Loss: 2.098 | Acc: 27.780% (3378/12160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 380/1563 Loss: 2.096 | Acc: 27.822% (3392/12192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 381/1563 Loss: 2.097 | Acc: 27.814% (3400/12224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 382/1563 Loss: 2.096 | Acc: 27.831% (3411/12256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 383/1563 Loss: 2.096 | Acc: 27.832% (3420/12288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 384/1563 Loss: 2.095 | Acc: 27.833% (3429/12320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 385/1563 Loss: 2.095 | Acc: 27.834% (3438/12352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 386/1563 Loss: 2.093 | Acc: 27.859% (3450/12384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 387/1563 Loss: 2.094 | Acc: 27.851% (3458/12416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 388/1563 Loss: 2.094 | Acc: 27.852% (3467/12448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 389/1563 Loss: 2.094 | Acc: 27.821% (3472/12480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 390/1563 Loss: 2.093 | Acc: 27.837% (3483/12512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 391/1563 Loss: 2.094 | Acc: 27.838% (3492/12544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 392/1563 Loss: 2.094 | Acc: 27.815% (3498/12576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 393/1563 Loss: 2.095 | Acc: 27.784% (3503/12608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 394/1563 Loss: 2.095 | Acc: 27.809% (3515/12640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 395/1563 Loss: 2.094 | Acc: 27.809% (3524/12672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 396/1563 Loss: 2.094 | Acc: 27.842% (3537/12704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 397/1563 Loss: 2.094 | Acc: 27.827% (3544/12736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 398/1563 Loss: 2.094 | Acc: 27.867% (3558/12768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 399/1563 Loss: 2.093 | Acc: 27.891% (3570/12800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 400/1563 Loss: 2.093 | Acc: 27.899% (3580/12832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 401/1563 Loss: 2.093 | Acc: 27.915% (3591/12864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 402/1563 Loss: 2.093 | Acc: 27.877% (3595/12896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 403/1563 Loss: 2.093 | Acc: 27.854% (3601/12928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 404/1563 Loss: 2.093 | Acc: 27.824% (3606/12960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 405/1563 Loss: 2.095 | Acc: 27.802% (3612/12992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 406/1563 Loss: 2.094 | Acc: 27.810% (3622/13024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 407/1563 Loss: 2.093 | Acc: 27.834% (3634/13056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 408/1563 Loss: 2.094 | Acc: 27.858% (3646/13088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 409/1563 Loss: 2.095 | Acc: 27.835% (3652/13120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 410/1563 Loss: 2.095 | Acc: 27.798% (3656/13152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 411/1563 Loss: 2.096 | Acc: 27.776% (3662/13184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 412/1563 Loss: 2.097 | Acc: 27.754% (3668/13216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 413/1563 Loss: 2.096 | Acc: 27.748% (3676/13248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 414/1563 Loss: 2.096 | Acc: 27.726% (3682/13280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 415/1563 Loss: 2.096 | Acc: 27.719% (3690/13312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 416/1563 Loss: 2.097 | Acc: 27.713% (3698/13344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 417/1563 Loss: 2.097 | Acc: 27.714% (3707/13376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 418/1563 Loss: 2.097 | Acc: 27.722% (3717/13408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 419/1563 Loss: 2.096 | Acc: 27.716% (3725/13440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 420/1563 Loss: 2.096 | Acc: 27.709% (3733/13472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 421/1563 Loss: 2.097 | Acc: 27.688% (3739/13504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 422/1563 Loss: 2.097 | Acc: 27.697% (3749/13536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 423/1563 Loss: 2.098 | Acc: 27.661% (3753/13568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 424/1563 Loss: 2.099 | Acc: 27.654% (3761/13600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 425/1563 Loss: 2.098 | Acc: 27.700% (3776/13632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 426/1563 Loss: 2.098 | Acc: 27.722% (3788/13664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 427/1563 Loss: 2.098 | Acc: 27.745% (3800/13696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 428/1563 Loss: 2.098 | Acc: 27.753% (3810/13728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 429/1563 Loss: 2.098 | Acc: 27.747% (3818/13760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 430/1563 Loss: 2.097 | Acc: 27.777% (3831/13792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 431/1563 Loss: 2.097 | Acc: 27.763% (3838/13824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 432/1563 Loss: 2.098 | Acc: 27.771% (3848/13856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 433/1563 Loss: 2.097 | Acc: 27.751% (3854/13888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 434/1563 Loss: 2.097 | Acc: 27.751% (3863/13920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 435/1563 Loss: 2.098 | Acc: 27.731% (3869/13952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 436/1563 Loss: 2.099 | Acc: 27.725% (3877/13984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 437/1563 Loss: 2.098 | Acc: 27.725% (3886/14016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 438/1563 Loss: 2.097 | Acc: 27.726% (3895/14048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 439/1563 Loss: 2.098 | Acc: 27.727% (3904/14080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 440/1563 Loss: 2.098 | Acc: 27.721% (3912/14112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 441/1563 Loss: 2.099 | Acc: 27.701% (3918/14144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 442/1563 Loss: 2.099 | Acc: 27.709% (3928/14176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 443/1563 Loss: 2.099 | Acc: 27.696% (3935/14208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 444/1563 Loss: 2.098 | Acc: 27.725% (3948/14240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 445/1563 Loss: 2.098 | Acc: 27.740% (3959/14272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 446/1563 Loss: 2.097 | Acc: 27.706% (3963/14304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 447/1563 Loss: 2.097 | Acc: 27.720% (3974/14336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 448/1563 Loss: 2.097 | Acc: 27.735% (3985/14368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 449/1563 Loss: 2.097 | Acc: 27.743% (3995/14400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 450/1563 Loss: 2.097 | Acc: 27.744% (4004/14432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 451/1563 Loss: 2.097 | Acc: 27.710% (4008/14464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 452/1563 Loss: 2.097 | Acc: 27.697% (4015/14496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 453/1563 Loss: 2.097 | Acc: 27.705% (4025/14528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 454/1563 Loss: 2.096 | Acc: 27.706% (4034/14560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 455/1563 Loss: 2.096 | Acc: 27.686% (4040/14592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 456/1563 Loss: 2.096 | Acc: 27.687% (4049/14624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 457/1563 Loss: 2.096 | Acc: 27.702% (4060/14656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 458/1563 Loss: 2.096 | Acc: 27.717% (4071/14688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 459/1563 Loss: 2.095 | Acc: 27.751% (4085/14720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 460/1563 Loss: 2.096 | Acc: 27.752% (4094/14752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 461/1563 Loss: 2.095 | Acc: 27.760% (4104/14784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 462/1563 Loss: 2.096 | Acc: 27.761% (4113/14816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 463/1563 Loss: 2.095 | Acc: 27.775% (4124/14848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 464/1563 Loss: 2.095 | Acc: 27.769% (4132/14880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 465/1563 Loss: 2.095 | Acc: 27.756% (4139/14912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 466/1563 Loss: 2.095 | Acc: 27.770% (4150/14944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 467/1563 Loss: 2.094 | Acc: 27.764% (4158/14976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 468/1563 Loss: 2.094 | Acc: 27.765% (4167/15008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 469/1563 Loss: 2.095 | Acc: 27.759% (4175/15040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 470/1563 Loss: 2.094 | Acc: 27.760% (4184/15072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 471/1563 Loss: 2.095 | Acc: 27.728% (4188/15104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 472/1563 Loss: 2.095 | Acc: 27.722% (4196/15136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 473/1563 Loss: 2.095 | Acc: 27.729% (4206/15168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 474/1563 Loss: 2.094 | Acc: 27.757% (4219/15200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 475/1563 Loss: 2.094 | Acc: 27.738% (4225/15232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 476/1563 Loss: 2.093 | Acc: 27.791% (4242/15264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 477/1563 Loss: 2.093 | Acc: 27.798% (4252/15296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 478/1563 Loss: 2.094 | Acc: 27.786% (4259/15328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 479/1563 Loss: 2.094 | Acc: 27.780% (4267/15360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 480/1563 Loss: 2.094 | Acc: 27.768% (4274/15392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 481/1563 Loss: 2.094 | Acc: 27.788% (4286/15424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 482/1563 Loss: 2.094 | Acc: 27.802% (4297/15456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 483/1563 Loss: 2.095 | Acc: 27.796% (4305/15488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 484/1563 Loss: 2.095 | Acc: 27.803% (4315/15520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 485/1563 Loss: 2.095 | Acc: 27.791% (4322/15552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 486/1563 Loss: 2.096 | Acc: 27.766% (4327/15584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 487/1563 Loss: 2.095 | Acc: 27.760% (4335/15616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 488/1563 Loss: 2.095 | Acc: 27.774% (4346/15648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 489/1563 Loss: 2.095 | Acc: 27.755% (4352/15680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 490/1563 Loss: 2.095 | Acc: 27.737% (4358/15712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 491/1563 Loss: 2.096 | Acc: 27.738% (4367/15744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 492/1563 Loss: 2.095 | Acc: 27.745% (4377/15776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 493/1563 Loss: 2.095 | Acc: 27.745% (4386/15808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 494/1563 Loss: 2.095 | Acc: 27.746% (4395/15840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 495/1563 Loss: 2.095 | Acc: 27.747% (4404/15872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 496/1563 Loss: 2.095 | Acc: 27.754% (4414/15904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 497/1563 Loss: 2.095 | Acc: 27.730% (4419/15936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 498/1563 Loss: 2.095 | Acc: 27.724% (4427/15968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 499/1563 Loss: 2.094 | Acc: 27.719% (4435/16000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 500/1563 Loss: 2.094 | Acc: 27.707% (4442/16032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 501/1563 Loss: 2.094 | Acc: 27.720% (4453/16064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 502/1563 Loss: 2.094 | Acc: 27.746% (4466/16096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 503/1563 Loss: 2.094 | Acc: 27.747% (4475/16128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 504/1563 Loss: 2.094 | Acc: 27.723% (4480/16160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 505/1563 Loss: 2.094 | Acc: 27.730% (4490/16192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 506/1563 Loss: 2.095 | Acc: 27.712% (4496/16224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 507/1563 Loss: 2.095 | Acc: 27.713% (4505/16256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 508/1563 Loss: 2.095 | Acc: 27.726% (4516/16288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 509/1563 Loss: 2.095 | Acc: 27.733% (4526/16320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 510/1563 Loss: 2.094 | Acc: 27.746% (4537/16352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 511/1563 Loss: 2.094 | Acc: 27.753% (4547/16384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 512/1563 Loss: 2.094 | Acc: 27.753% (4556/16416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 513/1563 Loss: 2.093 | Acc: 27.785% (4570/16448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 514/1563 Loss: 2.093 | Acc: 27.785% (4579/16480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 515/1563 Loss: 2.094 | Acc: 27.768% (4585/16512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 516/1563 Loss: 2.094 | Acc: 27.750% (4591/16544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 517/1563 Loss: 2.095 | Acc: 27.739% (4598/16576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 518/1563 Loss: 2.095 | Acc: 27.758% (4610/16608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 519/1563 Loss: 2.095 | Acc: 27.776% (4622/16640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 520/1563 Loss: 2.095 | Acc: 27.747% (4626/16672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 521/1563 Loss: 2.095 | Acc: 27.754% (4636/16704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 522/1563 Loss: 2.095 | Acc: 27.743% (4643/16736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 523/1563 Loss: 2.095 | Acc: 27.731% (4650/16768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 524/1563 Loss: 2.094 | Acc: 27.720% (4657/16800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 525/1563 Loss: 2.095 | Acc: 27.697% (4662/16832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 526/1563 Loss: 2.095 | Acc: 27.686% (4669/16864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 527/1563 Loss: 2.096 | Acc: 27.687% (4678/16896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 528/1563 Loss: 2.096 | Acc: 27.664% (4683/16928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 529/1563 Loss: 2.096 | Acc: 27.671% (4693/16960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 530/1563 Loss: 2.096 | Acc: 27.695% (4706/16992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 531/1563 Loss: 2.096 | Acc: 27.714% (4718/17024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 532/1563 Loss: 2.096 | Acc: 27.726% (4729/17056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 533/1563 Loss: 2.096 | Acc: 27.715% (4736/17088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 534/1563 Loss: 2.096 | Acc: 27.728% (4747/17120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 535/1563 Loss: 2.096 | Acc: 27.717% (4754/17152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 536/1563 Loss: 2.096 | Acc: 27.723% (4764/17184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 537/1563 Loss: 2.095 | Acc: 27.730% (4774/17216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 538/1563 Loss: 2.095 | Acc: 27.748% (4786/17248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 539/1563 Loss: 2.095 | Acc: 27.743% (4794/17280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 540/1563 Loss: 2.095 | Acc: 27.732% (4801/17312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 541/1563 Loss: 2.095 | Acc: 27.733% (4810/17344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 542/1563 Loss: 2.095 | Acc: 27.734% (4819/17376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 543/1563 Loss: 2.095 | Acc: 27.729% (4827/17408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 544/1563 Loss: 2.095 | Acc: 27.752% (4840/17440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 545/1563 Loss: 2.096 | Acc: 27.730% (4845/17472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 546/1563 Loss: 2.096 | Acc: 27.725% (4853/17504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 547/1563 Loss: 2.096 | Acc: 27.714% (4860/17536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 548/1563 Loss: 2.097 | Acc: 27.692% (4865/17568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 549/1563 Loss: 2.097 | Acc: 27.682% (4872/17600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 550/1563 Loss: 2.097 | Acc: 27.677% (4880/17632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 551/1563 Loss: 2.097 | Acc: 27.683% (4890/17664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 552/1563 Loss: 2.097 | Acc: 27.684% (4899/17696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 553/1563 Loss: 2.098 | Acc: 27.674% (4906/17728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 554/1563 Loss: 2.098 | Acc: 27.663% (4913/17760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 555/1563 Loss: 2.099 | Acc: 27.658% (4921/17792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 556/1563 Loss: 2.098 | Acc: 27.665% (4931/17824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 557/1563 Loss: 2.098 | Acc: 27.677% (4942/17856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 558/1563 Loss: 2.098 | Acc: 27.683% (4952/17888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 559/1563 Loss: 2.098 | Acc: 27.695% (4963/17920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 560/1563 Loss: 2.097 | Acc: 27.691% (4971/17952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 561/1563 Loss: 2.096 | Acc: 27.725% (4986/17984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 562/1563 Loss: 2.096 | Acc: 27.742% (4998/18016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 563/1563 Loss: 2.097 | Acc: 27.743% (5007/18048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 564/1563 Loss: 2.096 | Acc: 27.738% (5015/18080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 565/1563 Loss: 2.097 | Acc: 27.750% (5026/18112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 566/1563 Loss: 2.096 | Acc: 27.772% (5039/18144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 567/1563 Loss: 2.096 | Acc: 27.789% (5051/18176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 568/1563 Loss: 2.096 | Acc: 27.795% (5061/18208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 569/1563 Loss: 2.096 | Acc: 27.785% (5068/18240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 570/1563 Loss: 2.095 | Acc: 27.791% (5078/18272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 571/1563 Loss: 2.096 | Acc: 27.786% (5086/18304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 572/1563 Loss: 2.096 | Acc: 27.781% (5094/18336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 573/1563 Loss: 2.096 | Acc: 27.771% (5101/18368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 574/1563 Loss: 2.096 | Acc: 27.772% (5110/18400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 575/1563 Loss: 2.095 | Acc: 27.762% (5117/18432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 576/1563 Loss: 2.096 | Acc: 27.762% (5126/18464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 577/1563 Loss: 2.095 | Acc: 27.757% (5134/18496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 578/1563 Loss: 2.096 | Acc: 27.769% (5145/18528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 579/1563 Loss: 2.096 | Acc: 27.764% (5153/18560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 580/1563 Loss: 2.096 | Acc: 27.743% (5158/18592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 581/1563 Loss: 2.096 | Acc: 27.738% (5166/18624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 582/1563 Loss: 2.096 | Acc: 27.739% (5175/18656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 583/1563 Loss: 2.096 | Acc: 27.734% (5183/18688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 584/1563 Loss: 2.096 | Acc: 27.740% (5193/18720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 585/1563 Loss: 2.096 | Acc: 27.762% (5206/18752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 586/1563 Loss: 2.096 | Acc: 27.768% (5216/18784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 587/1563 Loss: 2.097 | Acc: 27.780% (5227/18816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 588/1563 Loss: 2.096 | Acc: 27.791% (5238/18848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 589/1563 Loss: 2.095 | Acc: 27.802% (5249/18880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 590/1563 Loss: 2.095 | Acc: 27.802% (5258/18912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 591/1563 Loss: 2.095 | Acc: 27.798% (5266/18944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 592/1563 Loss: 2.095 | Acc: 27.814% (5278/18976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 593/1563 Loss: 2.094 | Acc: 27.836% (5291/19008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 594/1563 Loss: 2.094 | Acc: 27.836% (5300/19040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 595/1563 Loss: 2.093 | Acc: 27.852% (5312/19072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 596/1563 Loss: 2.093 | Acc: 27.832% (5317/19104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 597/1563 Loss: 2.093 | Acc: 27.838% (5327/19136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 598/1563 Loss: 2.093 | Acc: 27.828% (5334/19168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 599/1563 Loss: 2.093 | Acc: 27.812% (5340/19200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 600/1563 Loss: 2.094 | Acc: 27.797% (5346/19232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 601/1563 Loss: 2.093 | Acc: 27.793% (5354/19264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 602/1563 Loss: 2.093 | Acc: 27.804% (5365/19296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 603/1563 Loss: 2.094 | Acc: 27.789% (5371/19328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 604/1563 Loss: 2.094 | Acc: 27.794% (5381/19360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 605/1563 Loss: 2.095 | Acc: 27.779% (5387/19392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 606/1563 Loss: 2.095 | Acc: 27.790% (5398/19424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 607/1563 Loss: 2.095 | Acc: 27.786% (5406/19456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 608/1563 Loss: 2.095 | Acc: 27.802% (5418/19488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 609/1563 Loss: 2.094 | Acc: 27.812% (5429/19520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 610/1563 Loss: 2.094 | Acc: 27.798% (5435/19552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 611/1563 Loss: 2.094 | Acc: 27.768% (5438/19584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 612/1563 Loss: 2.094 | Acc: 27.768% (5447/19616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 613/1563 Loss: 2.094 | Acc: 27.769% (5456/19648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 614/1563 Loss: 2.095 | Acc: 27.774% (5466/19680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 615/1563 Loss: 2.094 | Acc: 27.775% (5475/19712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 616/1563 Loss: 2.094 | Acc: 27.770% (5483/19744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 617/1563 Loss: 2.094 | Acc: 27.781% (5494/19776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 618/1563 Loss: 2.093 | Acc: 27.807% (5508/19808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 619/1563 Loss: 2.093 | Acc: 27.777% (5511/19840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 620/1563 Loss: 2.093 | Acc: 27.803% (5525/19872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 621/1563 Loss: 2.093 | Acc: 27.814% (5536/19904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 622/1563 Loss: 2.094 | Acc: 27.809% (5544/19936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 623/1563 Loss: 2.093 | Acc: 27.830% (5557/19968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 624/1563 Loss: 2.093 | Acc: 27.850% (5570/20000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 625/1563 Loss: 2.092 | Acc: 27.865% (5582/20032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 626/1563 Loss: 2.092 | Acc: 27.866% (5591/20064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 627/1563 Loss: 2.092 | Acc: 27.871% (5601/20096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 628/1563 Loss: 2.092 | Acc: 27.862% (5608/20128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 629/1563 Loss: 2.093 | Acc: 27.857% (5616/20160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 630/1563 Loss: 2.092 | Acc: 27.863% (5626/20192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 631/1563 Loss: 2.092 | Acc: 27.863% (5635/20224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 632/1563 Loss: 2.092 | Acc: 27.868% (5645/20256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 633/1563 Loss: 2.092 | Acc: 27.874% (5655/20288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 634/1563 Loss: 2.092 | Acc: 27.864% (5662/20320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 635/1563 Loss: 2.093 | Acc: 27.860% (5670/20352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 636/1563 Loss: 2.093 | Acc: 27.865% (5680/20384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 637/1563 Loss: 2.093 | Acc: 27.875% (5691/20416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 638/1563 Loss: 2.093 | Acc: 27.885% (5702/20448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 639/1563 Loss: 2.092 | Acc: 27.915% (5717/20480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 640/1563 Loss: 2.092 | Acc: 27.910% (5725/20512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 641/1563 Loss: 2.092 | Acc: 27.891% (5730/20544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 642/1563 Loss: 2.093 | Acc: 27.872% (5735/20576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 643/1563 Loss: 2.092 | Acc: 27.882% (5746/20608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 644/1563 Loss: 2.093 | Acc: 27.868% (5752/20640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 645/1563 Loss: 2.093 | Acc: 27.869% (5761/20672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 646/1563 Loss: 2.093 | Acc: 27.898% (5776/20704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 647/1563 Loss: 2.093 | Acc: 27.889% (5783/20736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 648/1563 Loss: 2.093 | Acc: 27.894% (5793/20768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 649/1563 Loss: 2.093 | Acc: 27.894% (5802/20800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 650/1563 Loss: 2.092 | Acc: 27.909% (5814/20832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 651/1563 Loss: 2.092 | Acc: 27.928% (5827/20864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 652/1563 Loss: 2.092 | Acc: 27.934% (5837/20896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 653/1563 Loss: 2.091 | Acc: 27.939% (5847/20928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 654/1563 Loss: 2.092 | Acc: 27.934% (5855/20960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 655/1563 Loss: 2.092 | Acc: 27.939% (5865/20992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 656/1563 Loss: 2.092 | Acc: 27.930% (5872/21024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 657/1563 Loss: 2.092 | Acc: 27.926% (5880/21056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 658/1563 Loss: 2.093 | Acc: 27.916% (5887/21088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 659/1563 Loss: 2.093 | Acc: 27.926% (5898/21120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 660/1563 Loss: 2.093 | Acc: 27.917% (5905/21152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 661/1563 Loss: 2.094 | Acc: 27.917% (5914/21184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 662/1563 Loss: 2.094 | Acc: 27.922% (5924/21216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 663/1563 Loss: 2.094 | Acc: 27.918% (5932/21248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 664/1563 Loss: 2.094 | Acc: 27.904% (5938/21280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 665/1563 Loss: 2.094 | Acc: 27.909% (5948/21312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 666/1563 Loss: 2.093 | Acc: 27.933% (5962/21344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 667/1563 Loss: 2.093 | Acc: 27.952% (5975/21376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 668/1563 Loss: 2.093 | Acc: 27.962% (5986/21408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 669/1563 Loss: 2.093 | Acc: 27.976% (5998/21440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 670/1563 Loss: 2.093 | Acc: 27.976% (6007/21472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 671/1563 Loss: 2.093 | Acc: 27.976% (6016/21504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 672/1563 Loss: 2.093 | Acc: 27.972% (6024/21536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 673/1563 Loss: 2.093 | Acc: 27.977% (6034/21568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 674/1563 Loss: 2.093 | Acc: 27.958% (6039/21600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 675/1563 Loss: 2.093 | Acc: 27.963% (6049/21632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 676/1563 Loss: 2.093 | Acc: 27.959% (6057/21664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 677/1563 Loss: 2.093 | Acc: 27.964% (6067/21696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 678/1563 Loss: 2.092 | Acc: 27.959% (6075/21728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 679/1563 Loss: 2.092 | Acc: 27.969% (6086/21760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 680/1563 Loss: 2.092 | Acc: 27.983% (6098/21792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 681/1563 Loss: 2.092 | Acc: 27.997% (6110/21824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 682/1563 Loss: 2.092 | Acc: 27.992% (6118/21856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 683/1563 Loss: 2.092 | Acc: 28.011% (6131/21888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 684/1563 Loss: 2.092 | Acc: 28.020% (6142/21920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 685/1563 Loss: 2.091 | Acc: 28.043% (6156/21952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 686/1563 Loss: 2.091 | Acc: 28.039% (6164/21984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 687/1563 Loss: 2.091 | Acc: 28.048% (6175/22016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 688/1563 Loss: 2.090 | Acc: 28.062% (6187/22048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 689/1563 Loss: 2.090 | Acc: 28.071% (6198/22080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 690/1563 Loss: 2.089 | Acc: 28.075% (6208/22112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 691/1563 Loss: 2.089 | Acc: 28.098% (6222/22144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 692/1563 Loss: 2.089 | Acc: 28.120% (6236/22176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 693/1563 Loss: 2.089 | Acc: 28.120% (6245/22208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 694/1563 Loss: 2.088 | Acc: 28.112% (6252/22240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 695/1563 Loss: 2.088 | Acc: 28.098% (6258/22272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 696/1563 Loss: 2.088 | Acc: 28.085% (6264/22304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 697/1563 Loss: 2.089 | Acc: 28.071% (6270/22336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 698/1563 Loss: 2.089 | Acc: 28.053% (6275/22368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 699/1563 Loss: 2.088 | Acc: 28.067% (6287/22400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 700/1563 Loss: 2.088 | Acc: 28.045% (6291/22432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 701/1563 Loss: 2.088 | Acc: 28.058% (6303/22464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 702/1563 Loss: 2.088 | Acc: 28.072% (6315/22496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 703/1563 Loss: 2.088 | Acc: 28.076% (6325/22528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 704/1563 Loss: 2.088 | Acc: 28.067% (6332/22560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 705/1563 Loss: 2.088 | Acc: 28.059% (6339/22592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 706/1563 Loss: 2.088 | Acc: 28.050% (6346/22624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 707/1563 Loss: 2.088 | Acc: 28.068% (6359/22656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 708/1563 Loss: 2.088 | Acc: 28.081% (6371/22688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 709/1563 Loss: 2.088 | Acc: 28.050% (6373/22720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 710/1563 Loss: 2.088 | Acc: 28.059% (6384/22752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 711/1563 Loss: 2.088 | Acc: 28.059% (6393/22784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 712/1563 Loss: 2.089 | Acc: 28.042% (6398/22816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 713/1563 Loss: 2.089 | Acc: 28.037% (6406/22848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 714/1563 Loss: 2.089 | Acc: 28.033% (6414/22880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 715/1563 Loss: 2.089 | Acc: 28.025% (6421/22912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 716/1563 Loss: 2.089 | Acc: 28.038% (6433/22944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 717/1563 Loss: 2.089 | Acc: 28.047% (6444/22976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 718/1563 Loss: 2.089 | Acc: 28.055% (6455/23008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 719/1563 Loss: 2.089 | Acc: 28.047% (6462/23040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 720/1563 Loss: 2.089 | Acc: 28.051% (6472/23072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 721/1563 Loss: 2.089 | Acc: 28.056% (6482/23104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 722/1563 Loss: 2.089 | Acc: 28.056% (6491/23136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 723/1563 Loss: 2.089 | Acc: 28.052% (6499/23168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 724/1563 Loss: 2.089 | Acc: 28.069% (6512/23200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 725/1563 Loss: 2.088 | Acc: 28.086% (6525/23232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 726/1563 Loss: 2.088 | Acc: 28.078% (6532/23264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 727/1563 Loss: 2.089 | Acc: 28.065% (6538/23296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 728/1563 Loss: 2.088 | Acc: 28.074% (6549/23328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 729/1563 Loss: 2.088 | Acc: 28.069% (6557/23360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 730/1563 Loss: 2.089 | Acc: 28.061% (6564/23392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 731/1563 Loss: 2.088 | Acc: 28.078% (6577/23424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 732/1563 Loss: 2.089 | Acc: 28.074% (6585/23456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 733/1563 Loss: 2.089 | Acc: 28.078% (6595/23488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 734/1563 Loss: 2.089 | Acc: 28.082% (6605/23520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 735/1563 Loss: 2.089 | Acc: 28.095% (6617/23552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 736/1563 Loss: 2.089 | Acc: 28.091% (6625/23584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 737/1563 Loss: 2.089 | Acc: 28.112% (6639/23616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 738/1563 Loss: 2.089 | Acc: 28.121% (6650/23648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 739/1563 Loss: 2.089 | Acc: 28.125% (6660/23680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 740/1563 Loss: 2.089 | Acc: 28.108% (6665/23712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 741/1563 Loss: 2.089 | Acc: 28.117% (6676/23744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 742/1563 Loss: 2.089 | Acc: 28.125% (6687/23776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 743/1563 Loss: 2.088 | Acc: 28.159% (6704/23808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 744/1563 Loss: 2.087 | Acc: 28.175% (6717/23840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 745/1563 Loss: 2.088 | Acc: 28.179% (6727/23872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 746/1563 Loss: 2.087 | Acc: 28.221% (6746/23904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 747/1563 Loss: 2.087 | Acc: 28.196% (6749/23936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 748/1563 Loss: 2.088 | Acc: 28.183% (6755/23968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 749/1563 Loss: 2.088 | Acc: 28.192% (6766/24000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 750/1563 Loss: 2.088 | Acc: 28.183% (6773/24032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 751/1563 Loss: 2.088 | Acc: 28.187% (6783/24064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 752/1563 Loss: 2.088 | Acc: 28.196% (6794/24096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 753/1563 Loss: 2.088 | Acc: 28.175% (6798/24128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 754/1563 Loss: 2.088 | Acc: 28.179% (6808/24160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 755/1563 Loss: 2.089 | Acc: 28.170% (6815/24192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 756/1563 Loss: 2.089 | Acc: 28.162% (6822/24224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 757/1563 Loss: 2.089 | Acc: 28.141% (6826/24256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 758/1563 Loss: 2.090 | Acc: 28.133% (6833/24288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 759/1563 Loss: 2.090 | Acc: 28.137% (6843/24320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 760/1563 Loss: 2.090 | Acc: 28.146% (6854/24352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 761/1563 Loss: 2.089 | Acc: 28.154% (6865/24384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 762/1563 Loss: 2.089 | Acc: 28.150% (6873/24416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 763/1563 Loss: 2.089 | Acc: 28.141% (6880/24448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 764/1563 Loss: 2.090 | Acc: 28.133% (6887/24480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 765/1563 Loss: 2.089 | Acc: 28.141% (6898/24512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 766/1563 Loss: 2.090 | Acc: 28.129% (6904/24544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 767/1563 Loss: 2.090 | Acc: 28.129% (6913/24576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 768/1563 Loss: 2.090 | Acc: 28.121% (6920/24608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 769/1563 Loss: 2.090 | Acc: 28.129% (6931/24640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 770/1563 Loss: 2.089 | Acc: 28.129% (6940/24672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 771/1563 Loss: 2.089 | Acc: 28.149% (6954/24704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 772/1563 Loss: 2.089 | Acc: 28.157% (6965/24736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 773/1563 Loss: 2.089 | Acc: 28.153% (6973/24768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 774/1563 Loss: 2.090 | Acc: 28.153% (6982/24800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 775/1563 Loss: 2.090 | Acc: 28.161% (6993/24832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 776/1563 Loss: 2.090 | Acc: 28.145% (6998/24864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 777/1563 Loss: 2.090 | Acc: 28.157% (7010/24896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 778/1563 Loss: 2.090 | Acc: 28.161% (7020/24928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 779/1563 Loss: 2.090 | Acc: 28.157% (7028/24960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 780/1563 Loss: 2.091 | Acc: 28.149% (7035/24992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 781/1563 Loss: 2.091 | Acc: 28.133% (7040/25024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 782/1563 Loss: 2.091 | Acc: 28.129% (7048/25056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 783/1563 Loss: 2.092 | Acc: 28.121% (7055/25088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 784/1563 Loss: 2.092 | Acc: 28.109% (7061/25120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 785/1563 Loss: 2.092 | Acc: 28.093% (7066/25152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 786/1563 Loss: 2.092 | Acc: 28.093% (7075/25184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 787/1563 Loss: 2.093 | Acc: 28.093% (7084/25216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 788/1563 Loss: 2.092 | Acc: 28.085% (7091/25248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 789/1563 Loss: 2.092 | Acc: 28.097% (7103/25280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 790/1563 Loss: 2.093 | Acc: 28.089% (7110/25312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 791/1563 Loss: 2.093 | Acc: 28.086% (7118/25344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 792/1563 Loss: 2.093 | Acc: 28.078% (7125/25376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 793/1563 Loss: 2.093 | Acc: 28.090% (7137/25408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 794/1563 Loss: 2.094 | Acc: 28.090% (7146/25440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 795/1563 Loss: 2.093 | Acc: 28.082% (7153/25472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 796/1563 Loss: 2.094 | Acc: 28.074% (7160/25504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 797/1563 Loss: 2.093 | Acc: 28.086% (7172/25536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 798/1563 Loss: 2.094 | Acc: 28.082% (7180/25568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 799/1563 Loss: 2.094 | Acc: 28.078% (7188/25600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 800/1563 Loss: 2.095 | Acc: 28.059% (7192/25632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 801/1563 Loss: 2.095 | Acc: 28.047% (7198/25664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 802/1563 Loss: 2.095 | Acc: 28.063% (7211/25696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 803/1563 Loss: 2.096 | Acc: 28.059% (7219/25728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 804/1563 Loss: 2.096 | Acc: 28.059% (7228/25760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 805/1563 Loss: 2.096 | Acc: 28.063% (7238/25792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 806/1563 Loss: 2.096 | Acc: 28.055% (7245/25824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 807/1563 Loss: 2.096 | Acc: 28.052% (7253/25856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 808/1563 Loss: 2.095 | Acc: 28.052% (7262/25888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 809/1563 Loss: 2.095 | Acc: 28.052% (7271/25920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 810/1563 Loss: 2.095 | Acc: 28.059% (7282/25952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 811/1563 Loss: 2.095 | Acc: 28.056% (7290/25984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 812/1563 Loss: 2.095 | Acc: 28.063% (7301/26016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 813/1563 Loss: 2.095 | Acc: 28.052% (7307/26048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 814/1563 Loss: 2.096 | Acc: 28.048% (7315/26080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 815/1563 Loss: 2.095 | Acc: 28.056% (7326/26112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 816/1563 Loss: 2.095 | Acc: 28.049% (7333/26144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 817/1563 Loss: 2.096 | Acc: 28.033% (7338/26176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 818/1563 Loss: 2.096 | Acc: 28.041% (7349/26208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 819/1563 Loss: 2.096 | Acc: 28.045% (7359/26240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 820/1563 Loss: 2.096 | Acc: 28.049% (7369/26272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 821/1563 Loss: 2.095 | Acc: 28.053% (7379/26304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 822/1563 Loss: 2.095 | Acc: 28.064% (7391/26336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 823/1563 Loss: 2.094 | Acc: 28.064% (7400/26368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 824/1563 Loss: 2.094 | Acc: 28.061% (7408/26400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 825/1563 Loss: 2.094 | Acc: 28.076% (7421/26432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 826/1563 Loss: 2.094 | Acc: 28.061% (7426/26464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 827/1563 Loss: 2.095 | Acc: 28.038% (7429/26496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 828/1563 Loss: 2.095 | Acc: 28.046% (7440/26528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 829/1563 Loss: 2.095 | Acc: 28.061% (7453/26560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 830/1563 Loss: 2.094 | Acc: 28.080% (7467/26592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 831/1563 Loss: 2.095 | Acc: 28.084% (7477/26624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 832/1563 Loss: 2.095 | Acc: 28.080% (7485/26656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 833/1563 Loss: 2.095 | Acc: 28.076% (7493/26688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 834/1563 Loss: 2.095 | Acc: 28.061% (7498/26720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 835/1563 Loss: 2.096 | Acc: 28.061% (7507/26752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 836/1563 Loss: 2.096 | Acc: 28.043% (7511/26784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 837/1563 Loss: 2.096 | Acc: 28.024% (7515/26816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 838/1563 Loss: 2.096 | Acc: 28.036% (7527/26848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 839/1563 Loss: 2.096 | Acc: 28.047% (7539/26880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 840/1563 Loss: 2.096 | Acc: 28.058% (7551/26912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 841/1563 Loss: 2.096 | Acc: 28.062% (7561/26944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 842/1563 Loss: 2.096 | Acc: 28.073% (7573/26976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 843/1563 Loss: 2.096 | Acc: 28.073% (7582/27008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 844/1563 Loss: 2.096 | Acc: 28.073% (7591/27040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 845/1563 Loss: 2.096 | Acc: 28.084% (7603/27072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 846/1563 Loss: 2.096 | Acc: 28.077% (7610/27104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 847/1563 Loss: 2.096 | Acc: 28.081% (7620/27136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 848/1563 Loss: 2.095 | Acc: 28.081% (7629/27168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 849/1563 Loss: 2.095 | Acc: 28.085% (7639/27200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 850/1563 Loss: 2.096 | Acc: 28.074% (7645/27232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 851/1563 Loss: 2.096 | Acc: 28.070% (7653/27264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 852/1563 Loss: 2.096 | Acc: 28.077% (7664/27296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 853/1563 Loss: 2.095 | Acc: 28.088% (7676/27328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 854/1563 Loss: 2.095 | Acc: 28.088% (7685/27360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 855/1563 Loss: 2.095 | Acc: 28.096% (7696/27392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 856/1563 Loss: 2.095 | Acc: 28.089% (7703/27424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 857/1563 Loss: 2.095 | Acc: 28.103% (7716/27456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 858/1563 Loss: 2.095 | Acc: 28.125% (7731/27488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 859/1563 Loss: 2.095 | Acc: 28.125% (7740/27520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 860/1563 Loss: 2.095 | Acc: 28.125% (7749/27552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 861/1563 Loss: 2.095 | Acc: 28.121% (7757/27584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 862/1563 Loss: 2.096 | Acc: 28.114% (7764/27616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 863/1563 Loss: 2.096 | Acc: 28.103% (7770/27648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 864/1563 Loss: 2.097 | Acc: 28.096% (7777/27680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 865/1563 Loss: 2.096 | Acc: 28.085% (7783/27712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 866/1563 Loss: 2.096 | Acc: 28.075% (7789/27744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 867/1563 Loss: 2.097 | Acc: 28.064% (7795/27776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 868/1563 Loss: 2.097 | Acc: 28.057% (7802/27808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 869/1563 Loss: 2.097 | Acc: 28.057% (7811/27840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 870/1563 Loss: 2.097 | Acc: 28.053% (7819/27872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 871/1563 Loss: 2.097 | Acc: 28.053% (7828/27904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 872/1563 Loss: 2.097 | Acc: 28.050% (7836/27936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 873/1563 Loss: 2.097 | Acc: 28.053% (7846/27968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 874/1563 Loss: 2.098 | Acc: 28.046% (7853/28000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 875/1563 Loss: 2.098 | Acc: 28.050% (7863/28032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 876/1563 Loss: 2.098 | Acc: 28.032% (7867/28064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 877/1563 Loss: 2.098 | Acc: 28.022% (7873/28096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 878/1563 Loss: 2.098 | Acc: 28.011% (7879/28128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 879/1563 Loss: 2.098 | Acc: 28.026% (7892/28160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 880/1563 Loss: 2.098 | Acc: 28.029% (7902/28192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 881/1563 Loss: 2.097 | Acc: 28.033% (7912/28224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 882/1563 Loss: 2.098 | Acc: 28.019% (7917/28256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 883/1563 Loss: 2.098 | Acc: 28.015% (7925/28288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 884/1563 Loss: 2.097 | Acc: 28.030% (7938/28320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 885/1563 Loss: 2.097 | Acc: 28.033% (7948/28352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 886/1563 Loss: 2.098 | Acc: 28.040% (7959/28384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 887/1563 Loss: 2.097 | Acc: 28.030% (7965/28416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 888/1563 Loss: 2.097 | Acc: 28.027% (7973/28448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 889/1563 Loss: 2.097 | Acc: 28.027% (7982/28480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 890/1563 Loss: 2.097 | Acc: 28.027% (7991/28512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 891/1563 Loss: 2.097 | Acc: 28.020% (7998/28544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 892/1563 Loss: 2.097 | Acc: 28.031% (8010/28576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 893/1563 Loss: 2.097 | Acc: 28.024% (8017/28608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 894/1563 Loss: 2.097 | Acc: 28.020% (8025/28640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 895/1563 Loss: 2.096 | Acc: 28.031% (8037/28672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 896/1563 Loss: 2.097 | Acc: 28.034% (8047/28704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 897/1563 Loss: 2.097 | Acc: 28.035% (8056/28736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 898/1563 Loss: 2.097 | Acc: 28.031% (8064/28768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 899/1563 Loss: 2.097 | Acc: 28.035% (8074/28800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 900/1563 Loss: 2.097 | Acc: 28.031% (8082/28832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 901/1563 Loss: 2.096 | Acc: 28.035% (8092/28864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 902/1563 Loss: 2.097 | Acc: 28.038% (8102/28896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 903/1563 Loss: 2.097 | Acc: 28.032% (8109/28928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 904/1563 Loss: 2.097 | Acc: 28.032% (8118/28960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 905/1563 Loss: 2.097 | Acc: 28.022% (8124/28992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 906/1563 Loss: 2.097 | Acc: 28.011% (8130/29024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 907/1563 Loss: 2.098 | Acc: 28.008% (8138/29056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 908/1563 Loss: 2.098 | Acc: 28.015% (8149/29088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 909/1563 Loss: 2.098 | Acc: 28.015% (8158/29120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 910/1563 Loss: 2.098 | Acc: 28.008% (8165/29152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 911/1563 Loss: 2.097 | Acc: 27.998% (8171/29184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 912/1563 Loss: 2.098 | Acc: 28.002% (8181/29216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 913/1563 Loss: 2.097 | Acc: 28.009% (8192/29248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 914/1563 Loss: 2.098 | Acc: 28.009% (8201/29280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 915/1563 Loss: 2.097 | Acc: 28.016% (8212/29312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 916/1563 Loss: 2.097 | Acc: 28.033% (8226/29344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 917/1563 Loss: 2.096 | Acc: 28.030% (8234/29376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 918/1563 Loss: 2.096 | Acc: 28.047% (8248/29408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 919/1563 Loss: 2.096 | Acc: 28.043% (8256/29440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 920/1563 Loss: 2.096 | Acc: 28.044% (8265/29472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 921/1563 Loss: 2.097 | Acc: 28.033% (8271/29504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 922/1563 Loss: 2.096 | Acc: 28.030% (8279/29536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 923/1563 Loss: 2.097 | Acc: 28.024% (8286/29568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 924/1563 Loss: 2.097 | Acc: 28.027% (8296/29600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 925/1563 Loss: 2.097 | Acc: 28.037% (8308/29632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 926/1563 Loss: 2.097 | Acc: 28.037% (8317/29664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 927/1563 Loss: 2.097 | Acc: 28.034% (8325/29696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 928/1563 Loss: 2.097 | Acc: 28.031% (8333/29728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 929/1563 Loss: 2.097 | Acc: 28.034% (8343/29760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 930/1563 Loss: 2.097 | Acc: 28.031% (8351/29792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 931/1563 Loss: 2.097 | Acc: 28.028% (8359/29824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 932/1563 Loss: 2.097 | Acc: 28.018% (8365/29856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 933/1563 Loss: 2.097 | Acc: 28.015% (8373/29888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 934/1563 Loss: 2.097 | Acc: 28.008% (8380/29920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 935/1563 Loss: 2.097 | Acc: 28.008% (8389/29952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 936/1563 Loss: 2.096 | Acc: 28.015% (8400/29984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 937/1563 Loss: 2.096 | Acc: 28.012% (8408/30016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 938/1563 Loss: 2.096 | Acc: 28.022% (8420/30048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 939/1563 Loss: 2.096 | Acc: 28.022% (8429/30080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 940/1563 Loss: 2.096 | Acc: 28.019% (8437/30112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 941/1563 Loss: 2.096 | Acc: 28.025% (8448/30144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 942/1563 Loss: 2.096 | Acc: 28.019% (8455/30176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 943/1563 Loss: 2.096 | Acc: 28.019% (8464/30208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 944/1563 Loss: 2.096 | Acc: 28.009% (8470/30240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 945/1563 Loss: 2.096 | Acc: 28.003% (8477/30272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 946/1563 Loss: 2.095 | Acc: 28.010% (8488/30304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 947/1563 Loss: 2.095 | Acc: 28.016% (8499/30336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 948/1563 Loss: 2.095 | Acc: 28.010% (8506/30368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 949/1563 Loss: 2.096 | Acc: 28.007% (8514/30400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 950/1563 Loss: 2.095 | Acc: 28.020% (8527/30432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 951/1563 Loss: 2.095 | Acc: 28.030% (8539/30464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 952/1563 Loss: 2.095 | Acc: 28.033% (8549/30496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 953/1563 Loss: 2.095 | Acc: 28.030% (8557/30528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 954/1563 Loss: 2.095 | Acc: 28.043% (8570/30560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 955/1563 Loss: 2.095 | Acc: 28.050% (8581/30592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 956/1563 Loss: 2.096 | Acc: 28.040% (8587/30624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 957/1563 Loss: 2.096 | Acc: 28.040% (8596/30656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 958/1563 Loss: 2.096 | Acc: 28.050% (8608/30688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 959/1563 Loss: 2.096 | Acc: 28.050% (8617/30720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 960/1563 Loss: 2.096 | Acc: 28.050% (8626/30752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 961/1563 Loss: 2.096 | Acc: 28.044% (8633/30784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 962/1563 Loss: 2.096 | Acc: 28.050% (8644/30816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 963/1563 Loss: 2.096 | Acc: 28.054% (8654/30848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 964/1563 Loss: 2.095 | Acc: 28.063% (8666/30880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 965/1563 Loss: 2.095 | Acc: 28.073% (8678/30912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 966/1563 Loss: 2.095 | Acc: 28.064% (8684/30944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 967/1563 Loss: 2.095 | Acc: 28.051% (8689/30976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 968/1563 Loss: 2.095 | Acc: 28.051% (8698/31008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 969/1563 Loss: 2.095 | Acc: 28.044% (8705/31040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 970/1563 Loss: 2.096 | Acc: 28.045% (8714/31072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 971/1563 Loss: 2.096 | Acc: 28.048% (8724/31104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 972/1563 Loss: 2.095 | Acc: 28.048% (8733/31136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 973/1563 Loss: 2.095 | Acc: 28.064% (8747/31168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 974/1563 Loss: 2.095 | Acc: 28.064% (8756/31200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 975/1563 Loss: 2.095 | Acc: 28.067% (8766/31232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 976/1563 Loss: 2.096 | Acc: 28.064% (8774/31264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 977/1563 Loss: 2.095 | Acc: 28.074% (8786/31296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 978/1563 Loss: 2.095 | Acc: 28.080% (8797/31328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 979/1563 Loss: 2.095 | Acc: 28.077% (8805/31360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 980/1563 Loss: 2.095 | Acc: 28.080% (8815/31392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 981/1563 Loss: 2.096 | Acc: 28.074% (8822/31424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 982/1563 Loss: 2.096 | Acc: 28.068% (8829/31456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 983/1563 Loss: 2.096 | Acc: 28.058% (8835/31488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 984/1563 Loss: 2.096 | Acc: 28.071% (8848/31520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 985/1563 Loss: 2.096 | Acc: 28.077% (8859/31552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 986/1563 Loss: 2.096 | Acc: 28.081% (8869/31584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 987/1563 Loss: 2.095 | Acc: 28.081% (8878/31616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 988/1563 Loss: 2.095 | Acc: 28.081% (8887/31648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 989/1563 Loss: 2.096 | Acc: 28.074% (8894/31680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 990/1563 Loss: 2.095 | Acc: 28.075% (8903/31712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 991/1563 Loss: 2.096 | Acc: 28.059% (8907/31744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 992/1563 Loss: 2.096 | Acc: 28.056% (8915/31776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 993/1563 Loss: 2.095 | Acc: 28.062% (8926/31808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 994/1563 Loss: 2.096 | Acc: 28.059% (8934/31840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 995/1563 Loss: 2.096 | Acc: 28.059% (8943/31872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 996/1563 Loss: 2.096 | Acc: 28.044% (8947/31904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 997/1563 Loss: 2.097 | Acc: 28.028% (8951/31936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 998/1563 Loss: 2.097 | Acc: 28.016% (8956/31968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 999/1563 Loss: 2.097 | Acc: 28.000% (8960/32000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1000/1563 Loss: 2.096 | Acc: 27.997% (8968/32032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1001/1563 Loss: 2.096 | Acc: 28.006% (8980/32064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1002/1563 Loss: 2.096 | Acc: 28.000% (8987/32096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1003/1563 Loss: 2.096 | Acc: 28.004% (8997/32128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1004/1563 Loss: 2.096 | Acc: 27.998% (9004/32160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1005/1563 Loss: 2.096 | Acc: 27.985% (9009/32192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1006/1563 Loss: 2.097 | Acc: 27.985% (9018/32224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1007/1563 Loss: 2.096 | Acc: 27.998% (9031/32256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1008/1563 Loss: 2.096 | Acc: 28.001% (9041/32288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1009/1563 Loss: 2.096 | Acc: 28.004% (9051/32320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1010/1563 Loss: 2.096 | Acc: 28.008% (9061/32352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1011/1563 Loss: 2.096 | Acc: 28.017% (9073/32384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1012/1563 Loss: 2.097 | Acc: 28.002% (9077/32416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1013/1563 Loss: 2.096 | Acc: 27.996% (9084/32448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1014/1563 Loss: 2.096 | Acc: 28.005% (9096/32480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1015/1563 Loss: 2.096 | Acc: 28.005% (9105/32512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1016/1563 Loss: 2.096 | Acc: 28.011% (9116/32544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1017/1563 Loss: 2.096 | Acc: 28.008% (9124/32576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1018/1563 Loss: 2.096 | Acc: 28.008% (9133/32608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1019/1563 Loss: 2.096 | Acc: 28.009% (9142/32640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1020/1563 Loss: 2.096 | Acc: 28.006% (9150/32672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1021/1563 Loss: 2.096 | Acc: 27.997% (9156/32704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1022/1563 Loss: 2.096 | Acc: 28.003% (9167/32736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1023/1563 Loss: 2.097 | Acc: 28.000% (9175/32768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1024/1563 Loss: 2.096 | Acc: 28.003% (9185/32800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1025/1563 Loss: 2.096 | Acc: 28.006% (9195/32832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1026/1563 Loss: 2.096 | Acc: 28.006% (9204/32864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1027/1563 Loss: 2.096 | Acc: 28.000% (9211/32896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1028/1563 Loss: 2.097 | Acc: 28.004% (9221/32928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1029/1563 Loss: 2.097 | Acc: 28.001% (9229/32960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1030/1563 Loss: 2.097 | Acc: 27.998% (9237/32992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1031/1563 Loss: 2.097 | Acc: 27.992% (9244/33024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1032/1563 Loss: 2.097 | Acc: 27.989% (9252/33056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1033/1563 Loss: 2.097 | Acc: 27.995% (9263/33088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1034/1563 Loss: 2.097 | Acc: 27.986% (9269/33120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1035/1563 Loss: 2.097 | Acc: 27.992% (9280/33152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1036/1563 Loss: 2.097 | Acc: 27.977% (9284/33184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1037/1563 Loss: 2.097 | Acc: 27.984% (9295/33216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1038/1563 Loss: 2.096 | Acc: 27.993% (9307/33248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1039/1563 Loss: 2.096 | Acc: 28.005% (9320/33280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1040/1563 Loss: 2.096 | Acc: 27.999% (9327/33312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1041/1563 Loss: 2.096 | Acc: 27.999% (9336/33344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1042/1563 Loss: 2.096 | Acc: 27.996% (9344/33376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1043/1563 Loss: 2.096 | Acc: 27.987% (9350/33408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1044/1563 Loss: 2.096 | Acc: 27.990% (9360/33440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1045/1563 Loss: 2.096 | Acc: 27.991% (9369/33472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1046/1563 Loss: 2.096 | Acc: 27.988% (9377/33504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1047/1563 Loss: 2.096 | Acc: 27.982% (9384/33536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1048/1563 Loss: 2.096 | Acc: 27.982% (9393/33568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1049/1563 Loss: 2.096 | Acc: 27.991% (9405/33600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1050/1563 Loss: 2.096 | Acc: 28.000% (9417/33632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1051/1563 Loss: 2.096 | Acc: 28.003% (9427/33664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1052/1563 Loss: 2.095 | Acc: 28.006% (9437/33696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1053/1563 Loss: 2.095 | Acc: 28.006% (9446/33728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1054/1563 Loss: 2.095 | Acc: 28.001% (9453/33760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1055/1563 Loss: 2.096 | Acc: 27.986% (9457/33792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1056/1563 Loss: 2.096 | Acc: 27.986% (9466/33824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1057/1563 Loss: 2.096 | Acc: 27.974% (9471/33856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1058/1563 Loss: 2.096 | Acc: 27.966% (9477/33888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1059/1563 Loss: 2.096 | Acc: 27.963% (9485/33920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1060/1563 Loss: 2.096 | Acc: 27.954% (9491/33952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1061/1563 Loss: 2.096 | Acc: 27.946% (9497/33984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1062/1563 Loss: 2.096 | Acc: 27.943% (9505/34016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1063/1563 Loss: 2.096 | Acc: 27.940% (9513/34048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1064/1563 Loss: 2.097 | Acc: 27.934% (9520/34080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1065/1563 Loss: 2.097 | Acc: 27.929% (9527/34112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1066/1563 Loss: 2.097 | Acc: 27.932% (9537/34144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1067/1563 Loss: 2.096 | Acc: 27.947% (9551/34176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1068/1563 Loss: 2.097 | Acc: 27.947% (9560/34208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1069/1563 Loss: 2.097 | Acc: 27.941% (9567/34240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1070/1563 Loss: 2.097 | Acc: 27.924% (9570/34272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1071/1563 Loss: 2.098 | Acc: 27.915% (9576/34304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1072/1563 Loss: 2.097 | Acc: 27.915% (9585/34336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1073/1563 Loss: 2.098 | Acc: 27.904% (9590/34368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1074/1563 Loss: 2.098 | Acc: 27.907% (9600/34400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1075/1563 Loss: 2.098 | Acc: 27.898% (9606/34432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1076/1563 Loss: 2.097 | Acc: 27.907% (9618/34464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1077/1563 Loss: 2.097 | Acc: 27.899% (9624/34496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1078/1563 Loss: 2.097 | Acc: 27.916% (9639/34528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1079/1563 Loss: 2.097 | Acc: 27.931% (9653/34560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1080/1563 Loss: 2.097 | Acc: 27.926% (9660/34592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1081/1563 Loss: 2.097 | Acc: 27.911% (9664/34624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1082/1563 Loss: 2.097 | Acc: 27.909% (9672/34656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1083/1563 Loss: 2.097 | Acc: 27.903% (9679/34688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1084/1563 Loss: 2.097 | Acc: 27.897% (9686/34720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1085/1563 Loss: 2.097 | Acc: 27.901% (9696/34752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1086/1563 Loss: 2.097 | Acc: 27.889% (9701/34784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1087/1563 Loss: 2.097 | Acc: 27.898% (9713/34816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1088/1563 Loss: 2.097 | Acc: 27.901% (9723/34848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1089/1563 Loss: 2.097 | Acc: 27.910% (9735/34880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1090/1563 Loss: 2.097 | Acc: 27.904% (9742/34912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1091/1563 Loss: 2.097 | Acc: 27.908% (9752/34944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1092/1563 Loss: 2.097 | Acc: 27.919% (9765/34976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1093/1563 Loss: 2.097 | Acc: 27.919% (9774/35008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1094/1563 Loss: 2.097 | Acc: 27.920% (9783/35040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1095/1563 Loss: 2.096 | Acc: 27.925% (9794/35072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1096/1563 Loss: 2.097 | Acc: 27.917% (9800/35104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1097/1563 Loss: 2.096 | Acc: 27.914% (9808/35136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1098/1563 Loss: 2.096 | Acc: 27.912% (9816/35168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1099/1563 Loss: 2.096 | Acc: 27.898% (9820/35200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1100/1563 Loss: 2.096 | Acc: 27.895% (9828/35232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1101/1563 Loss: 2.096 | Acc: 27.901% (9839/35264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1102/1563 Loss: 2.096 | Acc: 27.910% (9851/35296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1103/1563 Loss: 2.096 | Acc: 27.913% (9861/35328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1104/1563 Loss: 2.096 | Acc: 27.904% (9867/35360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1105/1563 Loss: 2.096 | Acc: 27.913% (9879/35392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1106/1563 Loss: 2.096 | Acc: 27.913% (9888/35424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1107/1563 Loss: 2.096 | Acc: 27.913% (9897/35456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1108/1563 Loss: 2.096 | Acc: 27.911% (9905/35488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1109/1563 Loss: 2.096 | Acc: 27.925% (9919/35520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1110/1563 Loss: 2.096 | Acc: 27.925% (9928/35552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1111/1563 Loss: 2.096 | Acc: 27.928% (9938/35584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1112/1563 Loss: 2.096 | Acc: 27.926% (9946/35616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1113/1563 Loss: 2.096 | Acc: 27.934% (9958/35648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1114/1563 Loss: 2.096 | Acc: 27.937% (9968/35680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1115/1563 Loss: 2.096 | Acc: 27.943% (9979/35712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1116/1563 Loss: 2.095 | Acc: 27.957% (9993/35744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1117/1563 Loss: 2.095 | Acc: 27.943% (9997/35776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1118/1563 Loss: 2.096 | Acc: 27.941% (10005/35808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1119/1563 Loss: 2.096 | Acc: 27.930% (10010/35840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1120/1563 Loss: 2.096 | Acc: 27.930% (10019/35872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1121/1563 Loss: 2.096 | Acc: 27.924% (10026/35904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1122/1563 Loss: 2.095 | Acc: 27.933% (10038/35936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1123/1563 Loss: 2.096 | Acc: 27.928% (10045/35968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1124/1563 Loss: 2.096 | Acc: 27.919% (10051/36000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1125/1563 Loss: 2.096 | Acc: 27.922% (10061/36032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1126/1563 Loss: 2.096 | Acc: 27.925% (10071/36064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1127/1563 Loss: 2.097 | Acc: 27.923% (10079/36096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1128/1563 Loss: 2.096 | Acc: 27.931% (10091/36128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1129/1563 Loss: 2.097 | Acc: 27.931% (10100/36160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1130/1563 Loss: 2.097 | Acc: 27.929% (10108/36192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1131/1563 Loss: 2.097 | Acc: 27.929% (10117/36224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1132/1563 Loss: 2.097 | Acc: 27.929% (10126/36256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1133/1563 Loss: 2.098 | Acc: 27.913% (10129/36288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1134/1563 Loss: 2.098 | Acc: 27.924% (10142/36320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1135/1563 Loss: 2.098 | Acc: 27.930% (10153/36352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1136/1563 Loss: 2.098 | Acc: 27.927% (10161/36384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1137/1563 Loss: 2.098 | Acc: 27.941% (10175/36416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1138/1563 Loss: 2.097 | Acc: 27.944% (10185/36448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1139/1563 Loss: 2.097 | Acc: 27.939% (10192/36480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1140/1563 Loss: 2.097 | Acc: 27.939% (10201/36512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1141/1563 Loss: 2.097 | Acc: 27.947% (10213/36544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1142/1563 Loss: 2.097 | Acc: 27.950% (10223/36576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1143/1563 Loss: 2.097 | Acc: 27.947% (10231/36608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1144/1563 Loss: 2.097 | Acc: 27.945% (10239/36640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1145/1563 Loss: 2.097 | Acc: 27.950% (10250/36672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1146/1563 Loss: 2.097 | Acc: 27.962% (10263/36704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1147/1563 Loss: 2.097 | Acc: 27.959% (10271/36736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1148/1563 Loss: 2.096 | Acc: 27.973% (10285/36768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1149/1563 Loss: 2.096 | Acc: 27.992% (10301/36800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1150/1563 Loss: 2.096 | Acc: 27.984% (10307/36832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1151/1563 Loss: 2.097 | Acc: 27.989% (10318/36864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1152/1563 Loss: 2.097 | Acc: 27.989% (10327/36896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1153/1563 Loss: 2.097 | Acc: 27.995% (10338/36928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1154/1563 Loss: 2.097 | Acc: 28.003% (10350/36960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1155/1563 Loss: 2.097 | Acc: 28.003% (10359/36992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1156/1563 Loss: 2.096 | Acc: 28.006% (10369/37024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1157/1563 Loss: 2.097 | Acc: 27.998% (10375/37056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1158/1563 Loss: 2.096 | Acc: 28.009% (10388/37088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1159/1563 Loss: 2.096 | Acc: 28.004% (10395/37120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1160/1563 Loss: 2.096 | Acc: 27.993% (10400/37152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1161/1563 Loss: 2.096 | Acc: 27.993% (10409/37184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1162/1563 Loss: 2.097 | Acc: 27.988% (10416/37216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1163/1563 Loss: 2.097 | Acc: 27.983% (10423/37248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1164/1563 Loss: 2.097 | Acc: 27.975% (10429/37280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1165/1563 Loss: 2.097 | Acc: 27.978% (10439/37312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1166/1563 Loss: 2.097 | Acc: 27.986% (10451/37344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1167/1563 Loss: 2.097 | Acc: 27.981% (10458/37376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1168/1563 Loss: 2.097 | Acc: 27.981% (10467/37408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1169/1563 Loss: 2.097 | Acc: 27.994% (10481/37440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1170/1563 Loss: 2.098 | Acc: 27.986% (10487/37472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1171/1563 Loss: 2.097 | Acc: 27.989% (10497/37504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1172/1563 Loss: 2.097 | Acc: 27.984% (10504/37536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1173/1563 Loss: 2.097 | Acc: 27.989% (10515/37568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1174/1563 Loss: 2.098 | Acc: 27.981% (10521/37600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1175/1563 Loss: 2.097 | Acc: 27.997% (10536/37632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1176/1563 Loss: 2.097 | Acc: 28.003% (10547/37664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1177/1563 Loss: 2.097 | Acc: 28.006% (10557/37696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1178/1563 Loss: 2.097 | Acc: 28.008% (10567/37728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1179/1563 Loss: 2.097 | Acc: 28.001% (10573/37760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1180/1563 Loss: 2.097 | Acc: 28.001% (10582/37792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1181/1563 Loss: 2.097 | Acc: 27.988% (10586/37824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1182/1563 Loss: 2.097 | Acc: 27.980% (10592/37856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1183/1563 Loss: 2.097 | Acc: 27.982% (10602/37888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1184/1563 Loss: 2.098 | Acc: 27.975% (10608/37920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1185/1563 Loss: 2.098 | Acc: 27.967% (10614/37952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1186/1563 Loss: 2.098 | Acc: 27.959% (10620/37984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1187/1563 Loss: 2.098 | Acc: 27.962% (10630/38016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1188/1563 Loss: 2.098 | Acc: 27.957% (10637/38048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1189/1563 Loss: 2.097 | Acc: 27.957% (10646/38080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1190/1563 Loss: 2.097 | Acc: 27.957% (10655/38112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1191/1563 Loss: 2.097 | Acc: 27.952% (10662/38144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1192/1563 Loss: 2.097 | Acc: 27.947% (10669/38176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1193/1563 Loss: 2.097 | Acc: 27.950% (10679/38208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1194/1563 Loss: 2.097 | Acc: 27.945% (10686/38240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1195/1563 Loss: 2.097 | Acc: 27.932% (10690/38272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1196/1563 Loss: 2.097 | Acc: 27.937% (10701/38304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1197/1563 Loss: 2.097 | Acc: 27.932% (10708/38336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1198/1563 Loss: 2.097 | Acc: 27.932% (10717/38368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1199/1563 Loss: 2.097 | Acc: 27.927% (10724/38400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1200/1563 Loss: 2.097 | Acc: 27.925% (10732/38432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1201/1563 Loss: 2.097 | Acc: 27.917% (10738/38464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1202/1563 Loss: 2.097 | Acc: 27.925% (10750/38496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1203/1563 Loss: 2.097 | Acc: 27.928% (10760/38528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1204/1563 Loss: 2.097 | Acc: 27.933% (10771/38560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1205/1563 Loss: 2.097 | Acc: 27.931% (10779/38592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1206/1563 Loss: 2.097 | Acc: 27.936% (10790/38624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1207/1563 Loss: 2.097 | Acc: 27.941% (10801/38656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1208/1563 Loss: 2.097 | Acc: 27.944% (10811/38688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1209/1563 Loss: 2.097 | Acc: 27.934% (10816/38720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1210/1563 Loss: 2.097 | Acc: 27.937% (10826/38752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1211/1563 Loss: 2.097 | Acc: 27.926% (10831/38784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1212/1563 Loss: 2.096 | Acc: 27.927% (10840/38816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1213/1563 Loss: 2.096 | Acc: 27.924% (10848/38848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1214/1563 Loss: 2.096 | Acc: 27.937% (10862/38880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1215/1563 Loss: 2.096 | Acc: 27.937% (10871/38912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1216/1563 Loss: 2.096 | Acc: 27.940% (10881/38944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1217/1563 Loss: 2.096 | Acc: 27.940% (10890/38976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1218/1563 Loss: 2.096 | Acc: 27.940% (10899/39008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1219/1563 Loss: 2.096 | Acc: 27.938% (10907/39040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1220/1563 Loss: 2.096 | Acc: 27.941% (10917/39072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1221/1563 Loss: 2.096 | Acc: 27.938% (10925/39104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1222/1563 Loss: 2.096 | Acc: 27.951% (10939/39136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1223/1563 Loss: 2.096 | Acc: 27.944% (10945/39168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1224/1563 Loss: 2.096 | Acc: 27.949% (10956/39200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1225/1563 Loss: 2.095 | Acc: 27.947% (10964/39232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1226/1563 Loss: 2.095 | Acc: 27.942% (10971/39264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1227/1563 Loss: 2.095 | Acc: 27.937% (10978/39296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1228/1563 Loss: 2.095 | Acc: 27.942% (10989/39328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1229/1563 Loss: 2.095 | Acc: 27.945% (10999/39360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1230/1563 Loss: 2.095 | Acc: 27.950% (11010/39392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1231/1563 Loss: 2.095 | Acc: 27.945% (11017/39424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1232/1563 Loss: 2.095 | Acc: 27.950% (11028/39456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1233/1563 Loss: 2.095 | Acc: 27.953% (11038/39488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1234/1563 Loss: 2.095 | Acc: 27.948% (11045/39520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1235/1563 Loss: 2.095 | Acc: 27.940% (11051/39552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1236/1563 Loss: 2.095 | Acc: 27.941% (11060/39584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1237/1563 Loss: 2.095 | Acc: 27.953% (11074/39616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1238/1563 Loss: 2.095 | Acc: 27.956% (11084/39648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1239/1563 Loss: 2.095 | Acc: 27.959% (11094/39680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1240/1563 Loss: 2.095 | Acc: 27.961% (11104/39712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1241/1563 Loss: 2.095 | Acc: 27.961% (11113/39744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1242/1563 Loss: 2.095 | Acc: 27.957% (11120/39776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1243/1563 Loss: 2.095 | Acc: 27.957% (11129/39808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1244/1563 Loss: 2.095 | Acc: 27.959% (11139/39840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1245/1563 Loss: 2.095 | Acc: 27.952% (11145/39872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1246/1563 Loss: 2.095 | Acc: 27.957% (11156/39904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1247/1563 Loss: 2.095 | Acc: 27.965% (11168/39936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1248/1563 Loss: 2.095 | Acc: 27.967% (11178/39968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1249/1563 Loss: 2.095 | Acc: 27.965% (11186/40000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1250/1563 Loss: 2.095 | Acc: 27.960% (11193/40032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1251/1563 Loss: 2.095 | Acc: 27.958% (11201/40064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1252/1563 Loss: 2.095 | Acc: 27.953% (11208/40096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1253/1563 Loss: 2.095 | Acc: 27.951% (11216/40128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1254/1563 Loss: 2.095 | Acc: 27.948% (11224/40160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1255/1563 Loss: 2.095 | Acc: 27.948% (11233/40192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1256/1563 Loss: 2.095 | Acc: 27.939% (11238/40224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1257/1563 Loss: 2.095 | Acc: 27.939% (11247/40256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1258/1563 Loss: 2.095 | Acc: 27.934% (11254/40288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1259/1563 Loss: 2.095 | Acc: 27.932% (11262/40320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1260/1563 Loss: 2.096 | Acc: 27.939% (11274/40352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1261/1563 Loss: 2.096 | Acc: 27.944% (11285/40384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1262/1563 Loss: 2.096 | Acc: 27.944% (11294/40416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1263/1563 Loss: 2.096 | Acc: 27.952% (11306/40448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1264/1563 Loss: 2.096 | Acc: 27.955% (11316/40480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1265/1563 Loss: 2.096 | Acc: 27.945% (11321/40512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1266/1563 Loss: 2.096 | Acc: 27.950% (11332/40544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1267/1563 Loss: 2.095 | Acc: 27.955% (11343/40576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1268/1563 Loss: 2.096 | Acc: 27.940% (11346/40608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1269/1563 Loss: 2.096 | Acc: 27.938% (11354/40640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1270/1563 Loss: 2.096 | Acc: 27.941% (11364/40672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1271/1563 Loss: 2.096 | Acc: 27.951% (11377/40704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1272/1563 Loss: 2.096 | Acc: 27.953% (11387/40736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1273/1563 Loss: 2.096 | Acc: 27.943% (11392/40768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1274/1563 Loss: 2.096 | Acc: 27.956% (11406/40800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1275/1563 Loss: 2.096 | Acc: 27.954% (11414/40832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1276/1563 Loss: 2.096 | Acc: 27.954% (11423/40864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1277/1563 Loss: 2.096 | Acc: 27.939% (11426/40896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1278/1563 Loss: 2.096 | Acc: 27.942% (11436/40928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1279/1563 Loss: 2.096 | Acc: 27.942% (11445/40960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1280/1563 Loss: 2.096 | Acc: 27.949% (11457/40992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1281/1563 Loss: 2.096 | Acc: 27.942% (11463/41024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1282/1563 Loss: 2.097 | Acc: 27.947% (11474/41056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1283/1563 Loss: 2.097 | Acc: 27.950% (11484/41088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1284/1563 Loss: 2.097 | Acc: 27.947% (11492/41120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1285/1563 Loss: 2.097 | Acc: 27.940% (11498/41152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1286/1563 Loss: 2.097 | Acc: 27.940% (11507/41184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1287/1563 Loss: 2.097 | Acc: 27.945% (11518/41216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1288/1563 Loss: 2.097 | Acc: 27.958% (11532/41248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1289/1563 Loss: 2.096 | Acc: 27.968% (11545/41280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1290/1563 Loss: 2.096 | Acc: 27.963% (11552/41312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1291/1563 Loss: 2.097 | Acc: 27.953% (11557/41344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1292/1563 Loss: 2.096 | Acc: 27.956% (11567/41376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1293/1563 Loss: 2.096 | Acc: 27.958% (11577/41408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1294/1563 Loss: 2.096 | Acc: 27.954% (11584/41440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1295/1563 Loss: 2.096 | Acc: 27.968% (11599/41472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1296/1563 Loss: 2.096 | Acc: 27.971% (11609/41504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1297/1563 Loss: 2.096 | Acc: 27.966% (11616/41536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1298/1563 Loss: 2.096 | Acc: 27.966% (11625/41568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1299/1563 Loss: 2.097 | Acc: 27.962% (11632/41600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1300/1563 Loss: 2.097 | Acc: 27.957% (11639/41632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1301/1563 Loss: 2.096 | Acc: 27.959% (11649/41664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1302/1563 Loss: 2.096 | Acc: 27.964% (11660/41696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1303/1563 Loss: 2.096 | Acc: 27.962% (11668/41728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1304/1563 Loss: 2.096 | Acc: 27.965% (11678/41760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1305/1563 Loss: 2.096 | Acc: 27.965% (11687/41792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1306/1563 Loss: 2.096 | Acc: 27.960% (11694/41824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1307/1563 Loss: 2.096 | Acc: 27.967% (11706/41856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1308/1563 Loss: 2.096 | Acc: 27.972% (11717/41888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1309/1563 Loss: 2.096 | Acc: 27.970% (11725/41920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1310/1563 Loss: 2.096 | Acc: 27.965% (11732/41952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1311/1563 Loss: 2.096 | Acc: 27.965% (11741/41984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1312/1563 Loss: 2.096 | Acc: 27.966% (11750/42016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1313/1563 Loss: 2.096 | Acc: 27.970% (11761/42048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1314/1563 Loss: 2.096 | Acc: 27.978% (11773/42080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1315/1563 Loss: 2.096 | Acc: 27.983% (11784/42112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1316/1563 Loss: 2.096 | Acc: 27.978% (11791/42144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1317/1563 Loss: 2.096 | Acc: 27.976% (11799/42176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1318/1563 Loss: 2.096 | Acc: 27.978% (11809/42208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1319/1563 Loss: 2.096 | Acc: 27.969% (11814/42240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1320/1563 Loss: 2.096 | Acc: 27.962% (11820/42272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1321/1563 Loss: 2.096 | Acc: 27.967% (11831/42304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1322/1563 Loss: 2.096 | Acc: 27.967% (11840/42336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1323/1563 Loss: 2.096 | Acc: 27.969% (11850/42368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1324/1563 Loss: 2.096 | Acc: 27.969% (11859/42400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1325/1563 Loss: 2.095 | Acc: 27.965% (11866/42432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1326/1563 Loss: 2.095 | Acc: 27.967% (11876/42464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1327/1563 Loss: 2.095 | Acc: 27.967% (11885/42496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1328/1563 Loss: 2.095 | Acc: 27.972% (11896/42528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1329/1563 Loss: 2.095 | Acc: 27.968% (11903/42560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1330/1563 Loss: 2.095 | Acc: 27.965% (11911/42592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1331/1563 Loss: 2.095 | Acc: 27.973% (11923/42624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1332/1563 Loss: 2.095 | Acc: 27.977% (11934/42656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1333/1563 Loss: 2.095 | Acc: 27.980% (11944/42688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1334/1563 Loss: 2.095 | Acc: 27.982% (11954/42720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1335/1563 Loss: 2.095 | Acc: 27.978% (11961/42752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1336/1563 Loss: 2.095 | Acc: 27.968% (11966/42784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1337/1563 Loss: 2.095 | Acc: 27.966% (11974/42816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1338/1563 Loss: 2.095 | Acc: 27.976% (11987/42848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1339/1563 Loss: 2.095 | Acc: 27.976% (11996/42880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1340/1563 Loss: 2.095 | Acc: 27.983% (12008/42912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1341/1563 Loss: 2.095 | Acc: 27.981% (12016/42944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1342/1563 Loss: 2.095 | Acc: 27.988% (12028/42976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1343/1563 Loss: 2.095 | Acc: 27.985% (12036/43008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1344/1563 Loss: 2.095 | Acc: 27.988% (12046/43040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1345/1563 Loss: 2.095 | Acc: 27.979% (12051/43072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1346/1563 Loss: 2.096 | Acc: 27.974% (12058/43104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1347/1563 Loss: 2.096 | Acc: 27.967% (12064/43136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1348/1563 Loss: 2.096 | Acc: 27.958% (12069/43168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1349/1563 Loss: 2.097 | Acc: 27.949% (12074/43200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1350/1563 Loss: 2.096 | Acc: 27.961% (12088/43232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1351/1563 Loss: 2.097 | Acc: 27.956% (12095/43264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1352/1563 Loss: 2.097 | Acc: 27.954% (12103/43296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1353/1563 Loss: 2.097 | Acc: 27.961% (12115/43328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1354/1563 Loss: 2.097 | Acc: 27.968% (12127/43360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1355/1563 Loss: 2.097 | Acc: 27.973% (12138/43392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1356/1563 Loss: 2.097 | Acc: 27.980% (12150/43424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1357/1563 Loss: 2.097 | Acc: 27.978% (12158/43456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1358/1563 Loss: 2.097 | Acc: 27.982% (12169/43488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1359/1563 Loss: 2.097 | Acc: 27.983% (12178/43520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1360/1563 Loss: 2.097 | Acc: 27.976% (12184/43552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1361/1563 Loss: 2.097 | Acc: 27.974% (12192/43584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1362/1563 Loss: 2.097 | Acc: 27.971% (12200/43616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1363/1563 Loss: 2.097 | Acc: 27.978% (12212/43648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1364/1563 Loss: 2.097 | Acc: 27.972% (12218/43680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1365/1563 Loss: 2.097 | Acc: 27.963% (12223/43712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1366/1563 Loss: 2.097 | Acc: 27.956% (12229/43744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1367/1563 Loss: 2.098 | Acc: 27.949% (12235/43776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1368/1563 Loss: 2.097 | Acc: 27.954% (12246/43808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1369/1563 Loss: 2.097 | Acc: 27.952% (12254/43840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1370/1563 Loss: 2.097 | Acc: 27.949% (12262/43872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1371/1563 Loss: 2.097 | Acc: 27.961% (12276/43904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1372/1563 Loss: 2.097 | Acc: 27.966% (12287/43936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1373/1563 Loss: 2.097 | Acc: 27.959% (12293/43968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1374/1563 Loss: 2.097 | Acc: 27.961% (12303/44000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1375/1563 Loss: 2.097 | Acc: 27.966% (12314/44032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1376/1563 Loss: 2.097 | Acc: 27.968% (12324/44064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1377/1563 Loss: 2.098 | Acc: 27.955% (12327/44096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1378/1563 Loss: 2.098 | Acc: 27.960% (12338/44128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1379/1563 Loss: 2.098 | Acc: 27.953% (12344/44160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1380/1563 Loss: 2.098 | Acc: 27.953% (12353/44192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1381/1563 Loss: 2.098 | Acc: 27.953% (12362/44224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1382/1563 Loss: 2.098 | Acc: 27.958% (12373/44256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1383/1563 Loss: 2.098 | Acc: 27.956% (12381/44288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1384/1563 Loss: 2.098 | Acc: 27.958% (12391/44320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1385/1563 Loss: 2.098 | Acc: 27.951% (12397/44352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1386/1563 Loss: 2.098 | Acc: 27.954% (12407/44384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1387/1563 Loss: 2.098 | Acc: 27.954% (12416/44416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1388/1563 Loss: 2.098 | Acc: 27.952% (12424/44448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1389/1563 Loss: 2.098 | Acc: 27.956% (12435/44480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1390/1563 Loss: 2.098 | Acc: 27.954% (12443/44512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1391/1563 Loss: 2.098 | Acc: 27.966% (12457/44544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1392/1563 Loss: 2.097 | Acc: 27.970% (12468/44576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1393/1563 Loss: 2.097 | Acc: 27.984% (12483/44608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1394/1563 Loss: 2.098 | Acc: 27.975% (12488/44640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1395/1563 Loss: 2.098 | Acc: 27.977% (12498/44672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1396/1563 Loss: 2.098 | Acc: 27.977% (12507/44704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1397/1563 Loss: 2.098 | Acc: 27.975% (12515/44736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1398/1563 Loss: 2.097 | Acc: 27.973% (12523/44768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1399/1563 Loss: 2.098 | Acc: 27.969% (12530/44800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1400/1563 Loss: 2.098 | Acc: 27.971% (12540/44832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1401/1563 Loss: 2.098 | Acc: 27.973% (12550/44864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1402/1563 Loss: 2.098 | Acc: 27.976% (12560/44896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1403/1563 Loss: 2.098 | Acc: 27.969% (12566/44928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1404/1563 Loss: 2.098 | Acc: 27.967% (12574/44960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1405/1563 Loss: 2.098 | Acc: 27.958% (12579/44992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1406/1563 Loss: 2.098 | Acc: 27.961% (12589/45024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1407/1563 Loss: 2.098 | Acc: 27.965% (12600/45056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1408/1563 Loss: 2.098 | Acc: 27.968% (12610/45088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1409/1563 Loss: 2.099 | Acc: 27.968% (12619/45120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1410/1563 Loss: 2.099 | Acc: 27.972% (12630/45152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1411/1563 Loss: 2.099 | Acc: 27.972% (12639/45184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1412/1563 Loss: 2.099 | Acc: 27.977% (12650/45216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1413/1563 Loss: 2.099 | Acc: 27.968% (12655/45248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1414/1563 Loss: 2.099 | Acc: 27.968% (12664/45280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1415/1563 Loss: 2.099 | Acc: 27.962% (12670/45312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1416/1563 Loss: 2.099 | Acc: 27.968% (12682/45344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1417/1563 Loss: 2.099 | Acc: 27.969% (12691/45376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1418/1563 Loss: 2.099 | Acc: 27.964% (12698/45408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1419/1563 Loss: 2.099 | Acc: 27.962% (12706/45440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1420/1563 Loss: 2.099 | Acc: 27.958% (12713/45472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1421/1563 Loss: 2.099 | Acc: 27.962% (12724/45504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1422/1563 Loss: 2.099 | Acc: 27.967% (12735/45536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1423/1563 Loss: 2.099 | Acc: 27.967% (12744/45568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1424/1563 Loss: 2.099 | Acc: 27.969% (12754/45600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1425/1563 Loss: 2.099 | Acc: 27.969% (12763/45632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1426/1563 Loss: 2.098 | Acc: 27.976% (12775/45664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1427/1563 Loss: 2.098 | Acc: 27.978% (12785/45696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1428/1563 Loss: 2.098 | Acc: 27.981% (12795/45728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1429/1563 Loss: 2.098 | Acc: 27.981% (12804/45760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1430/1563 Loss: 2.098 | Acc: 27.974% (12810/45792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1431/1563 Loss: 2.098 | Acc: 27.977% (12820/45824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1432/1563 Loss: 2.098 | Acc: 27.972% (12827/45856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1433/1563 Loss: 2.098 | Acc: 27.979% (12839/45888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1434/1563 Loss: 2.098 | Acc: 27.988% (12852/45920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1435/1563 Loss: 2.098 | Acc: 27.992% (12863/45952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1436/1563 Loss: 2.098 | Acc: 27.995% (12873/45984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1437/1563 Loss: 2.098 | Acc: 27.992% (12881/46016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1438/1563 Loss: 2.098 | Acc: 27.986% (12887/46048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1439/1563 Loss: 2.098 | Acc: 27.980% (12893/46080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1440/1563 Loss: 2.098 | Acc: 27.980% (12902/46112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1441/1563 Loss: 2.098 | Acc: 27.980% (12911/46144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1442/1563 Loss: 2.098 | Acc: 27.969% (12915/46176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1443/1563 Loss: 2.098 | Acc: 27.958% (12919/46208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1444/1563 Loss: 2.098 | Acc: 27.961% (12929/46240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1445/1563 Loss: 2.098 | Acc: 27.956% (12936/46272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1446/1563 Loss: 2.098 | Acc: 27.952% (12943/46304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1447/1563 Loss: 2.098 | Acc: 27.955% (12953/46336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1448/1563 Loss: 2.098 | Acc: 27.952% (12961/46368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1449/1563 Loss: 2.097 | Acc: 27.953% (12970/46400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1450/1563 Loss: 2.097 | Acc: 27.959% (12982/46432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1451/1563 Loss: 2.097 | Acc: 27.957% (12990/46464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1452/1563 Loss: 2.097 | Acc: 27.966% (13003/46496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1453/1563 Loss: 2.097 | Acc: 27.968% (13013/46528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1454/1563 Loss: 2.097 | Acc: 27.973% (13024/46560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1455/1563 Loss: 2.097 | Acc: 27.975% (13034/46592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1456/1563 Loss: 2.097 | Acc: 27.973% (13042/46624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1457/1563 Loss: 2.097 | Acc: 27.975% (13052/46656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1458/1563 Loss: 2.097 | Acc: 27.973% (13060/46688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1459/1563 Loss: 2.097 | Acc: 27.975% (13070/46720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1460/1563 Loss: 2.097 | Acc: 27.969% (13076/46752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1461/1563 Loss: 2.097 | Acc: 27.958% (13080/46784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1462/1563 Loss: 2.097 | Acc: 27.952% (13086/46816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1463/1563 Loss: 2.097 | Acc: 27.948% (13093/46848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1464/1563 Loss: 2.097 | Acc: 27.946% (13101/46880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1465/1563 Loss: 2.097 | Acc: 27.944% (13109/46912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1466/1563 Loss: 2.097 | Acc: 27.946% (13119/46944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1467/1563 Loss: 2.097 | Acc: 27.938% (13124/46976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1468/1563 Loss: 2.097 | Acc: 27.934% (13131/47008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1469/1563 Loss: 2.097 | Acc: 27.942% (13144/47040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1470/1563 Loss: 2.097 | Acc: 27.953% (13158/47072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1471/1563 Loss: 2.097 | Acc: 27.951% (13166/47104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1472/1563 Loss: 2.097 | Acc: 27.949% (13174/47136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1473/1563 Loss: 2.097 | Acc: 27.955% (13186/47168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1474/1563 Loss: 2.096 | Acc: 27.958% (13196/47200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1475/1563 Loss: 2.097 | Acc: 27.954% (13203/47232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1476/1563 Loss: 2.097 | Acc: 27.954% (13212/47264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1477/1563 Loss: 2.097 | Acc: 27.950% (13219/47296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1478/1563 Loss: 2.097 | Acc: 27.952% (13229/47328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1479/1563 Loss: 2.097 | Acc: 27.943% (13234/47360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1480/1563 Loss: 2.097 | Acc: 27.937% (13240/47392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1481/1563 Loss: 2.097 | Acc: 27.944% (13252/47424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1482/1563 Loss: 2.097 | Acc: 27.942% (13260/47456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1483/1563 Loss: 2.097 | Acc: 27.929% (13263/47488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1484/1563 Loss: 2.097 | Acc: 27.929% (13272/47520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1485/1563 Loss: 2.098 | Acc: 27.915% (13274/47552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1486/1563 Loss: 2.098 | Acc: 27.906% (13279/47584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1487/1563 Loss: 2.098 | Acc: 27.909% (13289/47616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1488/1563 Loss: 2.098 | Acc: 27.915% (13301/47648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1489/1563 Loss: 2.098 | Acc: 27.919% (13312/47680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1490/1563 Loss: 2.098 | Acc: 27.913% (13318/47712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1491/1563 Loss: 2.098 | Acc: 27.918% (13329/47744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1492/1563 Loss: 2.097 | Acc: 27.922% (13340/47776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1493/1563 Loss: 2.097 | Acc: 27.922% (13349/47808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1494/1563 Loss: 2.097 | Acc: 27.926% (13360/47840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1495/1563 Loss: 2.097 | Acc: 27.924% (13368/47872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1496/1563 Loss: 2.097 | Acc: 27.931% (13380/47904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1497/1563 Loss: 2.097 | Acc: 27.931% (13389/47936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1498/1563 Loss: 2.097 | Acc: 27.933% (13399/47968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1499/1563 Loss: 2.096 | Acc: 27.946% (13414/48000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1500/1563 Loss: 2.096 | Acc: 27.948% (13424/48032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1501/1563 Loss: 2.096 | Acc: 27.942% (13430/48064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1502/1563 Loss: 2.096 | Acc: 27.946% (13441/48096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1503/1563 Loss: 2.096 | Acc: 27.953% (13453/48128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1504/1563 Loss: 2.096 | Acc: 27.957% (13464/48160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1505/1563 Loss: 2.095 | Acc: 27.959% (13474/48192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1506/1563 Loss: 2.096 | Acc: 27.953% (13480/48224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1507/1563 Loss: 2.096 | Acc: 27.955% (13490/48256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1508/1563 Loss: 2.095 | Acc: 27.963% (13503/48288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1509/1563 Loss: 2.095 | Acc: 27.976% (13518/48320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1510/1563 Loss: 2.095 | Acc: 27.972% (13525/48352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1511/1563 Loss: 2.096 | Acc: 27.968% (13532/48384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1512/1563 Loss: 2.095 | Acc: 27.974% (13544/48416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1513/1563 Loss: 2.095 | Acc: 27.974% (13553/48448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1514/1563 Loss: 2.095 | Acc: 27.974% (13562/48480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1515/1563 Loss: 2.096 | Acc: 27.975% (13571/48512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1516/1563 Loss: 2.096 | Acc: 27.973% (13579/48544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1517/1563 Loss: 2.096 | Acc: 27.977% (13590/48576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1518/1563 Loss: 2.096 | Acc: 27.971% (13596/48608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1519/1563 Loss: 2.096 | Acc: 27.963% (13601/48640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1520/1563 Loss: 2.096 | Acc: 27.957% (13607/48672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1521/1563 Loss: 2.096 | Acc: 27.955% (13615/48704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1522/1563 Loss: 2.096 | Acc: 27.955% (13624/48736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1523/1563 Loss: 2.096 | Acc: 27.951% (13631/48768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1524/1563 Loss: 2.096 | Acc: 27.957% (13643/48800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1525/1563 Loss: 2.096 | Acc: 27.961% (13654/48832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1526/1563 Loss: 2.095 | Acc: 27.959% (13662/48864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1527/1563 Loss: 2.096 | Acc: 27.957% (13670/48896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1528/1563 Loss: 2.096 | Acc: 27.955% (13678/48928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1529/1563 Loss: 2.096 | Acc: 27.949% (13684/48960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1530/1563 Loss: 2.095 | Acc: 27.952% (13694/48992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1531/1563 Loss: 2.095 | Acc: 27.954% (13704/49024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1532/1563 Loss: 2.095 | Acc: 27.954% (13713/49056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1533/1563 Loss: 2.095 | Acc: 27.950% (13720/49088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1534/1563 Loss: 2.095 | Acc: 27.946% (13727/49120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1535/1563 Loss: 2.096 | Acc: 27.938% (13732/49152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1536/1563 Loss: 2.096 | Acc: 27.938% (13741/49184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1537/1563 Loss: 2.096 | Acc: 27.944% (13753/49216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1538/1563 Loss: 2.096 | Acc: 27.944% (13762/49248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1539/1563 Loss: 2.096 | Acc: 27.942% (13770/49280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1540/1563 Loss: 2.095 | Acc: 27.940% (13778/49312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1541/1563 Loss: 2.095 | Acc: 27.937% (13785/49344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1542/1563 Loss: 2.095 | Acc: 27.949% (13800/49376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1543/1563 Loss: 2.095 | Acc: 27.957% (13813/49408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1544/1563 Loss: 2.095 | Acc: 27.951% (13819/49440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1545/1563 Loss: 2.095 | Acc: 27.947% (13826/49472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1546/1563 Loss: 2.095 | Acc: 27.947% (13835/49504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1547/1563 Loss: 2.096 | Acc: 27.947% (13844/49536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1548/1563 Loss: 2.096 | Acc: 27.947% (13853/49568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1549/1563 Loss: 2.096 | Acc: 27.942% (13859/49600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1550/1563 Loss: 2.096 | Acc: 27.936% (13865/49632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1551/1563 Loss: 2.096 | Acc: 27.934% (13873/49664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1552/1563 Loss: 2.096 | Acc: 27.940% (13885/49696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1553/1563 Loss: 2.096 | Acc: 27.946% (13897/49728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1554/1563 Loss: 2.096 | Acc: 27.946% (13906/49760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1555/1563 Loss: 2.096 | Acc: 27.946% (13915/49792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1556/1563 Loss: 2.096 | Acc: 27.936% (13919/49824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1557/1563 Loss: 2.096 | Acc: 27.930% (13925/49856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1558/1563 Loss: 2.096 | Acc: 27.937% (13937/49888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1559/1563 Loss: 2.096 | Acc: 27.939% (13947/49920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1560/1563 Loss: 2.096 | Acc: 27.937% (13955/49952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1561/1563 Loss: 2.096 | Acc: 27.935% (13963/49984)\n",
            "torch.Size([16, 120, 1, 1])\n",
            "torch.Size([16, 120])\n",
            "Epoch 49 Step 1562/1563 Loss: 2.097 | Acc: 27.934% (13967/50000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 0/313 Test Loss: 2.054 | Test Acc: 28.125% (9/32)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 1/313 Test Loss: 2.043 | Test Acc: 25.000% (16/64)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 2/313 Test Loss: 2.061 | Test Acc: 28.125% (27/96)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 3/313 Test Loss: 2.087 | Test Acc: 27.344% (35/128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 4/313 Test Loss: 2.081 | Test Acc: 28.750% (46/160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 5/313 Test Loss: 2.028 | Test Acc: 29.167% (56/192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 6/313 Test Loss: 2.078 | Test Acc: 28.125% (63/224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 7/313 Test Loss: 2.057 | Test Acc: 29.297% (75/256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 8/313 Test Loss: 2.072 | Test Acc: 29.514% (85/288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 9/313 Test Loss: 2.048 | Test Acc: 29.062% (93/320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 10/313 Test Loss: 2.058 | Test Acc: 28.125% (99/352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 11/313 Test Loss: 2.115 | Test Acc: 27.604% (106/384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 12/313 Test Loss: 2.100 | Test Acc: 27.885% (116/416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 13/313 Test Loss: 2.097 | Test Acc: 27.679% (124/448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 14/313 Test Loss: 2.116 | Test Acc: 27.083% (130/480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 15/313 Test Loss: 2.113 | Test Acc: 26.953% (138/512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 16/313 Test Loss: 2.106 | Test Acc: 27.574% (150/544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 17/313 Test Loss: 2.086 | Test Acc: 27.604% (159/576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 18/313 Test Loss: 2.088 | Test Acc: 27.303% (166/608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 19/313 Test Loss: 2.070 | Test Acc: 27.500% (176/640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 20/313 Test Loss: 2.059 | Test Acc: 27.976% (188/672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 21/313 Test Loss: 2.058 | Test Acc: 28.267% (199/704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 22/313 Test Loss: 2.059 | Test Acc: 28.533% (210/736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 23/313 Test Loss: 2.050 | Test Acc: 29.036% (223/768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 24/313 Test Loss: 2.058 | Test Acc: 28.625% (229/800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 25/313 Test Loss: 2.047 | Test Acc: 29.327% (244/832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 26/313 Test Loss: 2.047 | Test Acc: 29.282% (253/864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 27/313 Test Loss: 2.037 | Test Acc: 29.576% (265/896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 28/313 Test Loss: 2.045 | Test Acc: 29.418% (273/928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 29/313 Test Loss: 2.032 | Test Acc: 29.583% (284/960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 30/313 Test Loss: 2.027 | Test Acc: 29.637% (294/992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 31/313 Test Loss: 2.022 | Test Acc: 29.980% (307/1024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 32/313 Test Loss: 2.034 | Test Acc: 29.640% (313/1056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 33/313 Test Loss: 2.032 | Test Acc: 29.779% (324/1088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 34/313 Test Loss: 2.023 | Test Acc: 29.554% (331/1120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 35/313 Test Loss: 2.035 | Test Acc: 29.427% (339/1152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 36/313 Test Loss: 2.035 | Test Acc: 29.392% (348/1184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 37/313 Test Loss: 2.038 | Test Acc: 29.605% (360/1216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 38/313 Test Loss: 2.038 | Test Acc: 29.487% (368/1248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 39/313 Test Loss: 2.034 | Test Acc: 29.531% (378/1280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 40/313 Test Loss: 2.035 | Test Acc: 29.954% (393/1312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 41/313 Test Loss: 2.044 | Test Acc: 30.060% (404/1344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 42/313 Test Loss: 2.043 | Test Acc: 30.015% (413/1376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 43/313 Test Loss: 2.049 | Test Acc: 29.830% (420/1408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 44/313 Test Loss: 2.058 | Test Acc: 29.792% (429/1440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 45/313 Test Loss: 2.051 | Test Acc: 30.163% (444/1472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 46/313 Test Loss: 2.051 | Test Acc: 30.253% (455/1504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 47/313 Test Loss: 2.049 | Test Acc: 30.078% (462/1536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 48/313 Test Loss: 2.042 | Test Acc: 30.293% (475/1568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 49/313 Test Loss: 2.050 | Test Acc: 30.000% (480/1600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 50/313 Test Loss: 2.049 | Test Acc: 29.963% (489/1632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 51/313 Test Loss: 2.043 | Test Acc: 30.048% (500/1664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 52/313 Test Loss: 2.038 | Test Acc: 30.248% (513/1696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 53/313 Test Loss: 2.037 | Test Acc: 30.093% (520/1728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 54/313 Test Loss: 2.046 | Test Acc: 30.000% (528/1760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 55/313 Test Loss: 2.045 | Test Acc: 30.022% (538/1792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 56/313 Test Loss: 2.045 | Test Acc: 30.044% (548/1824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 57/313 Test Loss: 2.043 | Test Acc: 30.388% (564/1856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 58/313 Test Loss: 2.041 | Test Acc: 30.403% (574/1888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 59/313 Test Loss: 2.041 | Test Acc: 30.417% (584/1920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 60/313 Test Loss: 2.044 | Test Acc: 30.328% (592/1952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 61/313 Test Loss: 2.037 | Test Acc: 30.696% (609/1984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 62/313 Test Loss: 2.041 | Test Acc: 30.704% (619/2016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 63/313 Test Loss: 2.036 | Test Acc: 30.762% (630/2048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 64/313 Test Loss: 2.029 | Test Acc: 30.721% (639/2080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 65/313 Test Loss: 2.028 | Test Acc: 30.919% (653/2112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 66/313 Test Loss: 2.026 | Test Acc: 30.970% (664/2144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 67/313 Test Loss: 2.027 | Test Acc: 30.790% (670/2176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 68/313 Test Loss: 2.028 | Test Acc: 30.707% (678/2208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 69/313 Test Loss: 2.029 | Test Acc: 30.536% (684/2240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 70/313 Test Loss: 2.028 | Test Acc: 30.590% (695/2272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 71/313 Test Loss: 2.029 | Test Acc: 30.642% (706/2304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 72/313 Test Loss: 2.037 | Test Acc: 30.479% (712/2336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 73/313 Test Loss: 2.032 | Test Acc: 30.448% (721/2368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 74/313 Test Loss: 2.031 | Test Acc: 30.417% (730/2400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 75/313 Test Loss: 2.030 | Test Acc: 30.510% (742/2432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 76/313 Test Loss: 2.029 | Test Acc: 30.601% (754/2464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 77/313 Test Loss: 2.029 | Test Acc: 30.729% (767/2496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 78/313 Test Loss: 2.032 | Test Acc: 30.696% (776/2528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 79/313 Test Loss: 2.038 | Test Acc: 30.547% (782/2560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 80/313 Test Loss: 2.042 | Test Acc: 30.440% (789/2592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 81/313 Test Loss: 2.040 | Test Acc: 30.488% (800/2624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 82/313 Test Loss: 2.040 | Test Acc: 30.422% (808/2656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 83/313 Test Loss: 2.040 | Test Acc: 30.320% (815/2688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 84/313 Test Loss: 2.036 | Test Acc: 30.331% (825/2720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 85/313 Test Loss: 2.036 | Test Acc: 30.414% (837/2752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 86/313 Test Loss: 2.035 | Test Acc: 30.352% (845/2784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 87/313 Test Loss: 2.033 | Test Acc: 30.469% (858/2816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 88/313 Test Loss: 2.035 | Test Acc: 30.407% (866/2848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 89/313 Test Loss: 2.035 | Test Acc: 30.382% (875/2880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 90/313 Test Loss: 2.035 | Test Acc: 30.460% (887/2912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 91/313 Test Loss: 2.032 | Test Acc: 30.435% (896/2944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 92/313 Test Loss: 2.032 | Test Acc: 30.410% (905/2976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 93/313 Test Loss: 2.029 | Test Acc: 30.485% (917/3008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 94/313 Test Loss: 2.025 | Test Acc: 30.625% (931/3040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 95/313 Test Loss: 2.021 | Test Acc: 30.729% (944/3072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 96/313 Test Loss: 2.018 | Test Acc: 30.799% (956/3104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 97/313 Test Loss: 2.020 | Test Acc: 30.644% (961/3136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 98/313 Test Loss: 2.020 | Test Acc: 30.587% (969/3168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 99/313 Test Loss: 2.019 | Test Acc: 30.594% (979/3200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 100/313 Test Loss: 2.020 | Test Acc: 30.538% (987/3232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 101/313 Test Loss: 2.020 | Test Acc: 30.484% (995/3264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 102/313 Test Loss: 2.015 | Test Acc: 30.613% (1009/3296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 103/313 Test Loss: 2.017 | Test Acc: 30.529% (1016/3328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 104/313 Test Loss: 2.016 | Test Acc: 30.565% (1027/3360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 105/313 Test Loss: 2.012 | Test Acc: 30.690% (1041/3392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 106/313 Test Loss: 2.017 | Test Acc: 30.520% (1045/3424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 107/313 Test Loss: 2.017 | Test Acc: 30.498% (1054/3456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 108/313 Test Loss: 2.017 | Test Acc: 30.419% (1061/3488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 109/313 Test Loss: 2.018 | Test Acc: 30.398% (1070/3520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 110/313 Test Loss: 2.018 | Test Acc: 30.405% (1080/3552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 111/313 Test Loss: 2.017 | Test Acc: 30.441% (1091/3584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 112/313 Test Loss: 2.016 | Test Acc: 30.448% (1101/3616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 113/313 Test Loss: 2.018 | Test Acc: 30.400% (1109/3648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 114/313 Test Loss: 2.016 | Test Acc: 30.516% (1123/3680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 115/313 Test Loss: 2.018 | Test Acc: 30.442% (1130/3712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 116/313 Test Loss: 2.018 | Test Acc: 30.449% (1140/3744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 117/313 Test Loss: 2.019 | Test Acc: 30.350% (1146/3776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 118/313 Test Loss: 2.021 | Test Acc: 30.357% (1156/3808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 119/313 Test Loss: 2.018 | Test Acc: 30.469% (1170/3840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 120/313 Test Loss: 2.015 | Test Acc: 30.604% (1185/3872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 121/313 Test Loss: 2.017 | Test Acc: 30.507% (1191/3904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 122/313 Test Loss: 2.017 | Test Acc: 30.539% (1202/3936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 123/313 Test Loss: 2.013 | Test Acc: 30.620% (1215/3968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 124/313 Test Loss: 2.016 | Test Acc: 30.500% (1220/4000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 125/313 Test Loss: 2.016 | Test Acc: 30.531% (1231/4032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 126/313 Test Loss: 2.017 | Test Acc: 30.487% (1239/4064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 127/313 Test Loss: 2.020 | Test Acc: 30.420% (1246/4096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 128/313 Test Loss: 2.022 | Test Acc: 30.451% (1257/4128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 129/313 Test Loss: 2.022 | Test Acc: 30.409% (1265/4160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 130/313 Test Loss: 2.022 | Test Acc: 30.367% (1273/4192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 131/313 Test Loss: 2.023 | Test Acc: 30.398% (1284/4224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 132/313 Test Loss: 2.021 | Test Acc: 30.451% (1296/4256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 133/313 Test Loss: 2.022 | Test Acc: 30.434% (1305/4288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 134/313 Test Loss: 2.022 | Test Acc: 30.394% (1313/4320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 135/313 Test Loss: 2.022 | Test Acc: 30.469% (1326/4352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 136/313 Test Loss: 2.020 | Test Acc: 30.497% (1337/4384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 137/313 Test Loss: 2.019 | Test Acc: 30.503% (1347/4416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 138/313 Test Loss: 2.021 | Test Acc: 30.508% (1357/4448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 139/313 Test Loss: 2.020 | Test Acc: 30.558% (1369/4480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 140/313 Test Loss: 2.019 | Test Acc: 30.585% (1380/4512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 141/313 Test Loss: 2.017 | Test Acc: 30.568% (1389/4544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 142/313 Test Loss: 2.017 | Test Acc: 30.507% (1396/4576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 143/313 Test Loss: 2.017 | Test Acc: 30.577% (1409/4608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 144/313 Test Loss: 2.016 | Test Acc: 30.560% (1418/4640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 145/313 Test Loss: 2.013 | Test Acc: 30.586% (1429/4672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 146/313 Test Loss: 2.014 | Test Acc: 30.548% (1437/4704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 147/313 Test Loss: 2.015 | Test Acc: 30.448% (1442/4736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 148/313 Test Loss: 2.016 | Test Acc: 30.516% (1455/4768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 149/313 Test Loss: 2.017 | Test Acc: 30.458% (1462/4800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 150/313 Test Loss: 2.015 | Test Acc: 30.546% (1476/4832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 151/313 Test Loss: 2.013 | Test Acc: 30.592% (1488/4864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 152/313 Test Loss: 2.011 | Test Acc: 30.576% (1497/4896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 153/313 Test Loss: 2.012 | Test Acc: 30.540% (1505/4928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 154/313 Test Loss: 2.010 | Test Acc: 30.625% (1519/4960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 155/313 Test Loss: 2.009 | Test Acc: 30.609% (1528/4992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 156/313 Test Loss: 2.007 | Test Acc: 30.673% (1541/5024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 157/313 Test Loss: 2.010 | Test Acc: 30.676% (1551/5056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 158/313 Test Loss: 2.010 | Test Acc: 30.680% (1561/5088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 159/313 Test Loss: 2.011 | Test Acc: 30.645% (1569/5120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 160/313 Test Loss: 2.012 | Test Acc: 30.629% (1578/5152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 161/313 Test Loss: 2.011 | Test Acc: 30.729% (1593/5184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 162/313 Test Loss: 2.013 | Test Acc: 30.656% (1599/5216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 163/313 Test Loss: 2.015 | Test Acc: 30.602% (1606/5248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 164/313 Test Loss: 2.016 | Test Acc: 30.511% (1611/5280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 165/313 Test Loss: 2.014 | Test Acc: 30.553% (1623/5312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 166/313 Test Loss: 2.016 | Test Acc: 30.558% (1633/5344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 167/313 Test Loss: 2.016 | Test Acc: 30.655% (1648/5376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 168/313 Test Loss: 2.016 | Test Acc: 30.640% (1657/5408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 169/313 Test Loss: 2.015 | Test Acc: 30.662% (1668/5440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 170/313 Test Loss: 2.015 | Test Acc: 30.665% (1678/5472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 171/313 Test Loss: 2.015 | Test Acc: 30.687% (1689/5504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 172/313 Test Loss: 2.015 | Test Acc: 30.672% (1698/5536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 173/313 Test Loss: 2.016 | Test Acc: 30.693% (1709/5568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 174/313 Test Loss: 2.016 | Test Acc: 30.714% (1720/5600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 175/313 Test Loss: 2.016 | Test Acc: 30.717% (1730/5632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 176/313 Test Loss: 2.017 | Test Acc: 30.738% (1741/5664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 177/313 Test Loss: 2.019 | Test Acc: 30.723% (1750/5696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 178/313 Test Loss: 2.017 | Test Acc: 30.744% (1761/5728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 179/313 Test Loss: 2.016 | Test Acc: 30.764% (1772/5760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 180/313 Test Loss: 2.014 | Test Acc: 30.836% (1786/5792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 181/313 Test Loss: 2.014 | Test Acc: 30.821% (1795/5824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 182/313 Test Loss: 2.016 | Test Acc: 30.806% (1804/5856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 183/313 Test Loss: 2.016 | Test Acc: 30.876% (1818/5888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 184/313 Test Loss: 2.017 | Test Acc: 30.845% (1826/5920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 185/313 Test Loss: 2.018 | Test Acc: 30.847% (1836/5952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 186/313 Test Loss: 2.016 | Test Acc: 30.882% (1848/5984)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 187/313 Test Loss: 2.016 | Test Acc: 30.901% (1859/6016)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 188/313 Test Loss: 2.018 | Test Acc: 30.853% (1866/6048)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 189/313 Test Loss: 2.017 | Test Acc: 30.872% (1877/6080)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 190/313 Test Loss: 2.016 | Test Acc: 30.890% (1888/6112)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 191/313 Test Loss: 2.017 | Test Acc: 30.843% (1895/6144)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 192/313 Test Loss: 2.017 | Test Acc: 30.845% (1905/6176)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 193/313 Test Loss: 2.018 | Test Acc: 30.799% (1912/6208)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 194/313 Test Loss: 2.019 | Test Acc: 30.769% (1920/6240)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 195/313 Test Loss: 2.020 | Test Acc: 30.708% (1926/6272)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 196/313 Test Loss: 2.019 | Test Acc: 30.727% (1937/6304)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 197/313 Test Loss: 2.017 | Test Acc: 30.792% (1951/6336)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 198/313 Test Loss: 2.016 | Test Acc: 30.857% (1965/6368)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 199/313 Test Loss: 2.016 | Test Acc: 30.844% (1974/6400)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 200/313 Test Loss: 2.017 | Test Acc: 30.846% (1984/6432)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 201/313 Test Loss: 2.017 | Test Acc: 30.832% (1993/6464)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 202/313 Test Loss: 2.017 | Test Acc: 30.834% (2003/6496)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 203/313 Test Loss: 2.017 | Test Acc: 30.867% (2015/6528)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 204/313 Test Loss: 2.018 | Test Acc: 30.838% (2023/6560)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 205/313 Test Loss: 2.018 | Test Acc: 30.856% (2034/6592)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 206/313 Test Loss: 2.019 | Test Acc: 30.918% (2048/6624)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 207/313 Test Loss: 2.017 | Test Acc: 30.950% (2060/6656)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 208/313 Test Loss: 2.016 | Test Acc: 30.981% (2072/6688)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 209/313 Test Loss: 2.016 | Test Acc: 30.967% (2081/6720)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 210/313 Test Loss: 2.015 | Test Acc: 30.969% (2091/6752)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 211/313 Test Loss: 2.015 | Test Acc: 30.985% (2102/6784)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 212/313 Test Loss: 2.014 | Test Acc: 30.986% (2112/6816)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 213/313 Test Loss: 2.013 | Test Acc: 31.031% (2125/6848)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 214/313 Test Loss: 2.015 | Test Acc: 30.959% (2130/6880)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 215/313 Test Loss: 2.014 | Test Acc: 30.961% (2140/6912)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 216/313 Test Loss: 2.015 | Test Acc: 30.962% (2150/6944)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 217/313 Test Loss: 2.017 | Test Acc: 30.920% (2157/6976)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 218/313 Test Loss: 2.017 | Test Acc: 30.893% (2165/7008)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 219/313 Test Loss: 2.018 | Test Acc: 30.866% (2173/7040)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 220/313 Test Loss: 2.020 | Test Acc: 30.854% (2182/7072)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 221/313 Test Loss: 2.018 | Test Acc: 30.898% (2195/7104)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 222/313 Test Loss: 2.019 | Test Acc: 30.858% (2202/7136)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 223/313 Test Loss: 2.017 | Test Acc: 30.887% (2214/7168)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 224/313 Test Loss: 2.018 | Test Acc: 30.861% (2222/7200)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 225/313 Test Loss: 2.018 | Test Acc: 30.863% (2232/7232)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 226/313 Test Loss: 2.017 | Test Acc: 30.878% (2243/7264)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 227/313 Test Loss: 2.015 | Test Acc: 30.907% (2255/7296)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 228/313 Test Loss: 2.015 | Test Acc: 30.950% (2268/7328)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 229/313 Test Loss: 2.014 | Test Acc: 30.978% (2280/7360)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 230/313 Test Loss: 2.012 | Test Acc: 30.979% (2290/7392)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 231/313 Test Loss: 2.012 | Test Acc: 30.967% (2299/7424)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 232/313 Test Loss: 2.013 | Test Acc: 30.982% (2310/7456)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 233/313 Test Loss: 2.013 | Test Acc: 30.983% (2320/7488)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 234/313 Test Loss: 2.013 | Test Acc: 30.957% (2328/7520)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 235/313 Test Loss: 2.010 | Test Acc: 30.998% (2341/7552)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 236/313 Test Loss: 2.011 | Test Acc: 30.973% (2349/7584)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 237/313 Test Loss: 2.010 | Test Acc: 30.987% (2360/7616)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 238/313 Test Loss: 2.010 | Test Acc: 31.028% (2373/7648)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 239/313 Test Loss: 2.009 | Test Acc: 31.081% (2387/7680)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 240/313 Test Loss: 2.009 | Test Acc: 31.107% (2399/7712)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 241/313 Test Loss: 2.008 | Test Acc: 31.121% (2410/7744)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 242/313 Test Loss: 2.008 | Test Acc: 31.134% (2421/7776)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 243/313 Test Loss: 2.008 | Test Acc: 31.173% (2434/7808)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 244/313 Test Loss: 2.008 | Test Acc: 31.186% (2445/7840)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 245/313 Test Loss: 2.009 | Test Acc: 31.148% (2452/7872)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 246/313 Test Loss: 2.009 | Test Acc: 31.161% (2463/7904)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 247/313 Test Loss: 2.008 | Test Acc: 31.124% (2470/7936)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 248/313 Test Loss: 2.009 | Test Acc: 31.099% (2478/7968)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 249/313 Test Loss: 2.009 | Test Acc: 31.100% (2488/8000)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 250/313 Test Loss: 2.009 | Test Acc: 31.113% (2499/8032)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 251/313 Test Loss: 2.010 | Test Acc: 31.114% (2509/8064)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 252/313 Test Loss: 2.008 | Test Acc: 31.164% (2523/8096)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 253/313 Test Loss: 2.008 | Test Acc: 31.164% (2533/8128)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 254/313 Test Loss: 2.007 | Test Acc: 31.140% (2541/8160)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 255/313 Test Loss: 2.006 | Test Acc: 31.140% (2551/8192)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 256/313 Test Loss: 2.007 | Test Acc: 31.128% (2560/8224)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 257/313 Test Loss: 2.007 | Test Acc: 31.153% (2572/8256)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 258/313 Test Loss: 2.006 | Test Acc: 31.141% (2581/8288)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 259/313 Test Loss: 2.007 | Test Acc: 31.130% (2590/8320)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 260/313 Test Loss: 2.007 | Test Acc: 31.130% (2600/8352)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 261/313 Test Loss: 2.006 | Test Acc: 31.167% (2613/8384)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 262/313 Test Loss: 2.006 | Test Acc: 31.155% (2622/8416)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 263/313 Test Loss: 2.007 | Test Acc: 31.108% (2628/8448)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 264/313 Test Loss: 2.008 | Test Acc: 31.085% (2636/8480)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 265/313 Test Loss: 2.008 | Test Acc: 31.086% (2646/8512)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 266/313 Test Loss: 2.009 | Test Acc: 31.051% (2653/8544)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 267/313 Test Loss: 2.011 | Test Acc: 31.052% (2663/8576)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 268/313 Test Loss: 2.009 | Test Acc: 31.122% (2679/8608)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 269/313 Test Loss: 2.009 | Test Acc: 31.123% (2689/8640)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 270/313 Test Loss: 2.008 | Test Acc: 31.089% (2696/8672)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 271/313 Test Loss: 2.009 | Test Acc: 31.043% (2702/8704)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 272/313 Test Loss: 2.009 | Test Acc: 31.044% (2712/8736)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 273/313 Test Loss: 2.008 | Test Acc: 31.022% (2720/8768)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 274/313 Test Loss: 2.008 | Test Acc: 31.023% (2730/8800)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 275/313 Test Loss: 2.009 | Test Acc: 30.990% (2737/8832)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 276/313 Test Loss: 2.010 | Test Acc: 30.968% (2745/8864)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 277/313 Test Loss: 2.009 | Test Acc: 31.025% (2760/8896)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 278/313 Test Loss: 2.007 | Test Acc: 31.116% (2778/8928)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 279/313 Test Loss: 2.008 | Test Acc: 31.094% (2786/8960)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 280/313 Test Loss: 2.008 | Test Acc: 31.061% (2793/8992)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 281/313 Test Loss: 2.009 | Test Acc: 31.051% (2802/9024)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 282/313 Test Loss: 2.010 | Test Acc: 31.029% (2810/9056)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 283/313 Test Loss: 2.009 | Test Acc: 31.063% (2823/9088)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 284/313 Test Loss: 2.009 | Test Acc: 31.075% (2834/9120)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 285/313 Test Loss: 2.010 | Test Acc: 31.031% (2840/9152)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 286/313 Test Loss: 2.010 | Test Acc: 31.065% (2853/9184)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 287/313 Test Loss: 2.008 | Test Acc: 31.120% (2868/9216)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 288/313 Test Loss: 2.008 | Test Acc: 31.131% (2879/9248)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 289/313 Test Loss: 2.008 | Test Acc: 31.175% (2893/9280)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 290/313 Test Loss: 2.009 | Test Acc: 31.143% (2900/9312)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 291/313 Test Loss: 2.008 | Test Acc: 31.143% (2910/9344)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 292/313 Test Loss: 2.008 | Test Acc: 31.122% (2918/9376)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 293/313 Test Loss: 2.008 | Test Acc: 31.112% (2927/9408)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 294/313 Test Loss: 2.008 | Test Acc: 31.081% (2934/9440)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 295/313 Test Loss: 2.009 | Test Acc: 31.081% (2944/9472)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 296/313 Test Loss: 2.008 | Test Acc: 31.124% (2958/9504)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 297/313 Test Loss: 2.008 | Test Acc: 31.156% (2971/9536)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 298/313 Test Loss: 2.007 | Test Acc: 31.187% (2984/9568)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 299/313 Test Loss: 2.006 | Test Acc: 31.188% (2994/9600)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 300/313 Test Loss: 2.006 | Test Acc: 31.188% (3004/9632)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 301/313 Test Loss: 2.005 | Test Acc: 31.209% (3016/9664)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 302/313 Test Loss: 2.005 | Test Acc: 31.229% (3028/9696)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 303/313 Test Loss: 2.004 | Test Acc: 31.240% (3039/9728)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 304/313 Test Loss: 2.005 | Test Acc: 31.178% (3043/9760)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 305/313 Test Loss: 2.006 | Test Acc: 31.179% (3053/9792)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 306/313 Test Loss: 2.006 | Test Acc: 31.179% (3063/9824)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 307/313 Test Loss: 2.005 | Test Acc: 31.189% (3074/9856)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 308/313 Test Loss: 2.006 | Test Acc: 31.179% (3083/9888)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 309/313 Test Loss: 2.006 | Test Acc: 31.200% (3095/9920)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 310/313 Test Loss: 2.006 | Test Acc: 31.200% (3105/9952)\n",
            "torch.Size([32, 120, 1, 1])\n",
            "torch.Size([32, 120])\n",
            "Epoch 49 Step 311/313 Test Loss: 2.006 | Test Acc: 31.160% (3111/9984)\n",
            "torch.Size([16, 120, 1, 1])\n",
            "torch.Size([16, 120])\n",
            "Epoch 49 Step 312/313 Test Loss: 2.006 | Test Acc: 31.150% (3115/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyVVw_B3Nufk"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmfqQPa_2HGW",
        "outputId": "e7969e9c-cab9-4023-855c-ab6d6b3bafc0"
      },
      "source": [
        "print(best_acc)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[53.125, 37.36], [50.0, 35.77]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joAhGWcQ4oZA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}